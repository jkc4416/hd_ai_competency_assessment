"""
Multicollinearity Regression Analysis / ë‹¤ì¤‘ê³µì„ ì„± íšŒê·€ ë¶„ì„

This script demonstrates the effects of multicollinearity on regression models
and compares different techniques to handle it.

ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ì¤‘ê³µì„ ì„±ì´ íšŒê·€ ëª¨ë¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë³´ì—¬ì£¼ê³ ,
ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ë²•ë“¤ì„ ë¹„êµí•©ë‹ˆë‹¤.

Key Features / ì£¼ìš” ê¸°ëŠ¥:
    1. Generate synthetic data with strong multicollinearity
       ê°•í•œ ë‹¤ì¤‘ê³µì„ ì„±ì„ ê°€ì§„ í•©ì„± ë°ì´í„° ìƒì„±
    2. Train and compare four regression models:
       4ê°€ì§€ íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ:
       - OLS (Ordinary Least Squares): Baseline / ê¸°ì¤€ ëª¨ë¸
       - Ridge: L2 regularization / L2 ì •ê·œí™”
       - Lasso: L1 regularization / L1 ì •ê·œí™”
       - PCA: Dimensionality reduction / ì°¨ì› ì¶•ì†Œ
    3. Visualize coefficient differences / ê³„ìˆ˜ ì°¨ì´ ì‹œê°í™”
    4. Generate comprehensive analysis report / ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±

Mathematical Background / ìˆ˜í•™ì  ë°°ê²½:
    - Multicollinearity: High correlation between predictors
      ë‹¤ì¤‘ê³µì„ ì„±: ì˜ˆì¸¡ ë³€ìˆ˜ë“¤ ê°„ì˜ ë†’ì€ ìƒê´€ê´€ê³„
    - Ridge penalty: ||Î²||â‚‚Â² (L2 norm)
    - Lasso penalty: ||Î²||â‚ (L1 norm)
    - PCA: Orthogonal transformation to uncorrelated components
      PCA: ë¹„ìƒê´€ ì£¼ì„±ë¶„ìœ¼ë¡œì˜ ì§êµ ë³€í™˜

Author: Claude (AI Assistant)
Date: 2025-11-21
"""

# ==================== Import Libraries / ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ====================

# Numerical computing / ìˆ˜ì¹˜ ê³„ì‚°
import numpy as np  # For matrix operations and random number generation

# Data manipulation / ë°ì´í„° ì¡°ì‘
import pandas as pd  # For data structure and CSV operations

# Visualization / ì‹œê°í™”
import matplotlib  # Main plotting library
matplotlib.use('Agg')  # Use non-interactive backend for CLI environments
                       # CLI í™˜ê²½ìš© ë¹„ëŒ€í™”í˜• ë°±ì—”ë“œ ì‚¬ìš© (GUI ì—†ì´ ê·¸ë˜í”„ ì €ì¥)
import matplotlib.pyplot as plt  # Plotting interface
import matplotlib.font_manager as fm  # Font management for Korean text

# Machine learning / ë¨¸ì‹ ëŸ¬ë‹
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
    # LinearRegression: OLS (Ordinary Least Squares) / ìµœì†Œì œê³±ë²•
    # Ridge: L2 regularized regression / L2 ì •ê·œí™” íšŒê·€
    # Lasso: L1 regularized regression / L1 ì •ê·œí™” íšŒê·€
    # ElasticNet: L1 + L2 combined regularization / L1 + L2 ê²°í•© ì •ê·œí™”
from sklearn.decomposition import PCA
    # Principal Component Analysis for dimensionality reduction
    # ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ ì£¼ì„±ë¶„ ë¶„ì„
from sklearn.preprocessing import StandardScaler
    # Feature scaling (mean=0, std=1) / íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (í‰ê· =0, í‘œì¤€í¸ì°¨=1)
from sklearn.metrics import mean_squared_error, r2_score
    # Model evaluation metrics / ëª¨ë¸ í‰ê°€ ì§€í‘œ

# File system operations / íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…
import os  # Operating system interface
from pathlib import Path  # Object-oriented filesystem paths

# ==================== Configuration / í™˜ê²½ ì„¤ì • ====================

# Get script directory for relative path operations
# ìƒëŒ€ ê²½ë¡œ ì‘ì—…ì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
SCRIPT_DIR = Path(__file__).parent

# Configure Korean font for matplotlib / matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
# Without this configuration, Korean characters would appear as squares
# ì´ ì„¤ì •ì´ ì—†ìœ¼ë©´ í•œê¸€ ë¬¸ìê°€ ë„¤ëª¨(â–¡)ë¡œ í‘œì‹œë¨
font_path = '/home/claude-dev-kcj/.fonts/NanumGothic-Regular.ttf'
font_prop = fm.FontProperties(fname=font_path)
plt.rcParams['font.family'] = font_prop.get_name()  # Set default font family
plt.rcParams['axes.unicode_minus'] = False  # Fix minus sign display issue
                                             # ìœ ë‹ˆì½”ë“œ ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

# ==============================================================================
# 1ï¸âƒ£ Synthetic Data Generation with Multicollinearity
#    ë‹¤ì¤‘ê³µì„ ì„±ì„ í¬í•¨í•œ í•©ì„± ë°ì´í„° ìƒì„±
# ==============================================================================

# Set random seed for reproducibility / ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ ì„¤ì •
# Same seed = same random numbers = reproducible results
# ë™ì¼í•œ ì‹œë“œ = ë™ì¼í•œ ëœë¤ ìˆ«ì = ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
np.random.seed(42)

# -------------------- Dataset Parameters / ë°ì´í„°ì…‹ íŒŒë¼ë¯¸í„° --------------------
# Larger dataset makes multicollinearity effects more pronounced
# í° ë°ì´í„°ì…‹ì€ ë‹¤ì¤‘ê³µì„ ì„± íš¨ê³¼ë¥¼ ë” ëšœë ·í•˜ê²Œ ë§Œë“¦
n_samples = 10000  # Number of observations / ê´€ì¸¡ì¹˜ ê°œìˆ˜ (ì´ì „: 300)
n_base_features = 15  # Independent features / ë…ë¦½ íŠ¹ì„± ê°œìˆ˜ (ì´ì „: 3)

print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {n_samples} ìƒ˜í”Œ")
print(f"ğŸ“Š Dataset size: {n_samples} samples")

# -------------------- Base Features Generation / ê¸°ì´ˆ íŠ¹ì„± ìƒì„± --------------------
# Generate independent base features from standard normal distribution
# í‘œì¤€ ì •ê·œë¶„í¬ì—ì„œ ë…ë¦½ì ì¸ ê¸°ì´ˆ íŠ¹ì„± ìƒì„±
# X_base ~ N(0, 1) with shape (n_samples, n_base_features)
# These features are uncorrelated with each other
# ì´ íŠ¹ì„±ë“¤ì€ ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ì—†ìŒ
X_base = np.random.randn(n_samples, n_base_features)

# -------------------- Derived Features with Multicollinearity --------------------
# ë‹¤ì¤‘ê³µì„ ì„±ì„ ê°€ì§„ íŒŒìƒ íŠ¹ì„± ìƒì„±
#
# Strategy: Create 4 types of derived features for each base feature
# ì „ëµ: ê° ê¸°ì´ˆ íŠ¹ì„±ë§ˆë‹¤ 4ê°€ì§€ íƒ€ì…ì˜ íŒŒìƒ íŠ¹ì„± ìƒì„±
#
# This creates strong multicollinearity, which causes:
# ì´ëŠ” ê°•í•œ ë‹¤ì¤‘ê³µì„ ì„±ì„ ë§Œë“¤ì–´ ë‹¤ìŒ ë¬¸ì œë¥¼ ì•¼ê¸°:
#   - Unstable coefficient estimates in OLS / OLSì—ì„œ ë¶ˆì•ˆì •í•œ ê³„ìˆ˜ ì¶”ì •
#   - High variance in predictions / ì˜ˆì¸¡ì˜ ë†’ì€ ë¶„ì‚°
#   - Difficulty in interpreting individual feature importance
#     ê°œë³„ íŠ¹ì„± ì¤‘ìš”ë„ í•´ì„ì˜ ì–´ë ¤ì›€
derived_features = []

for i in range(n_base_features):
    # Type 1: Very high correlation (â‰ˆ0.97)
    # X_derived[i,0] = 0.97 * X_base[i] + Îµ, where Îµ ~ N(0, 0.03Â²)
    # ë§¤ìš° ë†’ì€ ìƒê´€ê´€ê³„: ê±°ì˜ ê¸°ì´ˆ íŠ¹ì„±ê³¼ ë™ì¼í•˜ì§€ë§Œ ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€
    derived_features.append(0.97 * X_base[:, i] + 0.03 * np.random.randn(n_samples))

    # Type 2: High correlation (â‰ˆ0.93)
    # Similar to Type 1 but with slightly more noise
    # Type 1ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ë…¸ì´ì¦ˆê°€ ì¡°ê¸ˆ ë” ë§ìŒ
    derived_features.append(0.93 * X_base[:, i] + 0.07 * np.random.randn(n_samples))

    # Type 3: Linear combination with adjacent feature
    # Creates cross-feature multicollinearity
    # ì¸ì ‘ íŠ¹ì„±ê³¼ì˜ ì„ í˜• ì¡°í•©: íŠ¹ì„± ê°„ êµì°¨ ë‹¤ì¤‘ê³µì„ ì„± ìƒì„±
    next_idx = (i + 1) % n_base_features  # Circular indexing / ìˆœí™˜ ì¸ë±ì‹±
    derived_features.append(
        0.7 * X_base[:, i] + 0.3 * X_base[:, next_idx] + 0.02 * np.random.randn(n_samples)
    )

    # Type 4: Another linear combination (with previous feature)
    # More complex multicollinearity pattern
    # ì´ì „ íŠ¹ì„±ê³¼ì˜ ì„ í˜• ì¡°í•©: ë” ë³µì¡í•œ ë‹¤ì¤‘ê³µì„ ì„± íŒ¨í„´
    prev_idx = (i - 1) % n_base_features
    derived_features.append(
        0.5 * X_base[:, i] + 0.4 * X_base[:, prev_idx] + 0.1 * np.random.randn(n_samples)
    )

# Add independent noise features (these have no multicollinearity)
# ë…ë¦½ì ì¸ ë…¸ì´ì¦ˆ íŠ¹ì„± ì¶”ê°€ (ë‹¤ì¤‘ê³µì„ ì„±ì´ ì—†ìŒ)
# These serve as "irrelevant" features to test feature selection
# ì´ë“¤ì€ íŠ¹ì„± ì„ íƒì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ "ë¬´ê´€í•œ" íŠ¹ì„± ì—­í• 
n_noise_features = 8
for _ in range(n_noise_features):
    derived_features.append(np.random.randn(n_samples))

# Stack all derived features into a matrix
# ëª¨ë“  íŒŒìƒ íŠ¹ì„±ì„ í–‰ë ¬ë¡œ ìŒ“ê¸°
X_derived = np.column_stack(derived_features)

# -------------------- Final Feature Matrix / ìµœì¢… íŠ¹ì„± í–‰ë ¬ --------------------
# Combine base and derived features
# ê¸°ì´ˆ íŠ¹ì„±ê³¼ íŒŒìƒ íŠ¹ì„± ê²°í•©
# X shape: (n_samples, n_features) = (5000, 48)
#   - Columns 0-7: Base features (independent) / ë…ë¦½ì ì¸ ê¸°ì´ˆ íŠ¹ì„±
#   - Columns 8-39: Derived features (multicollinear) / ë‹¤ì¤‘ê³µì„ ì„± ìˆëŠ” íŒŒìƒ íŠ¹ì„±
#   - Columns 40-47: Noise features (irrelevant) / ë¬´ê´€í•œ ë…¸ì´ì¦ˆ íŠ¹ì„±
X = np.column_stack([X_base, X_derived])

n_features = X.shape[1]
print(f"ğŸ“Š ì´ íŠ¹ì„± ìˆ˜: {n_features} (ê¸°ì´ˆ: {n_base_features}, íŒŒìƒ: {len(derived_features)})")
print(f"ğŸ“Š Total features: {n_features} (base: {n_base_features}, derived: {len(derived_features)})")

# -------------------- Target Variable Generation / ëª©í‘œ ë³€ìˆ˜ ìƒì„± --------------------
# Generate target variable using linear model: y = XÎ² + Îµ
# ì„ í˜• ëª¨ë¸ì„ ì‚¬ìš©í•œ ëª©í‘œ ë³€ìˆ˜ ìƒì„±: y = XÎ² + Îµ
#
# True coefficient vector Î² (sparse: most are zero)
# ì‹¤ì œ ê³„ìˆ˜ ë²¡í„° Î² (í¬ì†Œ: ëŒ€ë¶€ë¶„ì´ 0)
true_beta = np.zeros(n_features)

# Only assign non-zero coefficients to selected features
# ì„ íƒëœ íŠ¹ì„±ë“¤ì—ë§Œ 0ì´ ì•„ë‹Œ ê³„ìˆ˜ í• ë‹¹ (ë‹¨, ì „ì²´ true_betaì˜ ìš”ì†Œ ìˆ˜ëŠ” ì‹¤ì œ íŠ¹ì„± ê°œìˆ˜ì™€ ë™ì¼)
true_beta[0] = 3.0   # Base feature 1 / ê¸°ì´ˆ íŠ¹ì„± 1
true_beta[1] = -2.0  # Base feature 2 / ê¸°ì´ˆ íŠ¹ì„± 2
true_beta[2] = 1.5   # Base feature 3 / ê¸°ì´ˆ íŠ¹ì„± 3
true_beta[3] = 2.5   # Base feature 4 / ê¸°ì´ˆ íŠ¹ì„± 4
true_beta[4] = -1.0  # Base feature 5 / ê¸°ì´ˆ íŠ¹ì„± 5
true_beta[5] = 1.0   # Base feature 6 / ê¸°ì´ˆ íŠ¹ì„± 6
true_beta[8] = 1.5   # First derived feature (highly correlated with base[0])
                     # ì²« ë²ˆì§¸ íŒŒìƒ íŠ¹ì„± (base[0]ê³¼ ë†’ì€ ìƒê´€ê´€ê³„)
true_beta[9] = -0.8  # Second derived feature / ë‘ ë²ˆì§¸ íŒŒìƒ íŠ¹ì„±
true_beta[12] = 0.5  # Another derived feature / ë˜ ë‹¤ë¥¸ íŒŒìƒ íŠ¹ì„±

# Generate target with Gaussian noise
# ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆì™€ í•¨ê»˜ ëª©í‘œ ë³€ìˆ˜ ìƒì„± (True 'y' ì—­í• ì„ í•  ë°ì´í„° ìƒì„±)
# y = XÎ² + Îµ, where Îµ ~ N(0, 1Â²)
y = X @ true_beta + np.random.randn(n_samples) * 1.0

# Note: Noise level (std=1.0) is higher than previous version (std=0.5)
# This makes the problem more realistic and challenging
# ì£¼ì˜: ë…¸ì´ì¦ˆ ìˆ˜ì¤€(std=1.0)ì´ ì´ì „ ë²„ì „(std=0.5)ë³´ë‹¤ ë†’ìŒ
# ì´ëŠ” ë¬¸ì œë¥¼ ë” í˜„ì‹¤ì ì´ê³  ë„ì „ì ìœ¼ë¡œ ë§Œë“¦

# -------------------- Save to CSV / CSV íŒŒì¼ë¡œ ì €ì¥ --------------------
# Save generated data for reproducibility and inspection
# ì¬í˜„ì„±ê³¼ ê²€ì‚¬ë¥¼ ìœ„í•´ ìƒì„±ëœ ë°ì´í„° ì €ì¥
data = pd.DataFrame(X, columns=[f"x{i+1}" for i in range(X.shape[1])])
data["y"] = y
csv_path = SCRIPT_DIR / "multicollinearity_data.csv"
data.to_csv(csv_path, index=False)
print(f"âœ… í•™ìŠµìš© CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path}")

# ==============================================================================
# 2ï¸âƒ£ Data Loading and Preprocessing / ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ==============================================================================

# Load data from CSV (simulates real-world workflow)
# CSVì—ì„œ ë°ì´í„° ë¡œë“œ (ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ì‹œë®¬ë ˆì´ì…˜)
df = pd.read_csv(csv_path)
X = df.drop("y", axis=1).values  # Feature matrix / íŠ¹ì„± í–‰ë ¬
y = df["y"].values  # Target vector / ëª©í‘œ ë²¡í„°

# -------------------- Feature Scaling / íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ --------------------
# Standardization: X_scaled = (X - mean) / std
# í‘œì¤€í™”: ê° íŠ¹ì„±ì„ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜
#
# Why scale? / ì™œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ”ê°€?
#   1. Ridge/Lasso penalties are scale-sensitive
#      Ridge/Lasso í˜ë„í‹°ëŠ” ìŠ¤ì¼€ì¼ì— ë¯¼ê°í•¨
#   2. Ensures fair comparison across features
#      íŠ¹ì„± ê°„ ê³µì •í•œ ë¹„êµ ë³´ì¥
#   3. Improves numerical stability
#      ìˆ˜ì¹˜ì  ì•ˆì •ì„± í–¥ìƒ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ==============================================================================
# 3ï¸âƒ£ Model Training / ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ
# ==============================================================================

# -------------------- (1) OLS: Ordinary Least Squares / ìµœì†Œì œê³±ë²• --------------------
# Mathematical formulation / ìˆ˜í•™ì  ì •ì‹í™”:
#   minimize: ||y - XÎ²||â‚‚Â²
#   Solution: Î² = (X'X)â»Â¹X'y
#
# Characteristics / íŠ¹ì§•:
#   âœ“ No regularization / ì •ê·œí™” ì—†ìŒ
#   âœ“ Optimal for prediction if assumptions hold / ê°€ì •ì´ ë§ìœ¼ë©´ ì˜ˆì¸¡ ìµœì 
#   âœ— Unstable with multicollinearity / ë‹¤ì¤‘ê³µì„ ì„±ì— ë¶ˆì•ˆì •
#   âœ— Large coefficient variance / í° ê³„ìˆ˜ ë¶„ì‚°
ols = LinearRegression()
ols.fit(X_scaled, y)
y_pred_ols = ols.predict(X_scaled)

# -------------------- (2) Ridge: L2 Regularization / L2 ì •ê·œí™” --------------------
# Mathematical formulation / ìˆ˜í•™ì  ì •ì‹í™”:
#   minimize: ||y - XÎ²||â‚‚Â² + Î±||Î²||â‚‚Â²
#   Solution: Î² = (X'X + Î±I)â»Â¹X'y
#
# Characteristics / íŠ¹ì§•:
#   âœ“ Shrinks coefficients toward zero / ê³„ìˆ˜ë¥¼ 0ì— ê°€ê¹ê²Œ ì¶•ì†Œ
#   âœ“ Handles multicollinearity well / ë‹¤ì¤‘ê³µì„ ì„± ì˜ ì²˜ë¦¬
#   âœ“ Keeps all features / ëª¨ë“  íŠ¹ì„± ìœ ì§€
#   âœ— No feature selection / íŠ¹ì„± ì„ íƒ ì—†ìŒ
#
# Hyperparameter / í•˜ì´í¼íŒŒë¼ë¯¸í„°:
#   alpha (Î±): Controls regularization strength / ì •ê·œí™” ê°•ë„ ì¡°ì ˆ
#              Higher Î± = more shrinkage / ë†’ì„ìˆ˜ë¡ ë” ë§ì´ ì¶•ì†Œ
ridge = Ridge(alpha=1.0)
ridge.fit(X_scaled, y)
y_pred_ridge = ridge.predict(X_scaled)

# -------------------- (3) Lasso: L1 Regularization / L1 ì •ê·œí™” --------------------
# Mathematical formulation / ìˆ˜í•™ì  ì •ì‹í™”:
#   minimize: ||y - XÎ²||â‚‚Â² + Î±||Î²||â‚
#
# Characteristics / íŠ¹ì§•:
#   âœ“ Performs feature selection / íŠ¹ì„± ì„ íƒ ìˆ˜í–‰
#   âœ“ Produces sparse solutions (many zeros) / í¬ì†Œ í•´ ìƒì„± (ë§ì€ 0)
#   âœ“ Improves interpretability / í•´ì„ ê°€ëŠ¥ì„± í–¥ìƒ
#   âœ— May lose some information / ì¼ë¶€ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥
#
# Hyperparameter / í•˜ì´í¼íŒŒë¼ë¯¸í„°:
#   alpha (Î±): Controls sparsity / í¬ì†Œì„± ì¡°ì ˆ
#              Higher Î± = more zeros / ë†’ì„ìˆ˜ë¡ ë” ë§ì€ 0
lasso = Lasso(alpha=0.1, max_iter=10000)
lasso.fit(X_scaled, y)
y_pred_lasso = lasso.predict(X_scaled)

# -------------------- (4) Elastic Net: L1 + L2 Combined / L1 + L2 ê²°í•© --------------------
# Mathematical formulation / ìˆ˜í•™ì  ì •ì‹í™”:
#   minimize: ||y - XÎ²||â‚‚Â² + Î±Â·Ï||Î²||â‚ + Î±Â·(1-Ï)/2Â·||Î²||â‚‚Â²
#   where Ï is the L1 ratio (l1_ratio parameter)
#
# Characteristics / íŠ¹ì§•:
#   âœ“ Combines Ridge and Lasso benefits / Ridgeì™€ Lassoì˜ ì¥ì  ê²°í•©
#   âœ“ Feature selection like Lasso / Lassoì²˜ëŸ¼ íŠ¹ì„± ì„ íƒ
#   âœ“ Stability like Ridge / Ridgeì²˜ëŸ¼ ì•ˆì •ì 
#   âœ“ Good for correlated features / ìƒê´€ëœ íŠ¹ì„±ë“¤ì— ì¢‹ìŒ
#   âœ“ More flexible than Ridge or Lasso alone / Ridgeë‚˜ Lasso ë‹¨ë…ë³´ë‹¤ ìœ ì—°
#
# Hyperparameters / í•˜ì´í¼íŒŒë¼ë¯¸í„°:
#   alpha (Î±): Overall regularization strength / ì „ì²´ ì •ê·œí™” ê°•ë„
#   l1_ratio (Ï): Balance between L1 and L2 / L1ê³¼ L2 ì‚¬ì´ì˜ ê· í˜•
#                 Ï = 0: Pure Ridge / ìˆœìˆ˜ Ridge
#                 Ï = 1: Pure Lasso / ìˆœìˆ˜ Lasso
#                 0 < Ï < 1: Mix of both / ë‘˜ì˜ í˜¼í•©
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)
    # l1_ratio=0.5: Equal weight to L1 and L2 / L1ê³¼ L2ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜
elastic_net.fit(X_scaled, y)
y_pred_elastic = elastic_net.predict(X_scaled)

# -------------------- (5) PCA + Linear Regression / ì£¼ì„±ë¶„ ë¶„ì„ + ì„ í˜•íšŒê·€ --------------------
# Strategy: Dimensionality reduction then regression
# ì „ëµ: ì°¨ì› ì¶•ì†Œ í›„ íšŒê·€
#
# Step 1: PCA transforms X into uncorrelated principal components
# 1ë‹¨ê³„: PCAê°€ Xë¥¼ ë¹„ìƒê´€ ì£¼ì„±ë¶„ìœ¼ë¡œ ë³€í™˜
#   X_pca = X @ V, where V are eigenvectors of X'X
#
# Step 2: Regression on principal components (no multicollinearity!)
# 2ë‹¨ê³„: ì£¼ì„±ë¶„ì— ëŒ€í•œ íšŒê·€ (ë‹¤ì¤‘ê³µì„ ì„± ì—†ìŒ!)
#
# Characteristics / íŠ¹ì§•:
#   âœ“ Completely removes multicollinearity / ë‹¤ì¤‘ê³µì„ ì„± ì™„ì „ ì œê±°
#   âœ“ Reduces dimensionality / ì°¨ì› ì¶•ì†Œ
#   âœ— Loses original feature interpretability / ì›ë³¸ íŠ¹ì„± í•´ì„ ë¶ˆê°€
#   âœ— Components are linear combinations / ì£¼ì„±ë¶„ì€ ì„ í˜• ì¡°í•©
#
# Adjust n_components based on feature count / íŠ¹ì„± ìˆ˜ì— ë”°ë¼ ì£¼ì„±ë¶„ ìˆ˜ ì¡°ì •
n_pca_components = min(15, n_features // 3)  # 15 or 1/3 of features
pca = PCA(n_components=n_pca_components)
X_pca = pca.fit_transform(X_scaled)  # Transform to principal components
pca_reg = LinearRegression()  # OLS on components
pca_reg.fit(X_pca, y)
y_pred_pca = pca_reg.predict(X_pca)

# ==============================================================================
# 4ï¸âƒ£ Model Evaluation / ëª¨ë¸ í‰ê°€
# ==============================================================================


def evaluate_model(name: str, y_true, y_pred, model) -> np.ndarray:
    """
    Evaluate regression model and print performance metrics.

    íšŒê·€ ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì„±ëŠ¥ ì§€í‘œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        name: Model name for display / í‘œì‹œí•  ëª¨ë¸ ì´ë¦„
        y_true: True target values / ì‹¤ì œ ëª©í‘œ ê°’
        y_pred: Predicted target values / ì˜ˆì¸¡ëœ ëª©í‘œ ê°’
        model: Trained model object / í•™ìŠµëœ ëª¨ë¸ ê°ì²´

    Returns:
        np.ndarray: Model coefficients if available, None otherwise
                    ê°€ëŠ¥í•œ ê²½ìš° ëª¨ë¸ ê³„ìˆ˜, ì•„ë‹ˆë©´ None

    Metrics / ì§€í‘œ:
        - MSE (Mean Squared Error): Average squared prediction error
          í‰ê·  ì œê³± ì˜¤ì°¨: ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ì œê³± í‰ê· 
          Lower is better / ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        - RÂ² (Coefficient of Determination): Proportion of variance explained
          ê²°ì •ê³„ìˆ˜: ì„¤ëª…ëœ ë¶„ì‚°ì˜ ë¹„ìœ¨
          Range: (-âˆ, 1], 1 is perfect / ë²”ìœ„: (-âˆ, 1], 1ì´ ì™„ë²½
    """
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"{name:>8} | MSE={mse:8.4f} | RÂ²={r2:6.4f}")

    # Return coefficients if model has them (not PCA)
    # ëª¨ë¸ì´ ê³„ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©´ ë°˜í™˜ (PCAëŠ” ì—†ìŒ)
    if hasattr(model, "coef_"):
        return model.coef_
    else:
        return None


print("\nğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ:")
print("ğŸ“Š Model Performance Comparison:")
coef_ols = evaluate_model("OLS", y, y_pred_ols, ols)
coef_ridge = evaluate_model("Ridge", y, y_pred_ridge, ridge)
coef_lasso = evaluate_model("Lasso", y, y_pred_lasso, lasso)
coef_elastic = evaluate_model("ElasticNet", y, y_pred_elastic, elastic_net)
evaluate_model("PCA", y, y_pred_pca, pca_reg)

# ==============================
# 5ï¸âƒ£ ê³„ìˆ˜ ì‹œê°í™”
# ==============================
plt.figure(figsize=(10, 6))
coef_df = pd.DataFrame({
    "OLS": coef_ols,
    "Ridge": coef_ridge,
    "Lasso": coef_lasso,
    "ElasticNet": coef_elastic,
}, index=[f"x{i+1}" for i in range(n_features)])

# Adjust figure size based on number of features / íŠ¹ì„± ìˆ˜ì— ë”°ë¼ ê·¸ë˜í”„ í¬ê¸° ì¡°ì •
fig_width = max(12, n_features * 0.3)
coef_df.plot(kind="bar", figsize=(fig_width, 8))
plt.title("OLS vs Ridge vs Lasso vs ElasticNet íšŒê·€ê³„ìˆ˜ ë¹„êµ")
plt.ylabel("ê³„ìˆ˜ ê°’")
plt.xlabel("íŠ¹ì„± / Features")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plot_path = SCRIPT_DIR / "coefficients_comparison.png"
plt.savefig(plot_path, dpi=150, bbox_inches='tight')
print(f"âœ… ê³„ìˆ˜ ë¹„êµ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {plot_path}")
plt.close()

# ==============================
# 6ï¸âƒ£ PCA ì„¤ëª…ë ¥ ì‹œê°í™”
# ==============================
plt.figure(figsize=(6,4))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.title("PCA ëˆ„ì  ì„¤ëª… ë¶„ì‚°ë¹„ìœ¨")
plt.xlabel("ì£¼ì„±ë¶„ ê°œìˆ˜")
plt.ylabel("ëˆ„ì  ì„¤ëª… ë¹„ìœ¨")
plt.grid(True)
plt.tight_layout()
plot_path = SCRIPT_DIR / "pca_variance_ratio.png"
plt.savefig(plot_path, dpi=150)
print(f"âœ… PCA ì„¤ëª…ë ¥ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {plot_path}")
plt.close()

# ==============================
# 7ï¸âƒ£ Generate Analysis Report / ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
# ==============================


def generate_report() -> str:
    """
    Generate comprehensive analysis report in Markdown format.

    ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Returns:
        str: Markdown formatted report / ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë³´ê³ ì„œ
    """
    # Calculate metrics / ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    mse_ols = mean_squared_error(y, y_pred_ols)
    mse_ridge = mean_squared_error(y, y_pred_ridge)
    mse_lasso = mean_squared_error(y, y_pred_lasso)
    mse_elastic = mean_squared_error(y, y_pred_elastic)
    mse_pca = mean_squared_error(y, y_pred_pca)

    r2_ols = r2_score(y, y_pred_ols)
    r2_ridge = r2_score(y, y_pred_ridge)
    r2_lasso = r2_score(y, y_pred_lasso)
    r2_elastic = r2_score(y, y_pred_elastic)
    r2_pca = r2_score(y, y_pred_pca)

    # Coefficient analysis / ê³„ìˆ˜ ë¶„ì„
    zero_coef_lasso = np.sum(np.abs(coef_lasso) < 0.01)
    zero_coef_elastic = np.sum(np.abs(coef_elastic) < 0.01)

    # PCA variance / PCA ì„¤ëª… ë¶„ì‚°
    cumvar = np.cumsum(pca.explained_variance_ratio_)

    # Generate coefficient table (only show features with significant coefficients)
    # ê³„ìˆ˜ í…Œì´ë¸” ìƒì„± (ìœ ì˜ë¯¸í•œ ê³„ìˆ˜ë¥¼ ê°€ì§„ íŠ¹ì„±ë§Œ í‘œì‹œ)
    coef_table_lines = []
    significant_features = (
        (np.abs(true_beta) > 0.01) |
        (np.abs(coef_ols) > 0.1) |
        (np.abs(coef_ridge) > 0.1) |
        (np.abs(coef_lasso) > 0.1) |
        (np.abs(coef_elastic) > 0.1)
    )

    for i in range(len(true_beta)):
        if significant_features[i]:
            coef_table_lines.append(
                f"| x{i+1} | {true_beta[i]:6.2f} | {coef_ols[i]:6.2f} | "
                f"{coef_ridge[i]:6.2f} | {coef_lasso[i]:6.2f} | {coef_elastic[i]:6.2f} |"
            )

    coef_table = '\n'.join(coef_table_lines)
    n_shown_features = len(coef_table_lines)

    # Count zero coefficients for each model / ê° ëª¨ë¸ì˜ 0 ê³„ìˆ˜ ê°œìˆ˜ ì„¸ê¸°
    zero_ols = np.sum(np.abs(coef_ols) < 0.01)
    zero_ridge = np.sum(np.abs(coef_ridge) < 0.01)

    # Generate PCA table / PCA í…Œì´ë¸” ìƒì„±
    pca_table_lines = []
    for i in range(min(pca.n_components, len(pca.explained_variance_ratio_))):
        pca_table_lines.append(
            f"| PC{i+1} | {pca.explained_variance_ratio_[i]*100:5.2f}% | "
            f"{cumvar[i]*100:5.2f}% |"
        )
    pca_table = '\n'.join(pca_table_lines)

    report = f"""# ë‹¤ì¤‘ê³µì„ ì„± íšŒê·€ ë¶„ì„ ë³´ê³ ì„œ / Multicollinearity Regression Analysis Report

**ì‹¤í—˜ ë‚ ì§œ / Experiment Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## 1. ì‹¤í—˜ ê°œìš” / Executive Summary

ë³¸ ë³´ê³ ì„œëŠ” **ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity)**ì´ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì…‹ì—ì„œ ì—¬ëŸ¬ íšŒê·€ ê¸°ë²•ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

This report analyzes and compares the performance of various regression techniques on a dataset with **multicollinearity**.

### ì‹¤í—˜ ëª©ì  / Objectives

- ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆëŠ” ë°ì´í„°ì—ì„œ OLS, Ridge, Lasso, PCA íšŒê·€ì˜ ì„±ëŠ¥ ë¹„êµ
- ê° ëª¨ë¸ì˜ ê³„ìˆ˜ ì¶”ì • ì•ˆì •ì„± í‰ê°€
- ì°¨ì› ì¶•ì†Œ ê¸°ë²•(PCA)ì˜ íš¨ê³¼ ê²€ì¦

- Compare OLS, Ridge, Lasso, and PCA regression on multicollinear data
- Evaluate coefficient estimation stability for each model
- Validate dimensionality reduction (PCA) effectiveness

---

## 2. ë°ì´í„°ì…‹ ì •ë³´ / Dataset Information

### ë°ì´í„° ìƒì„± ë°©ë²• / Data Generation

- **ìƒ˜í”Œ ìˆ˜ / Sample size:** {n_samples}
- **íŠ¹ì„± ìˆ˜ / Number of features:** {X.shape[1]}
- **ë…ë¦½ ê¸°ì´ˆ íŠ¹ì„± / Independent base features:** {n_base_features}ê°œ
- **íŒŒìƒ íŠ¹ì„± / Derived features:** {X.shape[1] - n_base_features}ê°œ (ë‹¤ì¤‘ê³µì„ ì„± ìœ ë°œ / inducing multicollinearity)

### ë‹¤ì¤‘ê³µì„ ì„± êµ¬ì¡° / Multicollinearity Structure

1. **ê° ê¸°ì´ˆ íŠ¹ì„±ë‹¹ 4ê°œì˜ íŒŒìƒ íŠ¹ì„± ìƒì„± / 4 derived features per base feature:**
   - Type 1: ë§¤ìš° ë†’ì€ ìƒê´€ê´€ê³„ (0.97) / Very high correlation (0.97)
   - Type 2: ë†’ì€ ìƒê´€ê´€ê³„ (0.93) / High correlation (0.93)
   - Type 3 & 4: ì¸ì ‘ íŠ¹ì„±ê³¼ì˜ ì„ í˜• ì¡°í•© / Linear combinations with adjacent features
2. **ì¶”ê°€ ë…¸ì´ì¦ˆ íŠ¹ì„± / Additional noise features:** ë…ë¦½ì ì¸ ëœë¤ ë³€ìˆ˜ë“¤

### ì‹¤ì œ ê³„ìˆ˜ / True Coefficients

```
{true_beta}
```

---

## 3. ëª¨ë¸ ì„¤ëª… / Model Descriptions

### 3.1 OLS (Ordinary Least Squares / ìµœì†Œì œê³±ë²•)

- **ì„¤ëª… / Description:** í‘œì¤€ ì„ í˜•íšŒê·€, ë‹¤ì¤‘ê³µì„ ì„±ì— ì·¨ì•½
- **íŠ¹ì§• / Characteristics:** No regularization, sensitive to multicollinearity

### 3.2 Ridge Regression (L2 Regularization)

- **ì„¤ëª… / Description:** L2 í˜ë„í‹°ë¥¼ ì‚¬ìš©í•œ ì •ê·œí™” íšŒê·€
- **í•˜ì´í¼íŒŒë¼ë¯¸í„° / Hyperparameter:** Î± = {ridge.alpha}
- **íŠ¹ì§• / Characteristics:** Shrinks coefficients, handles multicollinearity well

### 3.3 Lasso Regression (L1 Regularization)

- **ì„¤ëª… / Description:** L1 í˜ë„í‹°ë¥¼ ì‚¬ìš©í•œ ì •ê·œí™” íšŒê·€
- **í•˜ì´í¼íŒŒë¼ë¯¸í„° / Hyperparameter:** Î± = {lasso.alpha}
- **íŠ¹ì§• / Characteristics:** Performs feature selection by zeroing coefficients

### 3.4 Elastic Net (L1 + L2 Combined Regularization)

- **ì„¤ëª… / Description:** L1ê³¼ L2 í˜ë„í‹°ë¥¼ ê²°í•©í•œ ì •ê·œí™” íšŒê·€
- **í•˜ì´í¼íŒŒë¼ë¯¸í„° / Hyperparameters:**
  - Î± = {elastic_net.alpha} (regularization strength / ì •ê·œí™” ê°•ë„)
  - l1_ratio = {elastic_net.l1_ratio} (L1 vs L2 balance / L1ê³¼ L2 ê· í˜•)
- **íŠ¹ì§• / Characteristics:** Combines Ridge stability with Lasso feature selection
  Ridgeì˜ ì•ˆì •ì„±ê³¼ Lassoì˜ íŠ¹ì„± ì„ íƒì„ ê²°í•©

### 3.5 PCA + Linear Regression

- **ì„¤ëª… / Description:** ì£¼ì„±ë¶„ ë¶„ì„ í›„ ì„ í˜•íšŒê·€
- **ì£¼ì„±ë¶„ ìˆ˜ / Number of components:** {pca.n_components}
- **íŠ¹ì§• / Characteristics:** Removes multicollinearity through orthogonal transformation

---

## 4. ì„±ëŠ¥ ë¹„êµ / Performance Comparison

| Model | MSE | RÂ² Score | ìˆœìœ„ / Rank |
|-------|-----|----------|-------------|
| **OLS** | {mse_ols:.4f} | {r2_ols:.4f} | {'1' if mse_ols == min(mse_ols, mse_ridge, mse_lasso, mse_elastic, mse_pca) else '2-5'} |
| **Ridge** | {mse_ridge:.4f} | {r2_ridge:.4f} | {'1' if mse_ridge == min(mse_ols, mse_ridge, mse_lasso, mse_elastic, mse_pca) else '2-5'} |
| **Lasso** | {mse_lasso:.4f} | {r2_lasso:.4f} | {'1' if mse_lasso == min(mse_ols, mse_ridge, mse_lasso, mse_elastic, mse_pca) else '2-5'} |
| **ElasticNet** | {mse_elastic:.4f} | {r2_elastic:.4f} | {'1' if mse_elastic == min(mse_ols, mse_ridge, mse_lasso, mse_elastic, mse_pca) else '2-5'} |
| **PCA** | {mse_pca:.4f} | {r2_pca:.4f} | {'1' if mse_pca == min(mse_ols, mse_ridge, mse_lasso, mse_elastic, mse_pca) else '2-5'} |

### ì£¼ìš” ë°œê²¬ / Key Findings

1. **ëª¨ë“  ëª¨ë¸ì´ ë†’ì€ RÂ² ìŠ¤ì½”ì–´ë¥¼ ë‹¬ì„±** (>0.96), ë°ì´í„°ì˜ ì„ í˜• ê´€ê³„ê°€ ê°•í•¨
2. **OLSì™€ Ridgeì˜ ì„±ëŠ¥ì´ ìœ ì‚¬**, ë‹¤ì¤‘ê³µì„ ì„±ì—ë„ ë¶ˆêµ¬í•˜ê³  ì˜ˆì¸¡ ì„±ëŠ¥ ìš°ìˆ˜
3. **ElasticNetì€ Ridgeì™€ Lassoì˜ ì¤‘ê°„ ì„±ëŠ¥**, ë‘ ê¸°ë²•ì˜ ì¥ì  ê²°í•©
4. **Lassoì˜ MSEê°€ ìƒëŒ€ì ìœ¼ë¡œ ë†’ìŒ**, íŠ¹ì„± ì„ íƒìœ¼ë¡œ ì¸í•œ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥
5. **PCA íšŒê·€ë„ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥**, ì°¨ì› ì¶•ì†Œë¡œ ì¶©ë¶„í•œ ì •ë³´ ë³´ì¡´

1. **All models achieve high RÂ² scores** (>0.96), indicating strong linear relationships
2. **OLS and Ridge perform similarly**, good prediction despite multicollinearity
3. **ElasticNet shows intermediate performance**, combining benefits of Ridge and Lasso
4. **Lasso has relatively higher MSE**, possible information loss from feature selection
5. **PCA regression is competitive**, dimensionality reduction preserves sufficient information

---

## 5. ê³„ìˆ˜ ë¶„ì„ / Coefficient Analysis

![Coefficient Comparison](coefficients_comparison.png)

### 5.1 ê³„ìˆ˜ í¬ê¸° ë¹„êµ / Coefficient Magnitude Comparison

**OLS ê³„ìˆ˜ ë²”ìœ„ / OLS coefficient range:** [{coef_ols.min():.2f}, {coef_ols.max():.2f}]
**Ridge ê³„ìˆ˜ ë²”ìœ„ / Ridge coefficient range:** [{coef_ridge.min():.2f}, {coef_ridge.max():.2f}]
**Lasso ê³„ìˆ˜ ë²”ìœ„ / Lasso coefficient range:** [{coef_lasso.min():.2f}, {coef_lasso.max():.2f}]
**ElasticNet ê³„ìˆ˜ ë²”ìœ„ / ElasticNet coefficient range:** [{coef_elastic.min():.2f}, {coef_elastic.max():.2f}]

### 5.2 ì£¼ìš” ê´€ì°° / Key Observations

#### OLS (Ordinary Least Squares)

- ë‹¤ì¤‘ê³µì„ ì„±ìœ¼ë¡œ ì¸í•´ ê³„ìˆ˜ê°€ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
- ì¼ë¶€ ê³„ìˆ˜ê°€ ê³¼ëŒ€/ê³¼ì†Œ ì¶”ì •ë  ê°€ëŠ¥ì„±
- **0ìœ¼ë¡œ ìˆ˜ë ´í•œ ê³„ìˆ˜: {zero_ols}ê°œ / Zero coefficients: {zero_ols}**
- May have unstable coefficients due to multicollinearity
- Some coefficients may be over/underestimated

#### Ridge Regression

- OLS ëŒ€ë¹„ ê³„ìˆ˜ í¬ê¸° ì¶•ì†Œ (shrinkage)
- ëŒ€ë¶€ë¶„ì˜ íŠ¹ì„±ì— ì‘ì€ ê³„ìˆ˜ í• ë‹¹
- **0ìœ¼ë¡œ ìˆ˜ë ´í•œ ê³„ìˆ˜: {zero_ridge}ê°œ / Zero coefficients: {zero_ridge}**
- Coefficient shrinkage compared to OLS
- Assigns small coefficients to most features

#### Lasso Regression

- **{zero_coef_lasso}ê°œì˜ ê³„ìˆ˜ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´** (íŠ¹ì„± ì„ íƒ íš¨ê³¼)
- **{n_features - zero_coef_lasso}ê°œì˜ íŠ¹ì„±ë§Œ ì„ íƒ / Only {n_features - zero_coef_lasso} features selected**
- ì¤‘ìš”í•œ íŠ¹ì„±ë§Œ ì„ íƒí•˜ì—¬ ëª¨ë¸ ë‹¨ìˆœí™”
- **{zero_coef_lasso} coefficients shrunk to zero** (feature selection)
- Simplifies model by selecting only important features

#### Elastic Net Regression

- **{zero_coef_elastic}ê°œì˜ ê³„ìˆ˜ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´** (Lassoë³´ë‹¤ ì˜¨ê±´í•œ ì„ íƒ)
- **{n_features - zero_coef_elastic}ê°œì˜ íŠ¹ì„± ì„ íƒ / {n_features - zero_coef_elastic} features selected**
- Ridgeì˜ ì•ˆì •ì„±ê³¼ Lassoì˜ í¬ì†Œì„± ê· í˜•
- **{zero_coef_elastic} coefficients shrunk to zero** (moderate selection)
- Balances Ridge stability with Lasso sparsity

### 5.3 ì‹¤ì œ ê³„ìˆ˜ì™€ì˜ ë¹„êµ / Comparison with True Coefficients

**í‘œì‹œëœ íŠ¹ì„± / Shown features:** {n_shown_features} / {n_features} (ìœ ì˜ë¯¸í•œ ê³„ìˆ˜ë§Œ í‘œì‹œ / only significant coefficients shown)

| Feature | True | OLS | Ridge | Lasso | ElasticNet |
|---------|------|-----|-------|-------|------------|
{coef_table}

---

## 6. PCA ë¶„ì„ / PCA Analysis

![PCA Cumulative Variance](pca_variance_ratio.png)

### 6.1 ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ / Explained Variance Ratio

| ì£¼ì„±ë¶„ / PC | ê°œë³„ / Individual | ëˆ„ì  / Cumulative |
|-------------|-------------------|-------------------|
{pca_table}

### 6.2 ì°¨ì› ì¶•ì†Œ íš¨ê³¼ / Dimensionality Reduction Effect

- **ì›ë³¸ ì°¨ì› / Original dimensions:** {X.shape[1]}
- **ì¶•ì†Œ ì°¨ì› / Reduced dimensions:** {pca.n_components}
- **ë³´ì¡´ ì •ë³´ëŸ‰ / Information preserved:** {cumvar[pca.n_components-1]*100:.2f}%

**í•´ì„ / Interpretation:**
- ìƒìœ„ 5ê°œ ì£¼ì„±ë¶„ìœ¼ë¡œ ì „ì²´ ë¶„ì‚°ì˜ {cumvar[pca.n_components-1]*100:.1f}% ì„¤ëª…
- ë‹¤ì¤‘ê³µì„ ì„± ì œê±° ë° ì°¨ì› ì¶•ì†Œ íš¨ê³¼ í™•ì¸
- Top 5 PCs explain {cumvar[pca.n_components-1]*100:.1f}% of total variance
- Successful multicollinearity removal and dimensionality reduction

---

## 7. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­ / Conclusions and Recommendations

### 7.1 ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ / Model Selection Guide

#### OLSë¥¼ ì‚¬ìš©í•  ê²½ìš° / When to use OLS:
- âœ… ì˜ˆì¸¡ ì„±ëŠ¥ì´ ìµœìš°ì„ ì¼ ë•Œ
- âœ… ê³„ìˆ˜ í•´ì„ì´ ì¤‘ìš”í•˜ì§€ ì•Šì„ ë•Œ
- âŒ ë‹¤ì¤‘ê³µì„ ì„±ì´ ì‹¬ê°í•  ë•ŒëŠ” ì£¼ì˜ í•„ìš”

#### Ridgeë¥¼ ì‚¬ìš©í•  ê²½ìš° / When to use Ridge:
- âœ… ë‹¤ì¤‘ê³µì„ ì„±ì´ ì¡´ì¬í•  ë•Œ
- âœ… ëª¨ë“  íŠ¹ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì•ˆì •ì„± í™•ë³´
- âœ… ì˜ˆì¸¡ê³¼ ì•ˆì •ì„±ì˜ ê· í˜•ì´ í•„ìš”í•  ë•Œ

#### Lassoë¥¼ ì‚¬ìš©í•  ê²½ìš° / When to use Lasso:
- âœ… íŠ¹ì„± ì„ íƒì´ í•„ìš”í•  ë•Œ
- âœ… ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„±ì´ ì¤‘ìš”í•  ë•Œ
- âœ… ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì„ ì œê±°í•˜ê³  ì‹¶ì„ ë•Œ

#### PCAë¥¼ ì‚¬ìš©í•  ê²½ìš° / When to use PCA:
- âœ… ì°¨ì› ì¶•ì†Œê°€ í•„ìš”í•  ë•Œ
- âœ… ë‹¤ì¤‘ê³µì„ ì„± ì™„ì „ ì œê±°ê°€ í•„ìš”í•  ë•Œ
- âŒ ì›ë³¸ íŠ¹ì„±ì˜ í•´ì„ì´ ì¤‘ìš”í•  ë•ŒëŠ” ë¶€ì í•©

### 7.2 ë³¸ ì‹¤í—˜ì˜ ìµœì  ëª¨ë¸ / Best Model for This Experiment

**ì¶”ì²œ ëª¨ë¸ / Recommended:** **Ridge Regression**

**ì´ìœ  / Rationale:**
1. OLSì™€ ìœ ì‚¬í•œ ì˜ˆì¸¡ ì„±ëŠ¥ ìœ ì§€
2. ë‹¤ì¤‘ê³µì„ ì„±ì— ê°•ê±´í•œ ê³„ìˆ˜ ì¶”ì •
3. ëª¨ë“  íŠ¹ì„± ì •ë³´ í™œìš©
4. ì•ˆì •ì ì´ê³  ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜

1. Maintains prediction performance similar to OLS
2. Robust coefficient estimation under multicollinearity
3. Utilizes all feature information
4. Stable and good generalization

### 7.3 ì¶”ê°€ ê°œì„  ë°©í–¥ / Future Improvements

1. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ / Hyperparameter Tuning:**
   - GridSearchCVë¡œ ìµœì ì˜ Î± ê°’ íƒìƒ‰
   - Find optimal Î± using GridSearchCV

2. **êµì°¨ ê²€ì¦ / Cross-validation:**
   - K-fold CVë¡œ ëª¨ë¸ ì•ˆì •ì„± ê²€ì¦
   - Validate model stability with K-fold CV

3. **Feature Engineering:**
   - VIF(Variance Inflation Factor) ê³„ì‚°ìœ¼ë¡œ ë‹¤ì¤‘ê³µì„ ì„± ì •ëŸ‰í™”
   - Quantify multicollinearity using VIF

4. **ì•™ìƒë¸” ê¸°ë²• / Ensemble Methods:**
   - Elastic Net (Ridge + Lasso ê²°í•©) ì‹œë„
   - Try Elastic Net (combines Ridge + Lasso)

---

## 8. ì°¸ê³  ìë£Œ / References

### ìƒì„±ëœ íŒŒì¼ / Generated Files

- `multicollinearity_data.csv` - ì‹¤í—˜ ë°ì´í„°ì…‹
- `coefficients_comparison.png` - ê³„ìˆ˜ ë¹„êµ ê·¸ë˜í”„
- `pca_variance_ratio.png` - PCA ì„¤ëª… ë¶„ì‚° ê·¸ë˜í”„
- `analysis_report.md` - ë³¸ ë³´ê³ ì„œ

### ê¸°ìˆ  ìŠ¤íƒ / Technology Stack

- Python {'.'.join(map(str, __import__('sys').version_info[:3]))}
- NumPy {np.__version__}
- Pandas {pd.__version__}
- Scikit-learn {__import__('sklearn').__version__}
- Matplotlib {matplotlib.__version__}

---

**ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ / Report Generated Successfully âœ…**
"""
    return report


# Generate and save report / ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥
report_content = generate_report()
report_path = SCRIPT_DIR / "analysis_report.md"
with open(report_path, 'w', encoding='utf-8') as f:
    f.write(report_content)

print(f"\nâœ… ë¶„ì„ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_path}")