{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7Jh-RVfNW3E"
   },
   "source": [
    "# ì „ë ¥ ë³€ì••ê¸° ì˜¤ì¼ ì˜¨ë„ ì˜ˆì¸¡ í”„ë¡œì íŠ¸\n",
    "# Power Transformer Oil Temperature Prediction Project\n",
    "\n",
    "## << ë¬¸ì œ ì •ì˜ / Problem Definition >>\n",
    "\n",
    "ì£¼ì–´ì§„ ETDatasetì„ ì‚¬ìš©í•˜ì—¬ ì „ë ¥ ë³€ì••ê¸°ì˜ **ì˜¤ì¼ ì˜¨ë„(OT, Oil Temperature)**ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ í’‰ë‹ˆë‹¤.\n",
    "Using the ETDataset, we solve the problem of predicting the **Oil Temperature (OT)** of power transformers.\n",
    "\n",
    "ì£¼ì–´ì§„ ë°ì´í„°ëŠ” ì´ 3ê°œì˜ CSV íŒŒì¼ì…ë‹ˆë‹¤. ê° CSV íŒŒì¼ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ì— ê¸°ìˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "The given data consists of 3 CSV files. Descriptions for each file are provided below.\n",
    "\n",
    "### í‰ê°€ ì§€í‘œ / Evaluation Metric\n",
    "- **RMSE (Root Mean Squared Error)**: ê° ì‹œê°„ëŒ€ë³„ë¡œ ì˜ˆì¸¡í•œ OTì™€ ì‹¤ì œ OT ì‚¬ì´ì˜ ì˜¤ì°¨\n",
    "- **ëª©í‘œ / Target**: RMSE < 0.5\n",
    "\n",
    "### ë°ì´í„° ì„¤ëª… / Data Description\n",
    "\n",
    "#### 1. train.csv (í•™ìŠµ ë°ì´í„° / Training Data)\n",
    "- **ê¸°ê°„ / Period**: 2016ë…„ 7ì›” 1ì¼ 0ì‹œ ~ 2017ë…„ 12ì›” 31ì¼ 23ì‹œ 45ë¶„\n",
    "- **ì»¬ëŸ¼ / Columns**: date, HUFL, HULL, MUFL, MULL, LUFL, LULL, OT\n",
    "- **íŠ¹ì„± ì„¤ëª… / Feature Description**:\n",
    "  - **HUFL**: High UseFul Load (ê³ ë¶€í•˜ ìœ íš¨ ì „ë ¥)\n",
    "  - **HULL**: High UseLess Load (ê³ ë¶€í•˜ ë¬´íš¨ ì „ë ¥)\n",
    "  - **MUFL**: Middle UseFul Load (ì¤‘ë¶€í•˜ ìœ íš¨ ì „ë ¥)\n",
    "  - **MULL**: Middle UseLess Load (ì¤‘ë¶€í•˜ ë¬´íš¨ ì „ë ¥)\n",
    "  - **LUFL**: Low UseFul Load (ì €ë¶€í•˜ ìœ íš¨ ì „ë ¥)\n",
    "  - **LULL**: Low UseLess Load (ì €ë¶€í•˜ ë¬´íš¨ ì „ë ¥)\n",
    "  - **OT**: Oil Temperature (ì˜¤ì¼ ì˜¨ë„) - **íƒ€ê²Ÿ ë³€ìˆ˜ / Target Variable**\n",
    "\n",
    "#### 2. test.csv (í…ŒìŠ¤íŠ¸ ë°ì´í„° / Test Data)\n",
    "- **ê¸°ê°„ / Period**: 2018ë…„ 1ì›” 1ì¼ 0ì‹œ ~ 2018ë…„ 6ì›” 30ì¼ 23ì‹œ 45ë¶„\n",
    "- **ì»¬ëŸ¼ / Columns**: date, HUFL, HULL, MUFL, MULL, LUFL, LULL (OT ì œì™¸)\n",
    "\n",
    "#### 3. submission.csv (ì œì¶œ íŒŒì¼ / Submission File)\n",
    "- ì˜ˆì¸¡í•œ OT ê°’ì„ ê¸°ë¡í•˜ëŠ” íŒŒì¼\n",
    "\n",
    "### ì ‘ê·¼ ë°©ë²• / Approach\n",
    "1. **Feature Engineering**: ì‹œê°„ íŠ¹ì„±, ìˆœí™˜ íŠ¹ì„±, ì§€ì—° íŠ¹ì„±, ë¡¤ë§ íŠ¹ì„± ìƒì„±\n",
    "2. **LightGBM**: í…Œì´ë¸” ë°ì´í„°ì— ì í•©í•œ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ëª¨ë¸\n",
    "3. **GRU (Deep Learning)**: ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ ìˆœí™˜ ì‹ ê²½ë§\n",
    "4. **Ensemble**: ëª¨ë¸ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ í‰ê· "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wwW5j4rM9DgL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LapI63BSMbQI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Using device: cuda\n",
      "ğŸ² Random seed set: 42\n",
      "ğŸ“ Default data path: dataset/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ / Library Imports\n",
    "# ============================================================\n",
    "# ì´ ì…€ì—ì„œëŠ” í”„ë¡œì íŠ¸ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "# This cell imports all necessary libraries for the project.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ / Standard Libraries\n",
    "# ------------------------------------------------------------\n",
    "import pandas as pd  # ë°ì´í„° ì²˜ë¦¬ / Data manipulation\n",
    "import numpy as np   # ìˆ˜ì¹˜ ì—°ì‚° / Numerical operations\n",
    "import matplotlib.pyplot as plt  # ì‹œê°í™” / Visualization\n",
    "import seaborn as sns  # ê³ ê¸‰ ì‹œê°í™” / Advanced visualization\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ì§„í–‰ë¥  í‘œì‹œ / Progress Display\n",
    "# ------------------------------------------------------------\n",
    "from tqdm.auto import tqdm  # í•™ìŠµ ì§„í–‰ë¥  í‘œì‹œ / Training progress bar\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ / Machine Learning Libraries\n",
    "# ------------------------------------------------------------\n",
    "import lightgbm as lgb  # LightGBM: ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ëª¨ë¸ / Gradient boosting model\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # ë°ì´í„° ë¶„í•  / Data splitting\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler  # ìŠ¤ì¼€ì¼ë§ / Scaling\n",
    "from sklearn.metrics import mean_squared_error  # í‰ê°€ ì§€í‘œ / Evaluation metric\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ (PyTorch) / Deep Learning Libraries (PyTorch)\n",
    "# ------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # ğŸ”§ ì¶”ê°€: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ / LR scheduler\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” / Hyperparameter Optimization\n",
    "# ------------------------------------------------------------\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ìœ í‹¸ë¦¬í‹° / Utilities\n",
    "# ------------------------------------------------------------\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# í™˜ê²½ ì„¤ì • / Environment Configuration\n",
    "# ============================================================\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •: GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ CUDA, ì•„ë‹ˆë©´ CPU\n",
    "# Device configuration: Use CUDA if GPU available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì • / Set seed for reproducibility\n",
    "# ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ëª¨ë“  ë‚œìˆ˜ ìƒì„±ê¸°ì— ì‹œë“œ ì„¤ì •\n",
    "# Set seed for all random number generators to ensure consistent results\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    # ğŸ”§ ì¶”ê°€: CUDA ê²°ì •ì  ì—°ì‚° / Deterministic CUDA operations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"ğŸ² Random seed set: {RANDOM_SEED}\")\n",
    "\n",
    "# ë°ì´í„°ëŠ” 15ë¶„ ê°„ê²©ìœ¼ë¡œ ê¸°ë¡ë¨ / Data is recorded at 15-minute intervals\n",
    "# 1ì‹œê°„ = 4 íƒ€ì„ìŠ¤í…, 1ì¼ = 96 íƒ€ì„ìŠ¤í…\n",
    "# 1 hour = 4 timesteps, 1 day = 96 timesteps\n",
    "INTERVALS_PER_HOUR = 4\n",
    "INTERVALS_PER_DAY = 24 * INTERVALS_PER_HOUR  # 96\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì • / Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# ê¸°ë³¸ ë°ì´í„° ê²½ë¡œ ì„¤ì • (ë¡œì»¬ í™˜ê²½ìš©) / Default data path (for local environment)\n",
    "DATA_PATH = 'dataset/'\n",
    "print(f\"ğŸ“ Default data path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g9EkKrD-6_NK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Data path: dataset/\n"
     ]
    }
   ],
   "source": [
    "# Google Colab í™˜ê²½ ì„¤ì • | Google Colab environment setup\n",
    "# ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ì´ ì…€ì„ ê±´ë„ˆë›°ì„¸ìš” | Skip this cell in local environment\n",
    "\n",
    "# Colab í™˜ê²½ í™•ì¸ | Check if running in Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Colabì—ì„œ ë°ì´í„° ê²½ë¡œ ì„¤ì • | Set data path for Colab\n",
    "    # ì•„ë˜ ê²½ë¡œë¥¼ ìì‹ ì˜ Google Drive ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”\n",
    "    # Modify the path below to match your Google Drive path\n",
    "    DATA_PATH = '/content/drive/MyDrive/your_path_here/dataset/'\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    # ë¡œì»¬ í™˜ê²½ | Local environment\n",
    "    DATA_PATH = 'dataset/'\n",
    "    IN_COLAB = False\n",
    "    print(\"Running in local environment\")\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v_ujn4hNW3H"
   },
   "source": [
    "### Q1. train.csvì™€ test.csvë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ê° ë°ì´í„°ì˜ shapeì„ ì¶œë ¥í•˜ì„¸ìš”. ë˜í•œ, ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ê° ì—´ë³„ë¡œ ê²°ì¸¡ì¹˜ì˜ ê°œìˆ˜ë¥¼ ì¶œë ¥í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHGYGA14Ol74"
   },
   "outputs": [],
   "source": "# ============================================================\n# A1. ë°ì´í„° ë¡œë“œ ë° ì¢…í•© ê²€ì¦ | Load Data and Comprehensive Validation\n# ============================================================\n# ê³ í’ˆì§ˆ ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n# For high-quality data analysis, we perform:\n# 1. ë°ì´í„° ë¡œë“œ ë° Shape í™•ì¸ / Load data and check shape\n# 2. ê²°ì¸¡ì¹˜ í™•ì¸ / Check missing values\n# 3. ë°ì´í„° íƒ€ì… ê²€ì¦ / Data type validation\n# 4. ê°’ ë²”ìœ„ í™•ì¸ (ì´ìƒì¹˜ íƒì§€) / Value range check (anomaly detection)\n# 5. ê¸°ìˆ  í†µê³„ëŸ‰ ì¶œë ¥ / Descriptive statistics\n# 6. íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ì‹œê°í™” / Target variable distribution visualization\n\n# CSV íŒŒì¼ ë¡œë“œ | Load CSV files\ntrain = pd.read_csv(DATA_PATH + 'train.csv')\ntest = pd.read_csv(DATA_PATH + 'test.csv')\n\n# ============================================================\n# 1. ë°ì´í„° Shape ì¶œë ¥ | Print Data Shapes\n# ============================================================\nprint(\"=\" * 70)\nprint(\"[1. Data Shape | ë°ì´í„° Shape]\")\nprint(\"=\" * 70)\nprint(f\"Train shape: {train.shape} (rows: {train.shape[0]:,}, columns: {train.shape[1]})\")\nprint(f\"Test shape: {test.shape} (rows: {test.shape[0]:,}, columns: {test.shape[1]})\")\n\n# ============================================================\n# 2. ê²°ì¸¡ì¹˜ í™•ì¸ | Check Missing Values\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[2. Missing Values | ê²°ì¸¡ì¹˜]\")\nprint(\"=\" * 70)\nprint(\"\\nTrain Missing Values:\")\nprint(train.isnull().sum())\nprint(f\"\\nTotal missing in train: {train.isnull().sum().sum()}\")\n\nprint(\"\\nTest Missing Values:\")\nprint(test.isnull().sum())\nprint(f\"Total missing in test: {test.isnull().sum().sum()}\")\n\n# ============================================================\n# 3. ë°ì´í„° íƒ€ì… ê²€ì¦ | Data Type Validation\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[3. Data Types | ë°ì´í„° íƒ€ì…]\")\nprint(\"=\" * 70)\nprint(\"\\nTrain data types:\")\nprint(train.dtypes)\n\n# ì˜ˆìƒ ë°ì´í„° íƒ€ì… ê²€ì¦ | Validate expected data types\nexpected_numeric = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\nfor col in expected_numeric:\n    if col in train.columns:\n        if not pd.api.types.is_numeric_dtype(train[col]):\n            print(f\"âš ï¸ ê²½ê³ : {col}ì€ ìˆ«ìí˜•ì´ì–´ì•¼ í•¨ / {col} should be numeric\")\n        else:\n            print(f\"âœ… {col}: ìˆ«ìí˜• í™•ì¸ë¨ / numeric confirmed\")\n\n# ============================================================\n# 4. ê°’ ë²”ìœ„ í™•ì¸ (ì´ìƒì¹˜ íƒì§€) | Value Range Check (Anomaly Detection)\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[4. Value Range Check | ê°’ ë²”ìœ„ í™•ì¸ (ì´ìƒì¹˜ íƒì§€)]\")\nprint(\"=\" * 70)\n\nnumeric_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\nprint(\"\\nê°’ ë²”ìœ„ ë° ì´ìƒì¹˜ (1%/99% ë¶„ìœ„ìˆ˜ ê¸°ì¤€):\")\nprint(\"Value range and outliers (based on 1%/99% quantiles):\\n\")\n\nfor col in numeric_cols:\n    if col in train.columns:\n        q01 = train[col].quantile(0.01)\n        q99 = train[col].quantile(0.99)\n        outliers_low = (train[col] < q01).sum()\n        outliers_high = (train[col] > q99).sum()\n        print(f\"{col:6s}: min={train[col].min():8.2f}, max={train[col].max():8.2f}, \"\n              f\"Q01={q01:8.2f}, Q99={q99:8.2f}, \"\n              f\"outliers(low/high)={outliers_low}/{outliers_high}\")\n\n# ============================================================\n# 5. ê¸°ìˆ  í†µê³„ëŸ‰ | Descriptive Statistics\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[5. Descriptive Statistics | ê¸°ìˆ  í†µê³„ëŸ‰]\")\nprint(\"=\" * 70)\nprint(\"\\nTrain Statistics:\")\nprint(train.describe().round(4))\n\n# ============================================================\n# 6. íƒ€ê²Ÿ ë³€ìˆ˜ (OT) ë¶„í¬ ë¶„ì„ | Target Variable (OT) Distribution Analysis\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[6. Target Variable (OT) Distribution | íƒ€ê²Ÿ ë³€ìˆ˜ (OT) ë¶„í¬]\")\nprint(\"=\" * 70)\n\nprint(f\"\\nOT ìƒì„¸ í†µê³„ / OT Detailed Statistics:\")\nprint(f\"  Mean (í‰ê· ): {train['OT'].mean():.4f}\")\nprint(f\"  Std (í‘œì¤€í¸ì°¨): {train['OT'].std():.4f}\")\nprint(f\"  Min (ìµœì†Œ): {train['OT'].min():.4f}\")\nprint(f\"  25% (1ì‚¬ë¶„ìœ„): {train['OT'].quantile(0.25):.4f}\")\nprint(f\"  50% (ì¤‘ì•™ê°’): {train['OT'].quantile(0.50):.4f}\")\nprint(f\"  75% (3ì‚¬ë¶„ìœ„): {train['OT'].quantile(0.75):.4f}\")\nprint(f\"  Max (ìµœëŒ€): {train['OT'].max():.4f}\")\nprint(f\"  Skewness (ì™œë„): {train['OT'].skew():.4f}\")\nprint(f\"  Kurtosis (ì²¨ë„): {train['OT'].kurtosis():.4f}\")\n\n# ============================================================\n# 7. ì‹œê°í™” | Visualization\n# ============================================================\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 7-1. OT íˆìŠ¤í† ê·¸ë¨ | OT Histogram\naxes[0, 0].hist(train['OT'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0, 0].axvline(train['OT'].mean(), color='red', linestyle='--', linewidth=2, \n                   label=f\"Mean: {train['OT'].mean():.2f}\")\naxes[0, 0].axvline(train['OT'].median(), color='green', linestyle='--', linewidth=2, \n                   label=f\"Median: {train['OT'].median():.2f}\")\naxes[0, 0].set_xlabel('OT (Oil Temperature)')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].set_title('OT Distribution | OT ë¶„í¬')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 7-2. OT ì‹œê³„ì—´ | OT Time Series\ntrain['date'] = pd.to_datetime(train['date'])\naxes[0, 1].plot(train['date'], train['OT'], alpha=0.7, linewidth=0.5)\naxes[0, 1].set_xlabel('Date')\naxes[0, 1].set_ylabel('OT')\naxes[0, 1].set_title('OT Time Series | OT ì‹œê³„ì—´')\naxes[0, 1].grid(True, alpha=0.3)\n\n# 7-3. ì…ë ¥ íŠ¹ì„± Boxplot | Input Features Boxplot\nfeature_data = train[['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']]\naxes[1, 0].boxplot([feature_data[col] for col in feature_data.columns], \n                   labels=feature_data.columns)\naxes[1, 0].set_ylabel('Value')\naxes[1, 0].set_title('Feature Boxplot | íŠ¹ì„± Boxplot')\naxes[1, 0].grid(True, alpha=0.3)\n\n# 7-4. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ | Correlation Heatmap\ncorr_matrix = feature_data.corr()\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=axes[1, 1])\naxes[1, 1].set_title('Feature Correlation Matrix | íŠ¹ì„± ìƒê´€ê´€ê³„')\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================\n# 8. ë°ì´í„° í’ˆì§ˆ ìš”ì•½ | Data Quality Summary\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Data Quality Summary | ë°ì´í„° í’ˆì§ˆ ìš”ì•½]\")\nprint(\"=\" * 70)\nprint(\"âœ… ê²°ì¸¡ì¹˜: ì—†ìŒ / Missing values: None\")\nprint(\"âœ… ë°ì´í„° íƒ€ì…: ì •ìƒ / Data types: Normal\")\nprint(\"âœ… ê°’ ë²”ìœ„: í•©ë¦¬ì  / Value ranges: Reasonable\")\nprint(f\"ğŸ“Š í•™ìŠµ ë°ì´í„° ê¸°ê°„: {train['date'].min()} ~ {train['date'].max()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooFdqqcFNW3I"
   },
   "source": [
    "### Q2. 'date' ì—´ì„ ì‚¬ìš©í•˜ì—¬ 'hour', 'dayofweek', 'month' íŠ¹ì„±ì„ ìƒì„±í•˜ê³ , 'hour'ì™€ 'dayofweek'ì— ëŒ€í•´ sinê³¼ cos ë³€í™˜ì„ ì ìš©í•˜ì—¬ cyclic featureë¥¼ ë§Œë“œì„¸ìš”. ì£¼ê¸°ë¥¼ ê²°ì •í•˜ëŠ”ê±´ ììœ ë¡­ê²Œ ì •í•˜ì…”ë„ ë©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic0jxhDuOvQr"
   },
   "outputs": [],
   "source": "# ============================================================\n# A2. ìˆœí™˜ íŠ¹ì„± ìƒì„± (í™•ì¥) | Create Cyclic Features (Extended)\n# ============================================================\n# ì‹œê°„ ê´€ë ¨ íŠ¹ì„±ì€ ìˆœí™˜ì  íŠ¹ì„±ì„ ê°€ì§€ë¯€ë¡œ sin/cos ë³€í™˜ì„ ì ìš©í•©ë‹ˆë‹¤.\n# Time-related features have cyclic nature, so we apply sin/cos transformation.\n#\n# ğŸ”§ ê°œì„ : quarter, is_weekend íŠ¹ì„± ì¶”ê°€\n# ğŸ”§ Improvement: Added quarter, is_weekend features\n#\n# ìˆœí™˜ ì¸ì½”ë”©ì˜ í•„ìš”ì„± / Why Cyclic Encoding:\n# - 23ì‹œì™€ 0ì‹œëŠ” ì‹¤ì œë¡œ 1ì‹œê°„ ì°¨ì´ì§€ë§Œ, ìˆ«ìë¡œëŠ” 23 ì°¨ì´\n# - 23:00 and 00:00 are 1 hour apart, but numerically differ by 23\n# - sin/cos ë³€í™˜ìœ¼ë¡œ ì´ ë¬¸ì œ í•´ê²° (ì—°ì†ì ì¸ ì› ìœ„ì˜ ì ìœ¼ë¡œ í‘œí˜„)\n# - sin/cos transformation solves this (represented as points on a continuous circle)\n\ndef create_time_features(df):\n    \"\"\"\n    ì‹œê°„ ê´€ë ¨ íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤ (í™•ì¥ ë²„ì „).\n    Create time-related features (extended version).\n    \n    ìƒì„±ë˜ëŠ” íŠ¹ì„± / Generated features:\n    - ê¸°ë³¸ ì‹œê°„ íŠ¹ì„±: hour, minute, dayofweek, month, dayofyear, quarter\n    - Basic time features: hour, minute, dayofweek, month, dayofyear, quarter\n    - ìˆœí™˜ íŠ¹ì„±: *_sin, *_cos (6ê°œ ì‹œê°„ ë‹¨ìœ„)\n    - Cyclic features: *_sin, *_cos (6 time units)\n    - ì´ì§„ íŠ¹ì„±: is_weekend\n    - Binary features: is_weekend\n    \n    Args:\n        df: date ì—´ì´ ìˆëŠ” DataFrame / DataFrame with 'date' column\n    \n    Returns:\n        ì‹œê°„ íŠ¹ì„±ì´ ì¶”ê°€ëœ DataFrame / DataFrame with time features added\n    \"\"\"\n    df = df.copy()\n    \n    # date ì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜ / Convert date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # ============================================================\n    # ê¸°ë³¸ ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ | Extract Basic Time Features\n    # ============================================================\n    df['hour'] = df['date'].dt.hour              # ì‹œê°„ (0-23) / Hour\n    df['minute'] = df['date'].dt.minute          # ë¶„ (0, 15, 30, 45) / Minute\n    df['dayofweek'] = df['date'].dt.dayofweek    # ìš”ì¼ (0=ì›”ìš”ì¼) / Day of week\n    df['month'] = df['date'].dt.month            # ì›” (1-12) / Month\n    df['dayofyear'] = df['date'].dt.dayofyear    # ì—°ì¤‘ ì¼ìˆ˜ (1-365) / Day of year\n    \n    # ğŸ”§ NEW: quarter íŠ¹ì„± (ê³„ì ˆì„± í¬ì°©)\n    # ğŸ”§ NEW: quarter feature (captures seasonality)\n    df['quarter'] = df['date'].dt.quarter        # ë¶„ê¸° (1-4) / Quarter\n    \n    # ğŸ”§ NEW: is_weekend ì´ì§„ íŠ¹ì„±\n    # ğŸ”§ NEW: is_weekend binary feature\n    # ì£¼ë§ì—ëŠ” ì „ë ¥ ì†Œë¹„ íŒ¨í„´ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ\n    # Power consumption patterns may differ on weekends\n    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n    \n    # ============================================================\n    # ìˆœí™˜ ë³€í™˜ ì ìš© (sin/cos) | Apply Cyclic Transformation\n    # ============================================================\n    # ìˆœí™˜ íŠ¹ì„±ì€ ì£¼ê¸°ì˜ ëê³¼ ì‹œì‘ì´ ì—°ê²°ë˜ì–´ì•¼ í•¨\n    # Cyclic features should connect end to start of period\n    \n    # hour: 24ì‹œê°„ ì£¼ê¸° / 24-hour cycle\n    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n    \n    # minute: 60ë¶„ ì£¼ê¸° / 60-minute cycle\n    df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 60)\n    df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 60)\n    \n    # dayofweek: 7ì¼ ì£¼ê¸° / 7-day cycle\n    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n    \n    # month: 12ê°œì›” ì£¼ê¸° / 12-month cycle\n    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n    \n    # dayofyear: 365ì¼ ì£¼ê¸° (ê³„ì ˆì„± í¬ì°©)\n    # dayofyear: 365-day cycle (captures seasonality)\n    df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n    df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n    \n    # ğŸ”§ NEW: quarter: 4ë¶„ê¸° ì£¼ê¸°\n    # ğŸ”§ NEW: quarter: 4-quarter cycle\n    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n    \n    return df\n\n# Trainê³¼ Test ë°ì´í„°ì— ì‹œê°„ íŠ¹ì„± ì ìš© / Apply time features to train and test\ntrain = create_time_features(train)\ntest = create_time_features(test)\n\n# ============================================================\n# ê²°ê³¼ ì¶œë ¥ | Print Results\n# ============================================================\nprint(\"=\" * 70)\nprint(\"[Cyclic Features Created (Extended) | ìƒì„±ëœ ìˆœí™˜ íŠ¹ì„± (í™•ì¥)]\")\nprint(\"=\" * 70)\n\nprint(\"\\nğŸ“‹ ê¸°ë³¸ íŠ¹ì„± / Basic features:\")\nprint(\"  - hour, minute, dayofweek, month, dayofyear\")\nprint(\"  - quarter (NEW: ë¶„ê¸° / quarter)\")\nprint(\"  - is_weekend (NEW: ì£¼ë§ ì—¬ë¶€ / weekend indicator)\")\n\nprint(\"\\nğŸ”„ ìˆœí™˜ ë³€í™˜ íŠ¹ì„± / Cyclic transformed features:\")\nprint(\"  - hour_sin, hour_cos (24ì‹œê°„ ì£¼ê¸° / 24-hour cycle)\")\nprint(\"  - minute_sin, minute_cos (60ë¶„ ì£¼ê¸° / 60-minute cycle)\")\nprint(\"  - dayofweek_sin, dayofweek_cos (7ì¼ ì£¼ê¸° / 7-day cycle)\")\nprint(\"  - month_sin, month_cos (12ê°œì›” ì£¼ê¸° / 12-month cycle)\")\nprint(\"  - dayofyear_sin, dayofyear_cos (365ì¼ ì£¼ê¸° / 365-day cycle)\")\nprint(\"  - quarter_sin, quarter_cos (NEW: 4ë¶„ê¸° ì£¼ê¸° / 4-quarter cycle)\")\n\nprint(f\"\\nTrain shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\n\n# íŠ¹ì„± ìˆ˜ í™•ì¸ / Check feature count\ntime_features_count = 7 + 12  # 7 ê¸°ë³¸ + 12 ìˆœí™˜ (6ê°œ * 2)\nprint(f\"\\nâœ… ì´ ìƒì„±ëœ ì‹œê°„ íŠ¹ì„± ìˆ˜ / Total time features created: {time_features_count}\")\n\n# is_weekend ë¶„í¬ í™•ì¸ / Check is_weekend distribution\nprint(f\"\\n[is_weekend Distribution | ì£¼ë§ ë¶„í¬]\")\nprint(f\"  Train - Weekday: {(train['is_weekend'] == 0).sum():,} ({(train['is_weekend'] == 0).mean()*100:.1f}%)\")\nprint(f\"  Train - Weekend: {(train['is_weekend'] == 1).sum():,} ({(train['is_weekend'] == 1).mean()*100:.1f}%)\")\n\n# quarter ë¶„í¬ í™•ì¸ / Check quarter distribution\nprint(f\"\\n[Quarter Distribution | ë¶„ê¸° ë¶„í¬]\")\nquarter_dist = train['quarter'].value_counts().sort_index()\nfor q, count in quarter_dist.items():\n    print(f\"  Q{q}: {count:,} ({count/len(train)*100:.1f}%)\")\n\n# ============================================================\n# ìˆœí™˜ íŠ¹ì„± ì‹œê°í™” | Visualize Cyclic Features\n# ============================================================\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# 1. hour ìˆœí™˜ íŠ¹ì„± / Hour cyclic features\naxes[0].scatter(train['hour_sin'], train['hour_cos'], c=train['hour'], \n                cmap='hsv', alpha=0.3, s=1)\naxes[0].set_xlabel('hour_sin')\naxes[0].set_ylabel('hour_cos')\naxes[0].set_title('Hour Cyclic Encoding | ì‹œê°„ ìˆœí™˜ ì¸ì½”ë”©')\naxes[0].set_aspect('equal')\naxes[0].grid(True, alpha=0.3)\n\n# 2. dayofweek ìˆœí™˜ íŠ¹ì„± / Day of week cyclic features\naxes[1].scatter(train['dayofweek_sin'], train['dayofweek_cos'], c=train['dayofweek'], \n                cmap='tab10', alpha=0.3, s=1)\naxes[1].set_xlabel('dayofweek_sin')\naxes[1].set_ylabel('dayofweek_cos')\naxes[1].set_title('Day of Week Cyclic Encoding | ìš”ì¼ ìˆœí™˜ ì¸ì½”ë”©')\naxes[1].set_aspect('equal')\naxes[1].grid(True, alpha=0.3)\n\n# 3. is_weekendì— ë”°ë¥¸ OT ë¶„í¬ / OT distribution by is_weekend\nweekday_ot = train[train['is_weekend'] == 0]['OT']\nweekend_ot = train[train['is_weekend'] == 1]['OT']\naxes[2].hist(weekday_ot, bins=50, alpha=0.5, label=f'Weekday (mean={weekday_ot.mean():.2f})', color='blue')\naxes[2].hist(weekend_ot, bins=50, alpha=0.5, label=f'Weekend (mean={weekend_ot.mean():.2f})', color='red')\naxes[2].set_xlabel('OT')\naxes[2].set_ylabel('Frequency')\naxes[2].set_title('OT Distribution by is_weekend | ì£¼ë§/í‰ì¼ë³„ OT ë¶„í¬')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLL2e9JlNW3I"
   },
   "source": [
    "### Q3. 'OT' ì—´ì— ëŒ€í•´ 1ì‹œê°„ ì „, 2ì‹œê°„ ì „, 3ì‹œê°„ ì „ì˜ ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” lag íŠ¹ì„±ì„ ìƒì„±í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soHRdYpsOwRS"
   },
   "outputs": [],
   "source": "# ============================================================\n# A3. ì§€ì—°, ë¡¤ë§, EWM, ì°¨ë¶„ íŠ¹ì„± ìƒì„± (í™•ì¥)\n# A3. Create Lag, Rolling, EWM, and Differencing Features (Extended)\n# ============================================================\n# ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œ ê³¼ê±° ê°’ì€ ë¯¸ë˜ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n# In time series forecasting, past values provide important information.\n#\n# ğŸ”§ ê°œì„ : EWM(ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· ), ì°¨ë¶„ íŠ¹ì„± ì¶”ê°€\n# ğŸ”§ Improvement: Added EWM and differencing features\n\ndef create_lag_features(df, target_col='OT', lags_hours=[1, 2, 3, 6, 12, 24]):\n    \"\"\"\n    ì§€ì—° íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤.\n    Create lag features.\n    \n    Args:\n        df: ëŒ€ìƒ DataFrame / Target DataFrame\n        target_col: íƒ€ê²Ÿ ì—´ ì´ë¦„ / Target column name\n        lags_hours: ì§€ì—° ì‹œê°„ ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ ë‹¨ìœ„) / List of lag hours\n    \n    Returns:\n        ì§€ì—° íŠ¹ì„±ì´ ì¶”ê°€ëœ DataFrame / DataFrame with lag features\n    \"\"\"\n    df = df.copy()\n    \n    for lag_h in lags_hours:\n        shift_steps = lag_h * INTERVALS_PER_HOUR\n        df[f'{target_col}_lag_{lag_h}h'] = df[target_col].shift(shift_steps)\n    \n    return df\n\ndef create_rolling_features(df, target_col='OT', windows_hours=[1, 3, 6, 12, 24]):\n    \"\"\"\n    ë¡¤ë§ í†µê³„ëŸ‰ íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤.\n    Create rolling statistics features.\n    \n    Args:\n        df: ëŒ€ìƒ DataFrame / Target DataFrame\n        target_col: íƒ€ê²Ÿ ì—´ ì´ë¦„ / Target column name\n        windows_hours: ìœˆë„ìš° í¬ê¸° ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ ë‹¨ìœ„) / List of window sizes\n    \n    Returns:\n        ë¡¤ë§ íŠ¹ì„±ì´ ì¶”ê°€ëœ DataFrame / DataFrame with rolling features\n    \"\"\"\n    df = df.copy()\n    \n    for window_h in windows_hours:\n        window_steps = window_h * INTERVALS_PER_HOUR\n        \n        # ë¡¤ë§ í‰ê·  / Rolling mean\n        df[f'{target_col}_rolling_mean_{window_h}h'] = (\n            df[target_col].shift(1).rolling(window=window_steps, min_periods=1).mean()\n        )\n        \n        # ë¡¤ë§ í‘œì¤€í¸ì°¨ / Rolling std\n        df[f'{target_col}_rolling_std_{window_h}h'] = (\n            df[target_col].shift(1).rolling(window=window_steps, min_periods=1).std()\n        )\n    \n    return df\n\n# ğŸ”§ NEW: EWM (ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· ) íŠ¹ì„± í•¨ìˆ˜\n# ğŸ”§ NEW: EWM (Exponential Weighted Mean) feature function\ndef create_ewm_features(df, target_col='OT', spans_hours=[6, 12, 24]):\n    \"\"\"\n    ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· (EWM) íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤.\n    Create Exponential Weighted Mean (EWM) features.\n    \n    EWMì€ ìµœê·¼ ê°’ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬\n    ë‹¨ìˆœ ë¡¤ë§ í‰ê· ë³´ë‹¤ ìµœê·¼ ì¶”ì„¸ë¥¼ ë” ì˜ ë°˜ì˜í•©ë‹ˆë‹¤.\n    EWM gives higher weight to recent values, reflecting recent trends\n    better than simple rolling mean.\n    \n    Args:\n        df: ëŒ€ìƒ DataFrame / Target DataFrame\n        target_col: íƒ€ê²Ÿ ì—´ ì´ë¦„ / Target column name\n        spans_hours: span í¬ê¸° ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ ë‹¨ìœ„) / List of span sizes\n    \n    Returns:\n        EWM íŠ¹ì„±ì´ ì¶”ê°€ëœ DataFrame / DataFrame with EWM features\n    \"\"\"\n    df = df.copy()\n    \n    for span_h in spans_hours:\n        span_steps = span_h * INTERVALS_PER_HOUR\n        # shift(1)ë¡œ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ / Prevent data leakage with shift(1)\n        df[f'{target_col}_ewm_{span_h}h'] = (\n            df[target_col].shift(1).ewm(span=span_steps, min_periods=1).mean()\n        )\n    \n    return df\n\n# ğŸ”§ NEW: ì°¨ë¶„ íŠ¹ì„± í•¨ìˆ˜ (íŠ¸ë Œë“œ ê°ì§€)\n# ğŸ”§ NEW: Differencing feature function (trend detection)\ndef create_diff_features(df, target_col='OT', periods_hours=[1, 24]):\n    \"\"\"\n    ì°¨ë¶„ íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤ (íŠ¸ë Œë“œ ê°ì§€).\n    Create differencing features (trend detection).\n    \n    ì°¨ë¶„ì€ ì‹œê³„ì—´ì˜ ì¶”ì„¸ì™€ ë³€í™”ìœ¨ì„ í¬ì°©í•©ë‹ˆë‹¤.\n    Differencing captures trends and rate of change in time series.\n    - 1ì‹œê°„ ì°¨ë¶„: ë‹¨ê¸° ë³€í™” í¬ì°© / 1-hour diff: captures short-term changes\n    - 24ì‹œê°„ ì°¨ë¶„: ì¼ì¼ íŒ¨í„´ ë³€í™” í¬ì°© / 24-hour diff: captures daily pattern changes\n    \n    Args:\n        df: ëŒ€ìƒ DataFrame / Target DataFrame\n        target_col: íƒ€ê²Ÿ ì—´ ì´ë¦„ / Target column name\n        periods_hours: ì°¨ë¶„ ê¸°ê°„ ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ ë‹¨ìœ„) / List of diff periods\n    \n    Returns:\n        ì°¨ë¶„ íŠ¹ì„±ì´ ì¶”ê°€ëœ DataFrame / DataFrame with diff features\n    \"\"\"\n    df = df.copy()\n    \n    for period_h in periods_hours:\n        period_steps = period_h * INTERVALS_PER_HOUR\n        # í˜„ì¬ ëŒ€ë¹„ nì‹œê°„ ì „ì˜ ë³€í™”ëŸ‰ / Change from n hours ago\n        df[f'{target_col}_diff_{period_h}h'] = (\n            df[target_col].shift(1) - df[target_col].shift(1 + period_steps)\n        )\n    \n    return df\n\n# ============================================================\n# íŠ¹ì„± ìƒì„± ì‹¤í–‰ | Execute Feature Creation\n# ============================================================\n\n# ì§€ì—° íŠ¹ì„±: 1, 2, 3, 6, 12, 24ì‹œê°„ ì „\n# Lag features: 1, 2, 3, 6, 12, 24 hours ago\nlag_hours = [1, 2, 3, 6, 12, 24]\ntrain = create_lag_features(train, 'OT', lag_hours)\n\n# ë¡¤ë§ íŠ¹ì„±: 1, 3, 6, 12, 24ì‹œê°„ ìœˆë„ìš°\n# Rolling features: 1, 3, 6, 12, 24 hour windows\nrolling_hours = [1, 3, 6, 12, 24]\ntrain = create_rolling_features(train, 'OT', rolling_hours)\n\n# ğŸ”§ NEW: EWM íŠ¹ì„±: 6, 12, 24ì‹œê°„ span\n# ğŸ”§ NEW: EWM features: 6, 12, 24 hour spans\newm_hours = [6, 12, 24]\ntrain = create_ewm_features(train, 'OT', ewm_hours)\n\n# ğŸ”§ NEW: ì°¨ë¶„ íŠ¹ì„±: 1ì‹œê°„, 24ì‹œê°„\n# ğŸ”§ NEW: Differencing features: 1 hour, 24 hours\ndiff_hours = [1, 24]\ntrain = create_diff_features(train, 'OT', diff_hours)\n\n# ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸ / Check NaN count\nmax_nan_rows = INTERVALS_PER_HOUR * max(lag_hours)  # 24ì‹œê°„ ì§€ì—°ìœ¼ë¡œ ì¸í•œ ìµœëŒ€ NaN\n\n# ============================================================\n# ê²°ê³¼ ì¶œë ¥ | Print Results\n# ============================================================\nprint(\"=\" * 70)\nprint(\"[Lag, Rolling, EWM, Diff Features Created | ì§€ì—°, ë¡¤ë§, EWM, ì°¨ë¶„ íŠ¹ì„± ìƒì„±]\")\nprint(\"=\" * 70)\n\nprint(\"\\nğŸ“Š ì§€ì—° íŠ¹ì„± / Lag Features:\")\nfor lag_h in lag_hours:\n    shift_steps = lag_h * INTERVALS_PER_HOUR\n    print(f\"  - OT_lag_{lag_h}h: OT value from {lag_h} hour(s) ago (shift={shift_steps})\")\n\nprint(\"\\nğŸ“ˆ ë¡¤ë§ íŠ¹ì„± / Rolling Features:\")\nfor window_h in rolling_hours:\n    print(f\"  - OT_rolling_mean_{window_h}h: {window_h}ì‹œê°„ ì´ë™ í‰ê·  / {window_h}h rolling mean\")\n    print(f\"  - OT_rolling_std_{window_h}h: {window_h}ì‹œê°„ ì´ë™ í‘œì¤€í¸ì°¨ / {window_h}h rolling std\")\n\nprint(\"\\nğŸ“‰ EWM íŠ¹ì„± (NEW) / EWM Features (NEW):\")\nfor span_h in ewm_hours:\n    print(f\"  - OT_ewm_{span_h}h: {span_h}ì‹œê°„ ì§€ìˆ˜ ê°€ì¤‘ í‰ê·  / {span_h}h exponential weighted mean\")\n\nprint(\"\\nğŸ“ ì°¨ë¶„ íŠ¹ì„± (NEW) / Differencing Features (NEW):\")\nfor period_h in diff_hours:\n    print(f\"  - OT_diff_{period_h}h: {period_h}ì‹œê°„ ì „ ëŒ€ë¹„ ë³€í™”ëŸ‰ / Change from {period_h}h ago\")\n\nprint(f\"\\nâš ï¸ ì§€ì—°/ë¡¤ë§ìœ¼ë¡œ ì¸í•œ NaN í–‰: ìµœëŒ€ {max_nan_rows}ê°œ\")\nprint(f\"âš ï¸ NaN rows from lag/rolling: up to {max_nan_rows}\")\nprint(f\"\\nTrain shape: {train.shape}\")\n\n# ============================================================\n# íŠ¹ì„± ì‹œê°í™” | Feature Visualization\n# ============================================================\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 1000ê°œ) / Sample data (first 1000)\nsample_idx = range(SEQUENCE_LENGTH, SEQUENCE_LENGTH + 1000)\n\n# 1. Lag íŠ¹ì„± ë¹„êµ / Lag features comparison\naxes[0, 0].plot(train['OT'].iloc[sample_idx].values, label='OT (actual)', alpha=0.8)\naxes[0, 0].plot(train['OT_lag_1h'].iloc[sample_idx].values, label='OT_lag_1h', alpha=0.6)\naxes[0, 0].plot(train['OT_lag_6h'].iloc[sample_idx].values, label='OT_lag_6h', alpha=0.6)\naxes[0, 0].set_xlabel('Time Step')\naxes[0, 0].set_ylabel('OT')\naxes[0, 0].set_title('Lag Features Comparison | Lag íŠ¹ì„± ë¹„êµ')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Rolling íŠ¹ì„± / Rolling features\naxes[0, 1].plot(train['OT'].iloc[sample_idx].values, label='OT (actual)', alpha=0.8)\naxes[0, 1].plot(train['OT_rolling_mean_6h'].iloc[sample_idx].values, label='Rolling Mean 6h', alpha=0.8)\naxes[0, 1].fill_between(range(len(sample_idx)),\n                        (train['OT_rolling_mean_6h'] - train['OT_rolling_std_6h']).iloc[sample_idx].values,\n                        (train['OT_rolling_mean_6h'] + train['OT_rolling_std_6h']).iloc[sample_idx].values,\n                        alpha=0.2, label='Â±1 Std')\naxes[0, 1].set_xlabel('Time Step')\naxes[0, 1].set_ylabel('OT')\naxes[0, 1].set_title('Rolling Features | ë¡¤ë§ íŠ¹ì„±')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. EWM íŠ¹ì„± ë¹„êµ / EWM features comparison\naxes[1, 0].plot(train['OT'].iloc[sample_idx].values, label='OT (actual)', alpha=0.8)\naxes[1, 0].plot(train['OT_ewm_6h'].iloc[sample_idx].values, label='EWM 6h', alpha=0.8)\naxes[1, 0].plot(train['OT_ewm_24h'].iloc[sample_idx].values, label='EWM 24h', alpha=0.8)\naxes[1, 0].plot(train['OT_rolling_mean_24h'].iloc[sample_idx].values, label='Rolling Mean 24h', \n                alpha=0.6, linestyle='--')\naxes[1, 0].set_xlabel('Time Step')\naxes[1, 0].set_ylabel('OT')\naxes[1, 0].set_title('EWM vs Rolling Mean | EWM vs ë¡¤ë§ í‰ê· ')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. ì°¨ë¶„ íŠ¹ì„± / Differencing features\naxes[1, 1].plot(train['OT_diff_1h'].iloc[sample_idx].values, label='Diff 1h', alpha=0.8)\naxes[1, 1].plot(train['OT_diff_24h'].iloc[sample_idx].values, label='Diff 24h', alpha=0.8)\naxes[1, 1].axhline(0, color='gray', linestyle='--', alpha=0.5)\naxes[1, 1].set_xlabel('Time Step')\naxes[1, 1].set_ylabel('Difference')\naxes[1, 1].set_title('Differencing Features (Trend) | ì°¨ë¶„ íŠ¹ì„± (íŠ¸ë Œë“œ)')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================\n# íŠ¹ì„± í†µê³„ ìš”ì•½ | Feature Statistics Summary\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Feature Statistics Summary | íŠ¹ì„± í†µê³„ ìš”ì•½]\")\nprint(\"=\" * 70)\n\nnew_features = ['OT_ewm_6h', 'OT_ewm_12h', 'OT_ewm_24h', 'OT_diff_1h', 'OT_diff_24h']\nfor feat in new_features:\n    if feat in train.columns:\n        non_nan = train[feat].dropna()\n        print(f\"{feat:20s}: mean={non_nan.mean():8.4f}, std={non_nan.std():8.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpChME79NW3I"
   },
   "source": [
    "### Q4. ë¶ˆí•„ìš”í•œ ì—´ì¸ 'date'ë¥¼ ì œê±°í•˜ê³ , íŠ¹ì„± í–‰ë ¬ Xì™€ ëª©í‘œ ë³€ìˆ˜ yë¥¼ ìƒì„±í•˜ì—¬ ë°ì´í„°ë¥¼ ì‹œê°„ ìˆœì„œì— ë”°ë¼ 3:1 ë¹„ìœ¨ë¡œ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6FQ8Al0OzlX"
   },
   "outputs": [],
   "source": "# ============================================================\n# A4. ë°ì´í„° ì¤€ë¹„ ë° ë¶„í•  (ë¶„í¬ ì°¨ì´ ë¬¸ì„œí™” í¬í•¨)\n# A4. Prepare and Split Data (with Distribution Difference Documentation)\n# ============================================================\n# íŠ¹ì„± í–‰ë ¬ Xì™€ ëª©í‘œ ë³€ìˆ˜ yë¥¼ ìƒì„±í•˜ê³  ì‹œê°„ ìˆœì„œì— ë”°ë¼ ë¶„í• í•©ë‹ˆë‹¤.\n# Create feature matrix X and target variable y, then split by time order.\n#\n# ğŸ”§ ì‹œê³„ì—´ ë°ì´í„°ëŠ” ë°˜ë“œì‹œ ì‹œê°„ ìˆœì„œë¡œ ë¶„í• í•´ì•¼ í•©ë‹ˆë‹¤ (shuffle=False)\n# ğŸ”§ Time series data must be split by time order (shuffle=False)\n# ë¯¸ë˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡ì€ ë°ì´í„° ëˆ„ìˆ˜(data leakage)ë¥¼ ì¼ìœ¼í‚µë‹ˆë‹¤.\n# Using future data for prediction causes data leakage.\n\n# date ì—´ ì œê±° / Remove date column\ntrain_processed = train.drop(columns=['date'])\n\n# NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±° (ì§€ì—°/ë¡¤ë§ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ë°œìƒ)\n# Drop rows with NaN values (caused by lag/rolling features)\ninitial_len = len(train_processed)\ntrain_processed = train_processed.dropna()\ndropped_rows = initial_len - len(train_processed)\n\nprint(\"=\" * 70)\nprint(\"[Data Preparation | ë°ì´í„° ì¤€ë¹„]\")\nprint(\"=\" * 70)\nprint(f\"âš ï¸ NaN í–‰ ì œê±° / Dropped NaN rows: {dropped_rows}\")\nprint(f\"   ë‚¨ì€ ë°ì´í„° / Remaining data: {len(train_processed):,}\")\n\n# ============================================================\n# íŠ¹ì„± í–‰ë ¬ Xì™€ ëª©í‘œ ë³€ìˆ˜ y ìƒì„±\n# Create feature matrix X and target variable y\n# ============================================================\ny = train_processed['OT']\nX = train_processed.drop(columns=['OT'])\n\n# íŠ¹ì„± ëª©ë¡ í™•ì¸ / Check feature list\nprint(f\"\\nğŸ“‹ ì´ íŠ¹ì„± ìˆ˜ / Total features: {len(X.columns)}\")\nprint(f\"   ì»¬ëŸ¼ ëª©ë¡ / Column list ({len(X.columns)} features):\")\nfor i, col in enumerate(X.columns):\n    print(f\"     {i+1:2d}. {col}\")\n\n# ============================================================\n# ì‹œê°„ ìˆœì„œì— ë”°ë¼ 3:1 ë¹„ìœ¨ë¡œ ë¶„í•  (ì‹œê³„ì—´ì´ë¯€ë¡œ shuffle=False)\n# Split by time order in 3:1 ratio (no shuffle for time series)\n# ============================================================\ntrain_ratio = 0.75\nsplit_idx = int(len(X) * train_ratio)\n\nX_train = X.iloc[:split_idx].copy()\nX_val = X.iloc[split_idx:].copy()\ny_train = y.iloc[:split_idx].copy()\ny_val = y.iloc[split_idx:].copy()\n\n# ë¶„í•  ë‚ ì§œ í™•ì¸ (ì›ë³¸ trainì—ì„œ í™•ì¸)\n# Check split date (from original train data)\ntrain_dates = train.dropna()['date']\nsplit_date = train_dates.iloc[split_idx]\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Data Split | ë°ì´í„° ë¶„í• ]\")\nprint(\"=\" * 70)\nprint(f\"ğŸ“… ë¶„í•  ê¸°ì¤€ ë‚ ì§œ / Split date: {split_date}\")\nprint(f\"ğŸ“Š í•™ìŠµ ì„¸íŠ¸ / Training set: {len(X_train):,} samples ({train_ratio * 100:.0f}%)\")\nprint(f\"ğŸ“Š ê²€ì¦ ì„¸íŠ¸ / Validation set: {len(X_val):,} samples ({(1 - train_ratio) * 100:.0f}%)\")\n\n# ============================================================\n# ğŸ”§ NEW: íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ì°¨ì´ ë¶„ì„ ë° ë¬¸ì„œí™”\n# ğŸ”§ NEW: Target Variable Distribution Difference Analysis\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Target Variable Distribution Analysis | íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ë¶„ì„]\")\nprint(\"=\" * 70)\n\n# ìƒì„¸ í†µê³„ / Detailed statistics\nprint(\"\\nğŸ“Š Train OT Statistics:\")\nprint(f\"   Mean: {y_train.mean():.4f}\")\nprint(f\"   Std: {y_train.std():.4f}\")\nprint(f\"   Min: {y_train.min():.4f}\")\nprint(f\"   Max: {y_train.max():.4f}\")\nprint(f\"   Median: {y_train.median():.4f}\")\n\nprint(\"\\nğŸ“Š Validation OT Statistics:\")\nprint(f\"   Mean: {y_val.mean():.4f}\")\nprint(f\"   Std: {y_val.std():.4f}\")\nprint(f\"   Min: {y_val.min():.4f}\")\nprint(f\"   Max: {y_val.max():.4f}\")\nprint(f\"   Median: {y_val.median():.4f}\")\n\n# ë¶„í¬ ì°¨ì´ ê²½ê³  / Distribution difference warning\nmean_diff = y_train.mean() - y_val.mean()\nstd_diff = y_train.std() - y_val.std()\n\nprint(\"\\n\" + \"-\" * 50)\nprint(\"âš ï¸ ë¶„í¬ ì°¨ì´ ë¶„ì„ / Distribution Difference Analysis:\")\nprint(\"-\" * 50)\nprint(f\"   í‰ê·  ì°¨ì´ / Mean difference: {mean_diff:.4f}\")\nprint(f\"   í‘œì¤€í¸ì°¨ ì°¨ì´ / Std difference: {std_diff:.4f}\")\n\nif abs(mean_diff) > 2.0:\n    print(f\"\\n   ğŸš¨ ê²½ê³  / Warning: Trainê³¼ Validationì˜ í‰ê·  ì°¨ì´ê°€ í½ë‹ˆë‹¤ ({abs(mean_diff):.2f})\")\n    print(\"   ì´ëŠ” ê³„ì ˆì  íŒ¨í„´ ì°¨ì´ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.\")\n    print(\"   This reflects seasonal pattern differences.\")\n\n# ============================================================\n# ë¶„í¬ ì°¨ì´ì˜ ì›ì¸ ë¶„ì„ / Cause Analysis of Distribution Difference\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Distribution Difference Cause Analysis | ë¶„í¬ ì°¨ì´ ì›ì¸ ë¶„ì„]\")\nprint(\"=\" * 70)\n\n# ê¸°ê°„ë³„ ë¶„ì„ / Period analysis\ntrain_start = train_dates.iloc[0]\ntrain_end = train_dates.iloc[split_idx - 1]\nval_start = train_dates.iloc[split_idx]\nval_end = train_dates.iloc[-1]\n\nprint(f\"\\nğŸ“… í•™ìŠµ ë°ì´í„° ê¸°ê°„ / Training data period:\")\nprint(f\"   {train_start} ~ {train_end}\")\nprint(f\"   ì£¼ë¡œ ì—¬ë¦„~ê°€ì„ ê¸°ê°„ í¬í•¨ / Includes mainly summer~fall period\")\n\nprint(f\"\\nğŸ“… ê²€ì¦ ë°ì´í„° ê¸°ê°„ / Validation data period:\")\nprint(f\"   {val_start} ~ {val_end}\")\nprint(f\"   ì£¼ë¡œ ê°€ì„~ê²¨ìš¸ ê¸°ê°„ / Mainly fall~winter period\")\n\nprint(\"\"\"\nğŸ’¡ ë¶„í¬ ì°¨ì´ì˜ ì›ì¸ / Cause of Distribution Difference:\n   \n   1. ê³„ì ˆì  ìš”ì¸ / Seasonal Factors:\n      - í•™ìŠµ ë°ì´í„°: ë”ìš´ ì‹œê¸° í¬í•¨ â†’ ë†’ì€ OT (ì˜¤ì¼ ì˜¨ë„)\n      - Training data: Includes hot period â†’ Higher OT\n      - ê²€ì¦ ë°ì´í„°: ì¶”ìš´ ì‹œê¸° â†’ ë‚®ì€ OT\n      - Validation data: Cold period â†’ Lower OT\n   \n   2. ì „ë ¥ ì†Œë¹„ íŒ¨í„´ / Power Consumption Patterns:\n      - ì—¬ë¦„: ëƒ‰ë°© ìˆ˜ìš” ì¦ê°€ â†’ ë³€ì••ê¸° ë¶€í•˜ ì¦ê°€ â†’ ë†’ì€ OT\n      - Summer: Increased cooling demand â†’ Higher transformer load â†’ Higher OT\n      - ê²¨ìš¸: ë‚œë°© ìˆ˜ìš” (ì „ê¸° ë‚œë°©ì˜ ê²½ìš°) ë˜ëŠ” ê°ì†Œ\n      - Winter: Heating demand (if electric) or decreased\n   \n   3. ëª¨ë¸ë§ ì‹œ ê³ ë ¤ì‚¬í•­ / Modeling Considerations:\n      - ëª¨ë¸ì´ ê³„ì ˆì  ë³€ë™ì„ ì˜ í•™ìŠµí–ˆëŠ”ì§€ í™•ì¸ í•„ìš”\n      - Need to verify if model learned seasonal variations well\n      - ê²€ì¦ ì„¸íŠ¸ê°€ \"ì™¸ì‚½(extrapolation)\" ìƒí™©ì¼ ìˆ˜ ìˆìŒ\n      - Validation set may be an \"extrapolation\" scenario\n\"\"\")\n\n# ============================================================\n# ë¶„í¬ ë¹„êµ ì‹œê°í™” | Distribution Comparison Visualization\n# ============================================================\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Train vs Validation OT ë¶„í¬ íˆìŠ¤í† ê·¸ë¨\n# Train vs Validation OT distribution histogram\naxes[0, 0].hist(y_train, bins=50, alpha=0.6, label=f'Train (mean={y_train.mean():.2f})', \n                color='blue', edgecolor='black')\naxes[0, 0].hist(y_val, bins=50, alpha=0.6, label=f'Validation (mean={y_val.mean():.2f})', \n                color='red', edgecolor='black')\naxes[0, 0].axvline(y_train.mean(), color='blue', linestyle='--', linewidth=2)\naxes[0, 0].axvline(y_val.mean(), color='red', linestyle='--', linewidth=2)\naxes[0, 0].set_xlabel('OT (Oil Temperature)')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].set_title('OT Distribution: Train vs Validation | OT ë¶„í¬ ë¹„êµ')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. ì›”ë³„ í‰ê·  OT / Monthly average OT\ntrain_with_date = train.dropna().copy()\ntrain_with_date['split'] = ['Train' if i < split_idx else 'Validation' \n                            for i in range(len(train_with_date))]\nmonthly_ot = train_with_date.groupby([train_with_date['date'].dt.to_period('M'), 'split'])['OT'].mean().unstack()\nmonthly_ot.plot(kind='bar', ax=axes[0, 1], color=['blue', 'red'], alpha=0.7, width=0.8)\naxes[0, 1].set_xlabel('Month-Year')\naxes[0, 1].set_ylabel('Average OT')\naxes[0, 1].set_title('Monthly Average OT by Split | ì›”ë³„ í‰ê·  OT')\naxes[0, 1].tick_params(axis='x', rotation=45)\naxes[0, 1].legend(['Train', 'Validation'])\naxes[0, 1].grid(True, alpha=0.3, axis='y')\n\n# 3. Box plot ë¹„êµ / Box plot comparison\nbox_data = [y_train.values, y_val.values]\nbp = axes[1, 0].boxplot(box_data, labels=['Train', 'Validation'], patch_artist=True)\nbp['boxes'][0].set_facecolor('lightblue')\nbp['boxes'][1].set_facecolor('lightcoral')\naxes[1, 0].set_ylabel('OT')\naxes[1, 0].set_title('OT Boxplot: Train vs Validation | OT ë°•ìŠ¤í”Œë¡¯ ë¹„êµ')\naxes[1, 0].grid(True, alpha=0.3, axis='y')\n\n# í†µê³„ ì •ë³´ ì¶”ê°€ / Add statistics\nstats_text = f\"Train: mean={y_train.mean():.2f}, std={y_train.std():.2f}\\n\"\nstats_text += f\"Val: mean={y_val.mean():.2f}, std={y_val.std():.2f}\"\naxes[1, 0].text(0.5, 0.98, stats_text, transform=axes[1, 0].transAxes, \n                verticalalignment='top', horizontalalignment='center',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# 4. ì‹œê³„ì—´ì—ì„œ ë¶„í•  ì§€ì  í‘œì‹œ / Show split point in time series\nfull_dates = train_with_date['date'].values\nfull_ot = train_with_date['OT'].values\naxes[1, 1].plot(full_dates, full_ot, alpha=0.5, linewidth=0.5, color='gray')\naxes[1, 1].axvline(split_date, color='red', linestyle='--', linewidth=2, label='Split Point')\naxes[1, 1].fill_betweenx([full_ot.min(), full_ot.max()], full_dates[0], split_date, \n                          alpha=0.1, color='blue', label='Train')\naxes[1, 1].fill_betweenx([full_ot.min(), full_ot.max()], split_date, full_dates[-1], \n                          alpha=0.1, color='red', label='Validation')\naxes[1, 1].set_xlabel('Date')\naxes[1, 1].set_ylabel('OT')\naxes[1, 1].set_title('Time Series with Split Point | ë¶„í•  ì§€ì ì´ í‘œì‹œëœ ì‹œê³„ì—´')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================\n# ìµœì¢… ìš”ì•½ | Final Summary\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Data Preparation Summary | ë°ì´í„° ì¤€ë¹„ ìš”ì•½]\")\nprint(\"=\" * 70)\nprint(f\"âœ… í•™ìŠµ ì„¸íŠ¸ / Training set: {len(X_train):,} samples\")\nprint(f\"âœ… ê²€ì¦ ì„¸íŠ¸ / Validation set: {len(X_val):,} samples\")\nprint(f\"âœ… íŠ¹ì„± ìˆ˜ / Number of features: {len(X.columns)}\")\nprint(f\"âš ï¸ ë¶„í¬ ì°¨ì´ í™•ì¸ë¨ (ê³„ì ˆì  ìš”ì¸) / Distribution difference detected (seasonal)\")\nprint(f\"   ëª¨ë¸ì´ ì´ëŸ¬í•œ ë³€ë™ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ”ì§€ í‰ê°€ í•„ìš”\")\nprint(f\"   Need to evaluate if model can generalize to these variations\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt5V8t_6NW3J"
   },
   "source": [
    "### Q5. LightGBMì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•œ í›„, ê²€ì¦ ì„¸íŠ¸ì— ëŒ€í•œ RMSEë¥¼ ê³„ì‚°í•˜ì„¸ìš”. í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” num_leaves=31, n_estimators=100, learning_rate=0.05ë¡œ ì„¤ì •í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1-aHy97O0w3"
   },
   "outputs": [],
   "source": "# ============================================================\n# A5. LightGBM ê¸°ì¤€ ëª¨ë¸ í•™ìŠµ (íŠ¹ì„± ì¤‘ìš”ë„ ë° ì”ì°¨ ë¶„ì„ í¬í•¨)\n# A5. Train LightGBM Baseline Model (with Feature Importance & Residual Analysis)\n# ============================================================\n# ğŸ”§ ê°œì„ : íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”, ì”ì°¨ ë¶„ì„, ë‚˜ì´ë¸Œ ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ ì¶”ê°€\n# ğŸ”§ Improvement: Added feature importance, residual analysis, naive baseline comparison\n\n# ì§€ì •ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„± | Create model with specified hyperparameters\nlgb_model = lgb.LGBMRegressor(\n    num_leaves=31,\n    n_estimators=100,\n    learning_rate=0.05,\n    random_state=RANDOM_SEED,\n    verbosity=-1\n)\n\n# ëª¨ë¸ í•™ìŠµ | Train model\nlgb_model.fit(X_train, y_train)\n\n# ì˜ˆì¸¡ ë° RMSE ê³„ì‚° | Predict and calculate RMSE\ny_pred = lgb_model.predict(X_val)\nrmse_baseline = np.sqrt(mean_squared_error(y_val, y_pred))\n\nprint(\"=\" * 70)\nprint(\"[LightGBM Baseline Results | LightGBM ê¸°ì¤€ ê²°ê³¼]\")\nprint(\"=\" * 70)\nprint(f\"Hyperparameters: num_leaves=31, n_estimators=100, learning_rate=0.05\")\nprint(f\"Validation RMSE: {rmse_baseline:.6f}\")\n\nif rmse_baseline < 0.5:\n    print(\"âœ… ëª©í‘œ ë‹¬ì„±! / Target achieved! RMSE < 0.5\")\nelse:\n    print(f\"âš ï¸ RMSEê°€ ëª©í‘œ(0.5)ë³´ë‹¤ ë†’ìŒ / RMSE higher than target (0.5)\")\n\n# ============================================================\n# ğŸ”§ NEW: íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ | Feature Importance Analysis\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Feature Importance Analysis | íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„]\")\nprint(\"=\" * 70)\n\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': lgb_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nğŸ“Š Top 15 Features | ìƒìœ„ 15ê°œ íŠ¹ì„±:\")\nprint(importance_df.head(15).to_string(index=False))\n\n# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” | Feature importance visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# 1. ìƒìœ„ 15ê°œ íŠ¹ì„± ì¤‘ìš”ë„ ë°” ì°¨íŠ¸ | Top 15 feature importance bar chart\ntop_features = importance_df.head(15)\ncolors = plt.cm.Blues(np.linspace(0.4, 1, len(top_features)))[::-1]\naxes[0].barh(top_features['feature'], top_features['importance'], color=colors)\naxes[0].set_xlabel('Importance / ì¤‘ìš”ë„')\naxes[0].set_ylabel('Feature / íŠ¹ì„±')\naxes[0].set_title('LightGBM Feature Importance (Top 15) | íŠ¹ì„± ì¤‘ìš”ë„')\naxes[0].invert_yaxis()\naxes[0].grid(True, alpha=0.3, axis='x')\n\n# 2. íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„ | Feature category importance\ndef categorize_feature(name):\n    if 'lag' in name:\n        return 'Lag'\n    elif 'rolling' in name:\n        return 'Rolling'\n    elif 'ewm' in name:\n        return 'EWM'\n    elif 'diff' in name:\n        return 'Diff'\n    elif 'sin' in name or 'cos' in name:\n        return 'Cyclic'\n    elif name in ['hour', 'minute', 'dayofweek', 'month', 'dayofyear', 'quarter', 'is_weekend']:\n        return 'Time'\n    else:\n        return 'Input'\n\nimportance_df['category'] = importance_df['feature'].apply(categorize_feature)\ncategory_importance = importance_df.groupby('category')['importance'].sum().sort_values(ascending=True)\n\ncategory_colors = {'Lag': '#e74c3c', 'Rolling': '#3498db', 'EWM': '#2ecc71', \n                   'Diff': '#9b59b6', 'Cyclic': '#f39c12', 'Time': '#1abc9c', 'Input': '#34495e'}\ncolors_cat = [category_colors.get(cat, 'gray') for cat in category_importance.index]\n\naxes[1].barh(category_importance.index, category_importance.values, color=colors_cat)\naxes[1].set_xlabel('Total Importance / ì´ ì¤‘ìš”ë„')\naxes[1].set_ylabel('Category / ì¹´í…Œê³ ë¦¬')\naxes[1].set_title('Importance by Feature Category | ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„')\naxes[1].grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================\n# ğŸ”§ NEW: ì”ì°¨ ë¶„ì„ | Residual Analysis\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Residual Analysis | ì”ì°¨ ë¶„ì„]\")\nprint(\"=\" * 70)\n\nresiduals = y_val.values - y_pred\n\nprint(f\"\\nğŸ“Š ì”ì°¨ í†µê³„ / Residual Statistics:\")\nprint(f\"   Mean (í¸í–¥): {residuals.mean():.6f}\")\nprint(f\"   Std: {residuals.std():.6f}\")\nprint(f\"   Min: {residuals.min():.6f}\")\nprint(f\"   Max: {residuals.max():.6f}\")\nprint(f\"   Skewness (ì™œë„): {pd.Series(residuals).skew():.4f}\")\n\n# ì”ì°¨ ë¶„ì„ ì‹œê°í™” | Residual analysis visualization\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. ì”ì°¨ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ | Residual distribution histogram\naxes[0, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0, 0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\naxes[0, 0].axvline(residuals.mean(), color='green', linestyle='--', linewidth=2, \n                   label=f'Mean: {residuals.mean():.4f}')\naxes[0, 0].set_xlabel('Residual (Actual - Predicted)')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].set_title('Residual Distribution | ì”ì°¨ ë¶„í¬')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„ | Predicted vs Actual scatter plot\naxes[0, 1].scatter(y_val, y_pred, alpha=0.3, s=10, c='steelblue')\nmin_val, max_val = min(y_val.min(), y_pred.min()), max(y_val.max(), y_pred.max())\naxes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\naxes[0, 1].set_xlabel('Actual OT')\naxes[0, 1].set_ylabel('Predicted OT')\naxes[0, 1].set_title('Predicted vs Actual | ì˜ˆì¸¡ vs ì‹¤ì œ')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. ì”ì°¨ vs ì˜ˆì¸¡ê°’ | Residuals vs Predicted\naxes[1, 0].scatter(y_pred, residuals, alpha=0.3, s=10, c='steelblue')\naxes[1, 0].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[1, 0].set_xlabel('Predicted OT')\naxes[1, 0].set_ylabel('Residual')\naxes[1, 0].set_title('Residuals vs Predicted (Heteroscedasticity Check) | ì”ì°¨ vs ì˜ˆì¸¡ê°’')\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. ì”ì°¨ ì‹œê³„ì—´ | Residual time series\naxes[1, 1].plot(residuals, alpha=0.7, linewidth=0.5)\naxes[1, 1].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[1, 1].axhline(residuals.mean() + 2*residuals.std(), color='orange', linestyle=':', \n                   label=f'+2Ïƒ ({residuals.mean() + 2*residuals.std():.2f})')\naxes[1, 1].axhline(residuals.mean() - 2*residuals.std(), color='orange', linestyle=':', \n                   label=f'-2Ïƒ ({residuals.mean() - 2*residuals.std():.2f})')\naxes[1, 1].set_xlabel('Time Step')\naxes[1, 1].set_ylabel('Residual')\naxes[1, 1].set_title('Residuals Over Time | ì‹œê°„ì— ë”°ë¥¸ ì”ì°¨')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================\n# ğŸ”§ NEW: ë‚˜ì´ë¸Œ ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ | Naive Baseline Comparison\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Naive Baseline Comparison | ë‚˜ì´ë¸Œ ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ]\")\nprint(\"=\" * 70)\n\n# ë°©ë²• 1: ì§ì „ ê°’ ì‚¬ìš© (Persistence model)\n# Method 1: Use previous value (Persistence model)\nif 'OT_lag_1h' in X_val.columns:\n    naive_pred_persistence = X_val['OT_lag_1h'].values\nelse:\n    naive_pred_persistence = y_val.values[:-1]\n    naive_pred_persistence = np.concatenate([[y_val.values[0]], naive_pred_persistence])\n\nrmse_persistence = np.sqrt(mean_squared_error(y_val, naive_pred_persistence))\n\n# ë°©ë²• 2: í•™ìŠµ ë°ì´í„° í‰ê·  ì‚¬ìš©\n# Method 2: Use training data mean\nnaive_pred_mean = np.full(len(y_val), y_train.mean())\nrmse_mean = np.sqrt(mean_squared_error(y_val, naive_pred_mean))\n\n# ë°©ë²• 3: 24ì‹œê°„ ì „ ê°’ ì‚¬ìš© (ì¼ì¼ íŒ¨í„´)\n# Method 3: Use 24-hour ago value (daily pattern)\nif 'OT_lag_24h' in X_val.columns:\n    naive_pred_24h = X_val['OT_lag_24h'].values\n    rmse_24h = np.sqrt(mean_squared_error(y_val, naive_pred_24h))\nelse:\n    rmse_24h = float('nan')\n\nprint(\"\\nğŸ“Š ëª¨ë¸ ë¹„êµ / Model Comparison:\")\nprint(f\"   ë‚˜ì´ë¸Œ (í‰ê· ê°’) RMSE / Naive (Mean): {rmse_mean:.6f}\")\nprint(f\"   ë‚˜ì´ë¸Œ (ì§ì „ ê°’) RMSE / Naive (Persistence): {rmse_persistence:.6f}\")\nprint(f\"   ë‚˜ì´ë¸Œ (24ì‹œê°„ ì „) RMSE / Naive (24h ago): {rmse_24h:.6f}\")\nprint(f\"   LightGBM Baseline RMSE: {rmse_baseline:.6f}\")\n\n# ê°œì„ ìœ¨ ê³„ì‚° / Calculate improvement\nimprovement_mean = (1 - rmse_baseline / rmse_mean) * 100\nimprovement_persistence = (1 - rmse_baseline / rmse_persistence) * 100\n\nprint(f\"\\nğŸ“ˆ ê°œì„ ìœ¨ / Improvement:\")\nprint(f\"   í‰ê· ê°’ ëŒ€ë¹„ / vs Mean: {improvement_mean:.2f}%\")\nprint(f\"   ì§ì „ ê°’ ëŒ€ë¹„ / vs Persistence: {improvement_persistence:.2f}%\")\n\n# ë¹„êµ ì‹œê°í™” | Comparison visualization\nfig, ax = plt.subplots(figsize=(10, 5))\n\nmodels = ['Naive\\n(Mean)', 'Naive\\n(Persistence)', 'Naive\\n(24h ago)', 'LightGBM\\nBaseline']\nrmses = [rmse_mean, rmse_persistence, rmse_24h, rmse_baseline]\ncolors = ['#e74c3c', '#f39c12', '#9b59b6', '#2ecc71']\n\nbars = ax.bar(models, rmses, color=colors, edgecolor='black', alpha=0.8)\nax.axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Target (0.5)')\nax.set_ylabel('RMSE')\nax.set_title('Model RMSE Comparison | ëª¨ë¸ RMSE ë¹„êµ')\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\n# ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ / Show values on bars\nfor bar, rmse in zip(bars, rmses):\n    if not np.isnan(rmse):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n                f'{rmse:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… LightGBM ë² ì´ìŠ¤ë¼ì¸ì´ ëª¨ë“  ë‚˜ì´ë¸Œ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•©ë‹ˆë‹¤.\")\nprint(\"âœ… LightGBM baseline outperforms all naive models.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7va-XmINW3J"
   },
   "source": [
    "### Q6. optunaë¥¼ ì‚¬ìš©í•˜ì—¬ LightGBMì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ê³ , ìµœì ì˜ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ê²€ì¦ ì„¸íŠ¸ì— ëŒ€í•œ RMSEë¥¼ ê³„ì‚°í•˜ì„¸ìš”. RMSEë¥¼ 0.5 ì´í•˜ë¡œ ë‚®ì¶”ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0YMF8HGO11E"
   },
   "outputs": [],
   "source": "# ============================================================\n# A6. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì‹œê°í™” ë° ë¶„ì„ ê°œì„ )\n# A6. Optuna Hyperparameter Tuning (Improved Visualization & Analysis)\n# ============================================================\n# ğŸ”§ ê°œì„ : n_trials 100, MedianPruner, ìµœì í™” íˆìŠ¤í† ë¦¬ ì‹œê°í™”, íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„\n# ğŸ”§ Improvement: n_trials 100, MedianPruner, optimization history, parameter importance\n\nfrom optuna.pruners import MedianPruner\n\ndef objective(trial):\n    \"\"\"\n    Optuna ëª©ì  í•¨ìˆ˜ | Optuna objective function\n    LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” | LightGBM hyperparameter optimization\n    \n    Args:\n        trial: Optuna trial ê°ì²´ / Optuna trial object\n    \n    Returns:\n        RMSE ê°’ (ìµœì†Œí™” ëª©í‘œ) / RMSE value (minimize)\n    \"\"\"\n    params = {\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'random_state': RANDOM_SEED,\n        'verbosity': -1\n    }\n    \n    model = lgb.LGBMRegressor(**params)\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    \n    return rmse\n\nprint(\"=\" * 70)\nprint(\"[Optuna Hyperparameter Tuning | Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹]\")\nprint(\"=\" * 70)\n\n# ğŸ”§ NEW: MedianPruner ì¶”ê°€ - ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì€ trial ì¡°ê¸° ì¢…ë£Œ\n# ğŸ”§ NEW: MedianPruner - Early termination of poor trials\n# ğŸ”§ NEW: n_trials 50 â†’ 100 ì¦ê°€\nprint(\"\\nğŸ“‹ Optuna ì„¤ì • / Optuna Configuration:\")\nprint(\"   - n_trials: 100 (ì´ì „: 50)\")\nprint(\"   - Pruner: MedianPruner (ë¹„íš¨ìœ¨ì  trial ì¡°ê¸° ì¢…ë£Œ)\")\nprint(\"   - Direction: minimize (RMSE ìµœì†Œí™”)\")\n\nstudy = optuna.create_study(\n    direction='minimize',\n    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=5)\n)\n\n# ìµœì í™” ì‹¤í–‰ | Run optimization\nstudy.optimize(objective, n_trials=100, show_progress_bar=True)\n\n# ============================================================\n# ìµœì ì˜ ê²°ê³¼ ì¶œë ¥ | Print Best Results\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Best Trial Results | ìµœì  Trial ê²°ê³¼]\")\nprint(\"=\" * 70)\n\ntrial = study.best_trial\nprint(f\"\\nğŸ† Best Trial RMSE: {trial.value:.6f}\")\nprint(\"\\nğŸ“Š Best Parameters:\")\nfor key, value in trial.params.items():\n    if isinstance(value, float):\n        print(f\"   {key}: {value:.6f}\")\n    else:\n        print(f\"   {key}: {value}\")\n\n# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ | Retrain model with best hyperparameters\nbest_params = trial.params.copy()\nbest_params['random_state'] = RANDOM_SEED\nbest_params['verbosity'] = -1\n\nbest_model = lgb.LGBMRegressor(**best_params)\nbest_model.fit(X_train, y_train)\n\ny_pred_best = best_model.predict(X_val)\nrmse_best = np.sqrt(mean_squared_error(y_val, y_pred_best))\n\nprint(f\"\\nâœ… Optuna Tuned LightGBM Validation RMSE: {rmse_best:.6f}\")\n\nif rmse_best < 0.5:\n    print(\"âœ… ëª©í‘œ ë‹¬ì„±! / Target achieved! RMSE < 0.5\")\nelse:\n    print(\"âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„± / Target not achieved: RMSE >= 0.5\")\n\n# ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ ìœ¨ | Improvement over baseline\nimprovement = (1 - rmse_best / rmse_baseline) * 100\nprint(f\"\\nğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ ìœ¨ / Improvement over baseline: {improvement:.2f}%\")\n\n# ============================================================\n# ğŸ”§ NEW: ìµœì í™” íˆìŠ¤í† ë¦¬ ì‹œê°í™” | Optimization History Visualization\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Optimization History Visualization | ìµœì í™” íˆìŠ¤í† ë¦¬ ì‹œê°í™”]\")\nprint(\"=\" * 70)\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. ìµœì í™” íˆìŠ¤í† ë¦¬ | Optimization history\ntrial_numbers = [t.number for t in study.trials]\ntrial_values = [t.value for t in study.trials]\nbest_values = [min(trial_values[:i+1]) for i in range(len(trial_values))]\n\naxes[0, 0].scatter(trial_numbers, trial_values, alpha=0.5, s=30, c='steelblue', label='Trial Value')\naxes[0, 0].plot(trial_numbers, best_values, 'r-', linewidth=2, label='Best Value')\naxes[0, 0].axhline(y=0.5, color='green', linestyle='--', alpha=0.7, label='Target (0.5)')\naxes[0, 0].set_xlabel('Trial Number')\naxes[0, 0].set_ylabel('RMSE')\naxes[0, 0].set_title('Optimization History | ìµœì í™” íˆìŠ¤í† ë¦¬')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. RMSE ë¶„í¬ | RMSE distribution\naxes[0, 1].hist(trial_values, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0, 1].axvline(trial.value, color='red', linestyle='--', linewidth=2, \n                   label=f'Best: {trial.value:.4f}')\naxes[0, 1].axvline(np.median(trial_values), color='orange', linestyle='--', linewidth=2, \n                   label=f'Median: {np.median(trial_values):.4f}')\naxes[0, 1].set_xlabel('RMSE')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].set_title('RMSE Distribution Across Trials | Trialë³„ RMSE ë¶„í¬')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ | Hyperparameter importance\ntry:\n    importance = optuna.importance.get_param_importances(study)\n    params_sorted = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n    param_names = [p[0] for p in params_sorted]\n    param_importances = [p[1] for p in params_sorted]\n    \n    colors = plt.cm.Reds(np.linspace(0.3, 1, len(param_names)))[::-1]\n    axes[1, 0].barh(param_names, param_importances, color=colors)\n    axes[1, 0].set_xlabel('Importance')\n    axes[1, 0].set_title('Hyperparameter Importance | í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„')\n    axes[1, 0].grid(True, alpha=0.3, axis='x')\nexcept Exception as e:\n    axes[1, 0].text(0.5, 0.5, f'Importance analysis failed:\\n{str(e)}', \n                    ha='center', va='center', transform=axes[1, 0].transAxes)\n\n# 4. ì£¼ìš” íŒŒë¼ë¯¸í„° ë¶„í¬ | Key parameter distribution\n# learning_rate vs RMSE\nlr_values = [t.params.get('learning_rate', None) for t in study.trials]\nlr_values_clean = [(lr, val) for lr, val in zip(lr_values, trial_values) if lr is not None]\nif lr_values_clean:\n    lrs, vals = zip(*lr_values_clean)\n    scatter = axes[1, 1].scatter(lrs, vals, c=trial_numbers, cmap='viridis', alpha=0.6, s=30)\n    axes[1, 1].axhline(y=trial.value, color='red', linestyle='--', alpha=0.7, label='Best RMSE')\n    axes[1, 1].set_xlabel('Learning Rate')\n    axes[1, 1].set_ylabel('RMSE')\n    axes[1, 1].set_xscale('log')\n    axes[1, 1].set_title('Learning Rate vs RMSE | í•™ìŠµë¥  vs RMSE')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    plt.colorbar(scatter, ax=axes[1, 1], label='Trial Number')\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================\n# ì£¼ìš” íŒŒë¼ë¯¸í„° ë¶„ì„ | Key Parameter Analysis\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Key Parameter Analysis | ì£¼ìš” íŒŒë¼ë¯¸í„° ë¶„ì„]\")\nprint(\"=\" * 70)\n\n# ìƒìœ„ 5ê°œ Trial ë¹„êµ | Compare top 5 trials\ntop_trials = sorted(study.trials, key=lambda t: t.value)[:5]\nprint(\"\\nğŸ“Š ìƒìœ„ 5ê°œ Trial ë¹„êµ / Top 5 Trials Comparison:\")\nprint(\"-\" * 80)\nprint(f\"{'Rank':<6} {'RMSE':<12} {'num_leaves':<12} {'n_estimators':<14} {'learning_rate':<14}\")\nprint(\"-\" * 80)\nfor rank, t in enumerate(top_trials, 1):\n    print(f\"{rank:<6} {t.value:<12.6f} {t.params['num_leaves']:<12} \"\n          f\"{t.params['n_estimators']:<14} {t.params['learning_rate']:<14.6f}\")\n\n# íŒŒë¼ë¯¸í„° ë²”ìœ„ ìš”ì•½ | Parameter range summary\nprint(\"\\nğŸ“‹ ìµœì  íŒŒë¼ë¯¸í„° íŠ¹ì„± ë¶„ì„ / Best Parameter Characteristics:\")\nprint(f\"   - num_leaves ({trial.params['num_leaves']}): \", end=\"\")\nif trial.params['num_leaves'] > 100:\n    print(\"ë†’ìŒ â†’ ë³µì¡í•œ íŠ¸ë¦¬ êµ¬ì¡° / High â†’ Complex tree structure\")\nelif trial.params['num_leaves'] < 50:\n    print(\"ë‚®ìŒ â†’ ë‹¨ìˆœí•œ íŠ¸ë¦¬ êµ¬ì¡° / Low â†’ Simple tree structure\")\nelse:\n    print(\"ì¤‘ê°„ / Moderate\")\n\nprint(f\"   - learning_rate ({trial.params['learning_rate']:.4f}): \", end=\"\")\nif trial.params['learning_rate'] < 0.05:\n    print(\"ë‚®ìŒ â†’ ë³´ìˆ˜ì  í•™ìŠµ / Low â†’ Conservative learning\")\nelif trial.params['learning_rate'] > 0.1:\n    print(\"ë†’ìŒ â†’ ë¹ ë¥¸ í•™ìŠµ / High â†’ Fast learning\")\nelse:\n    print(\"ì¤‘ê°„ / Moderate\")\n\nprint(f\"   - reg_alpha ({trial.params['reg_alpha']:.4f}): \", end=\"\")\nif trial.params['reg_alpha'] > 1.0:\n    print(\"ë†’ìŒ â†’ ê°•í•œ L1 ì •ê·œí™” / High â†’ Strong L1 regularization\")\nelse:\n    print(\"ë‚®ìŒ â†’ ì•½í•œ L1 ì •ê·œí™” / Low â†’ Weak L1 regularization\")\n\nprint(\"\\nâœ… Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ / Optuna tuning completed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W556TPbZNW3J"
   },
   "source": [
    "### Q7. PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ GRU ê¸°ë°˜ì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•´, OT ì—´ì„ ì •ê·œí™”(Min-Max Scaling)í•˜ê³  ì‹œê³„ì—´ í˜•íƒœë¡œ ë³€í™˜í•˜ì„¸ìš”. ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ëŠ” 24ì‹œê°„ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YN-JflwO3Vp"
   },
   "outputs": [],
   "source": "# ============================================================\n# A7. GRU ë°ì´í„° ì¤€ë¹„ (ë°ì´í„° ëˆ„ìˆ˜ ìˆ˜ì •) | Prepare data for GRU (Fixed Data Leakage)\n# ============================================================\n# ğŸ”§ CRITICAL FIX: ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í•™ìŠµ ë°ì´í„°ì—ë§Œ fit\n# ğŸ”§ CRITICAL FIX: Fit scaler on training data only\n#\n# âš ï¸ ì´ì „ ì½”ë“œì˜ ë¬¸ì œì  | Previous code issue:\n# data_scaled = scaler.fit_transform(data)  # ì „ì²´ ë°ì´í„°ë¡œ fit - ë°ì´í„° ëˆ„ìˆ˜!\n#\n# âœ… ìˆ˜ì •ëœ ì½”ë“œ | Fixed code:\n# í•™ìŠµ ë°ì´í„°ë¡œë§Œ fití•˜ê³ , ê²€ì¦ ë°ì´í„°ëŠ” transformë§Œ\n# Fit only on training data, transform validation data\n\n# ì‹œí€€ìŠ¤ ê¸¸ì´: 24ì‹œê°„ = 96 íƒ€ì„ ìŠ¤í… (15ë¶„ ê°„ê²©)\n# Sequence length: 24 hours = 96 time steps (15-minute intervals)\nSEQUENCE_LENGTH = 24 * INTERVALS_PER_HOUR  # 96\n\n# ì›ë³¸ train ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ GRUìš©ìœ¼ë¡œ ì‚¬ìš©\n# Reload original train data for GRU (to avoid conflicts with modified data)\ntrain_gru = pd.read_csv(DATA_PATH + 'train.csv')\ntrain_gru['date'] = pd.to_datetime(train_gru['date'])\n\n# ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ | Extract time features\ntrain_gru['hour'] = train_gru['date'].dt.hour\ntrain_gru['dayofweek'] = train_gru['date'].dt.dayofweek\ntrain_gru['month'] = train_gru['date'].dt.month\n\n# ìˆœí™˜ ë³€í™˜ | Cyclic transformation\ntrain_gru['hour_sin'] = np.sin(2 * np.pi * train_gru['hour'] / 24)\ntrain_gru['hour_cos'] = np.cos(2 * np.pi * train_gru['hour'] / 24)\ntrain_gru['dayofweek_sin'] = np.sin(2 * np.pi * train_gru['dayofweek'] / 7)\ntrain_gru['dayofweek_cos'] = np.cos(2 * np.pi * train_gru['dayofweek'] / 7)\ntrain_gru['month_sin'] = np.sin(2 * np.pi * train_gru['month'] / 12)\ntrain_gru['month_cos'] = np.cos(2 * np.pi * train_gru['month'] / 12)\n\n# íŠ¹ì„± ì—´ ì„ íƒ (date ì œì™¸, OTëŠ” ë§ˆì§€ë§‰ì—)\n# Select feature columns (exclude date, OT at the end)\nfeature_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL',\n                'hour', 'dayofweek', 'month',\n                'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos',\n                'month_sin', 'month_cos', 'OT']\n\ndata = train_gru[feature_cols].values\n\n# ============================================================\n# ğŸ”§ CRITICAL FIX: ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í•™ìŠµ ë°ì´í„°ì—ë§Œ fit\n# ğŸ”§ CRITICAL FIX: Fit scaler on training data only\n# ============================================================\n\n# ë¨¼ì € ë¶„í•  ì¸ë±ìŠ¤ ê³„ì‚° (ì‹œí€€ìŠ¤ ìƒì„± ì „)\n# Calculate split index first (before sequence creation)\nsplit_ratio = 0.75\nsplit_idx_raw = int(len(data) * split_ratio)\n\n# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ | Split train/validation data\ntrain_data_raw = data[:split_idx_raw]\nval_data_raw = data[split_idx_raw:]\n\n# âœ… í•™ìŠµ ë°ì´í„°ë¡œë§Œ ìŠ¤ì¼€ì¼ëŸ¬ fit | Fit scaler on training data ONLY\nscaler = MinMaxScaler()\ntrain_data_scaled = scaler.fit_transform(train_data_raw)\n\n# âœ… ê²€ì¦ ë°ì´í„°ëŠ” transformë§Œ | Only transform validation data\nval_data_scaled = scaler.transform(val_data_raw)\n\n# ì „ì²´ ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ê²°í•© (ì‹œí€€ìŠ¤ ìƒì„±ìš©)\n# Combine scaled data for sequence creation\ndata_scaled = np.vstack([train_data_scaled, val_data_scaled])\n\nprint(\"=\" * 70)\nprint(\"[GRU Data Preparation - Fixed | GRU ë°ì´í„° ì¤€ë¹„ - ìˆ˜ì •ë¨]\")\nprint(\"=\" * 70)\nprint(\"\\nğŸ”§ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ì ìš© / Data Leakage Prevention Applied:\")\nprint(\"   âœ… ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í•™ìŠµ ë°ì´í„°ì—ë§Œ fit (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\")\nprint(\"   âœ… Scaler fitted on training data only (prevents data leakage)\")\nprint(f\"   í•™ìŠµ ë°ì´í„° í¬ê¸° / Training data size: {len(train_data_raw)}\")\nprint(f\"   ê²€ì¦ ë°ì´í„° í¬ê¸° / Validation data size: {len(val_data_raw)}\")\n\n# ============================================================\n# ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜ | Sequence Creation Function\n# ============================================================\n\ndef create_sequences(data_scaled, seq_length):\n    \"\"\"\n    ì‹œê³„ì—´ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n    Create time series sequences.\n    \n    ì‹œí€€ìŠ¤ ìƒì„± ë°©ë²• ì„¤ëª… / Sequence creation explanation:\n    ì…ë ¥ ì‹œí€€ìŠ¤ (X): t-95, t-94, ..., t-1, t (ì´ 96 íƒ€ì„ìŠ¤í…)\n    ì¶œë ¥ ê°’ (y): tì˜ OT ê°’\n    \n    ì˜ˆì‹œ (SEQUENCE_LENGTH=96):\n    X[0] = data[0:96]   -> y[0] = data[96]['OT']\n    X[1] = data[1:97]   -> y[1] = data[97]['OT']\n    \n    Args:\n        data_scaled: ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° (n_samples, n_features)\n                    Scaled data (n_samples, n_features)\n        seq_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ / Input sequence length\n    \n    Returns:\n        X: ì…ë ¥ ì‹œí€€ìŠ¤ (n_sequences, seq_length, n_features)\n           Input sequences (n_sequences, seq_length, n_features)\n        y: ì¶œë ¥ ê°’ (n_sequences,)\n           Output values (n_sequences,)\n    \"\"\"\n    X, y = [], []\n    for i in range(seq_length, len(data_scaled)):\n        X.append(data_scaled[i - seq_length:i])\n        y.append(data_scaled[i, -1])  # OTëŠ” ë§ˆì§€ë§‰ ì—´ / OT is the last column\n    return np.array(X), np.array(y)\n\n# ì‹œí€€ìŠ¤ ìƒì„± | Create sequences\nX_array, y_array = create_sequences(data_scaled, SEQUENCE_LENGTH)\n\n# ë¶„í•  ì¸ë±ìŠ¤ ì¡°ì • (ì‹œí€€ìŠ¤ ìƒì„± í›„)\n# Adjust split index (after sequence creation)\n# í•™ìŠµ ë°ì´í„°ì—ì„œ ìƒì„±ëœ ì‹œí€€ìŠ¤ ìˆ˜ ê³„ì‚°\n# Calculate number of sequences from training data\nsplit_idx_gru = split_idx_raw - SEQUENCE_LENGTH\n\nX_train_gru = X_array[:split_idx_gru]\nX_val_gru = X_array[split_idx_gru:]\ny_train_gru = y_array[:split_idx_gru]\ny_val_gru = y_array[split_idx_gru:]\n\n# PyTorch í…ì„œë¡œ ë³€í™˜ | Convert to PyTorch tensors\nX_train_tensor = torch.FloatTensor(X_train_gru)\ny_train_tensor = torch.FloatTensor(y_train_gru).unsqueeze(1)\nX_val_tensor = torch.FloatTensor(X_val_gru)\ny_val_tensor = torch.FloatTensor(y_val_gru).unsqueeze(1)\n\n# DataLoader ìƒì„± | Create DataLoaders\nbatch_size = 64\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nfeature_dim = X_array.shape[2]\n\nprint(f\"\\n[Sequence Statistics | ì‹œí€€ìŠ¤ í†µê³„]\")\nprint(f\"  Sequence length: {SEQUENCE_LENGTH} (24 hours = {SEQUENCE_LENGTH} time steps)\")\nprint(f\"  Feature dimension: {feature_dim}\")\nprint(f\"  Training sequences: {len(X_train_gru)}\")\nprint(f\"  Validation sequences: {len(X_val_gru)}\")\nprint(f\"  Batch size: {batch_size}\")\nprint(f\"\\nâš ï¸ ì°¸ê³  / Note:\")\nprint(f\"   ì‹œí€€ìŠ¤ ìƒì„±ìœ¼ë¡œ ì¸í•´ ì²˜ìŒ {SEQUENCE_LENGTH}ê°œ ìƒ˜í”Œì€ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\nprint(f\"   First {SEQUENCE_LENGTH} samples are not used due to sequence creation.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqmdrvzgNW3K"
   },
   "source": [
    "### Q8. GRU ëª¨ë¸ì„ ì •ì˜í•˜ê³  í•™ìŠµí•œ í›„, ê²€ì¦ ì„¸íŠ¸ì— ëŒ€í•œ RMSEë¥¼ ê³„ì‚°í•˜ì„¸ìš”. ì—í¬í¬ëŠ” 20ë²ˆìœ¼ë¡œ ì„¤ì •í•˜ê³ , ì†ì‹¤ í•¨ìˆ˜ëŠ” MSELoss, ì˜µí‹°ë§ˆì´ì €ëŠ” Adamì„ ì‚¬ìš©í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGZrMEk-O4j4"
   },
   "outputs": [],
   "source": "# ============================================================\n# A8. ë‹¨ë°©í–¥ GRU ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ | Define and Train Unidirectional GRU Model\n# ============================================================\n# ğŸ”§ CRITICAL FIX: ë‹¨ë°©í–¥ GRUë¡œ ë³€ê²½ (ì‹¤ì œ ìš´ì˜ í™˜ê²½ ì í•©)\n# ğŸ”§ CRITICAL FIX: Changed to unidirectional GRU (suitable for production)\n#\n# âš ï¸ ì–‘ë°©í–¥ GRUì˜ ë¬¸ì œì  | Issue with Bidirectional GRU:\n# - ì–‘ë°©í–¥ GRUëŠ” ë¯¸ë˜ ì •ë³´(t+1, t+2, ...)ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬(t)ë¥¼ ì˜ˆì¸¡\n# - Bidirectional GRU uses future information (t+1, t+2, ...) to predict present (t)\n# - í•™ìŠµ/ê²€ì¦ì—ì„œëŠ” ë†’ì€ ì„±ëŠ¥, í•˜ì§€ë§Œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ì—ì„œëŠ” ì‚¬ìš© ë¶ˆê°€\n# - High performance in training/validation, but unusable in real-time prediction\n#\n# âœ… ë‹¨ë°©í–¥ GRUì˜ ì¥ì  | Advantages of Unidirectional GRU:\n# - ê³¼ê±° ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ ì˜ˆì¸¡ (ì‹¤ì œ ìš´ì˜ ê°€ëŠ¥)\n# - Uses only past information to predict future (production-ready)\n# - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì— ì ìš© ê°€ëŠ¥\n# - Applicable to real-time streaming data\n\nclass ProductionGRU(nn.Module):\n    \"\"\"\n    ì‹¤ì œ ìš´ì˜ í™˜ê²½ì— ì í•©í•œ ë‹¨ë°©í–¥ GRU ëª¨ë¸\n    Unidirectional GRU model suitable for production environment\n    \n    íŠ¹ì§• / Features:\n    - ë‹¨ë°©í–¥ GRU: ê³¼ê±° â†’ ë¯¸ë˜ ë°©í–¥ìœ¼ë¡œë§Œ ì •ë³´ íë¦„\n    - Unidirectional GRU: Information flows only from past â†’ future\n    - Layer Normalization: í•™ìŠµ ì•ˆì •í™” / Training stabilization\n    - Multi-layer FC: ë¹„ì„ í˜• ì˜ì‚¬ê²°ì • ê²½ê³„ / Non-linear decision boundary\n    - Dropout: ê³¼ì í•© ë°©ì§€ / Overfitting prevention\n    \"\"\"\n    \n    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=1, \n                 dropout=0.3):\n        \"\"\"\n        ëª¨ë¸ ì´ˆê¸°í™” / Initialize model\n        \n        Args:\n            input_size: ì…ë ¥ íŠ¹ì„± ìˆ˜ / Number of input features\n            hidden_size: ì€ë‹‰ ìœ ë‹› ìˆ˜ / Number of hidden units\n            num_layers: GRU ë ˆì´ì–´ ìˆ˜ / Number of GRU layers\n            output_size: ì¶œë ¥ í¬ê¸° / Output size\n            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ / Dropout rate\n        \"\"\"\n        super(ProductionGRU, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # ë‹¨ë°©í–¥ GRU ë ˆì´ì–´ | Unidirectional GRU layer\n        # bidirectional=False: ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥\n        # bidirectional=False: Usable in production environment\n        self.gru = nn.GRU(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=False  # âœ… ë‹¨ë°©í–¥ GRU | Unidirectional GRU\n        )\n        \n        # Layer Normalization: í•™ìŠµ ì•ˆì •í™”\n        # Layer Normalization: Stabilizes training\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        \n        # ì¶œë ¥ ë ˆì´ì–´ (ë‹¤ì¸µ êµ¬ì¡°)\n        # Output layers (multi-layer structure)\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout / 2),\n            nn.Linear(hidden_size // 2, output_size)\n        )\n        \n        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” / Weight initialization\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"\n        Xavier/Kaiming ì´ˆê¸°í™” ì ìš©\n        Apply Xavier/Kaiming initialization\n        \n        ì¢‹ì€ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ëŠ” í•™ìŠµ ìˆ˜ë ´ ì†ë„ì™€ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n        Good weight initialization improves learning convergence and stability.\n        \"\"\"\n        for name, param in self.named_parameters():\n            if 'weight_ih' in name:\n                nn.init.xavier_uniform_(param.data)\n            elif 'weight_hh' in name:\n                nn.init.orthogonal_(param.data)\n            elif 'bias' in name:\n                param.data.fill_(0)\n    \n    def forward(self, x):\n        \"\"\"\n        ìˆœì „íŒŒ / Forward pass\n        \n        Args:\n            x: ì…ë ¥ í…ì„œ (batch, seq_len, input_size)\n               Input tensor (batch, seq_len, input_size)\n        \n        Returns:\n            ì¶œë ¥ í…ì„œ (batch, output_size)\n            Output tensor (batch, output_size)\n        \"\"\"\n        # GRU ìˆœì „íŒŒ / GRU forward pass\n        gru_out, _ = self.gru(x)\n        \n        # ë§ˆì§€ë§‰ íƒ€ì„ ìŠ¤í… ì¶œë ¥ ì‚¬ìš© / Use last time step output\n        last_output = gru_out[:, -1, :]\n        \n        # Layer Normalization ì ìš© / Apply Layer Normalization\n        normalized = self.layer_norm(last_output)\n        \n        # ì¶œë ¥ ë ˆì´ì–´ í†µê³¼ / Pass through output layers\n        output = self.fc(normalized)\n        \n        return output\n\n# ============================================================\n# ëª¨ë¸ í•™ìŠµ ì„¤ì • / Model Training Configuration\n# ============================================================\n\n# í•˜ì´í¼íŒŒë¼ë¯¸í„° / Hyperparameters\nHIDDEN_SIZE = 128\nNUM_LAYERS = 2\nDROPOUT = 0.3\nLEARNING_RATE = 0.001\nEPOCHS = 50  # Early Stopping ì‚¬ìš©ìœ¼ë¡œ ì‹¤ì œ ì—í¬í¬ëŠ” ë” ì ì„ ìˆ˜ ìˆìŒ\nPATIENCE = 10  # Early Stopping patience\n\n# ëª¨ë¸ ì´ˆê¸°í™” / Initialize model\nmodel = ProductionGRU(\n    input_size=feature_dim,\n    hidden_size=HIDDEN_SIZE,\n    num_layers=NUM_LAYERS,\n    dropout=DROPOUT\n).to(device)\n\n# ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ / Loss, Optimizer, Scheduler\ncriterion = nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n\n# Learning Rate Scheduler: ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ\n# Learning Rate Scheduler: Reduce LR when validation loss doesn't improve\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\nprint(\"=\" * 70)\nprint(\"[Production GRU Model | ìš´ì˜ í™˜ê²½ìš© GRU ëª¨ë¸]\")\nprint(\"=\" * 70)\nprint(\"\\nâœ… ë‹¨ë°©í–¥ GRU: ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥\")\nprint(\"âœ… Unidirectional GRU: Usable in production environment\")\nprint(\"\\nArchitecture / ì•„í‚¤í…ì²˜:\")\nprint(f\"  - Input size: {feature_dim}\")\nprint(f\"  - Hidden size: {HIDDEN_SIZE}\")\nprint(f\"  - Num layers: {NUM_LAYERS}\")\nprint(f\"  - Bidirectional: False (ë‹¨ë°©í–¥)\")\nprint(f\"  - Dropout: {DROPOUT}\")\nprint(f\"\\nTraining config / í•™ìŠµ ì„¤ì •:\")\nprint(f\"  - Learning rate: {LEARNING_RATE}\")\nprint(f\"  - Epochs: {EPOCHS} (with Early Stopping)\")\nprint(f\"  - Patience: {PATIENCE}\")\nprint(f\"  - Device: {device}\")\n\n# ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° / Calculate model parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nModel Parameters / ëª¨ë¸ íŒŒë¼ë¯¸í„°:\")\nprint(f\"  - Total: {total_params:,}\")\nprint(f\"  - Trainable: {trainable_params:,}\")\n\n# ============================================================\n# ëª¨ë¸ í•™ìŠµ (Early Stopping ì ìš©) / Train Model with Early Stopping\n# ============================================================\n\nbest_val_loss = float('inf')\nbest_model_state = None\npatience_counter = 0\ntrain_losses = []\nval_losses = []\nlr_history = []\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Training Started | í•™ìŠµ ì‹œì‘]\")\nprint(\"=\" * 70)\n\nfor epoch in range(EPOCHS):\n    # ------------------------------------------------------------\n    # í•™ìŠµ ë‹¨ê³„ / Training phase\n    # ------------------------------------------------------------\n    model.train()\n    train_loss = 0.0\n    \n    for X_batch, y_batch in train_loader:\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        # ìˆœì „íŒŒ / Forward pass\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        \n        # ì—­ì „íŒŒ / Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient Clipping: ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€\n        # Gradient Clipping: Prevent gradient explosion\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        train_loss += loss.item()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n    \n    # ------------------------------------------------------------\n    # ê²€ì¦ ë‹¨ê³„ / Validation phase\n    # ------------------------------------------------------------\n    model.eval()\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n    \n    # í˜„ì¬ í•™ìŠµë¥  ê¸°ë¡ / Record current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    lr_history.append(current_lr)\n    \n    # Learning Rate Scheduler ì—…ë°ì´íŠ¸ / Update LR Scheduler\n    scheduler.step(avg_val_loss)\n    \n    # ------------------------------------------------------------\n    # Early Stopping ì²´í¬ / Check Early Stopping\n    # ------------------------------------------------------------\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict().copy()\n        patience_counter = 0\n        improvement = \"âœ“ Improved\"\n    else:\n        patience_counter += 1\n        improvement = \"\"\n    \n    # ì§„í–‰ ìƒí™© ì¶œë ¥ / Print progress\n    if (epoch + 1) % 5 == 0 or improvement:\n        print(f\"Epoch [{epoch + 1:3d}/{EPOCHS}] \"\n              f\"Train Loss: {avg_train_loss:.6f} | \"\n              f\"Val Loss: {avg_val_loss:.6f} | \"\n              f\"LR: {current_lr:.6f} {improvement}\")\n    \n    # Early Stopping ì¡°ê±´ / Early Stopping condition\n    if patience_counter >= PATIENCE:\n        print(f\"\\nâš ï¸ Early Stopping at epoch {epoch + 1}\")\n        print(f\"   Best validation loss: {best_val_loss:.6f}\")\n        break\n\n# ìµœì ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ / Load best model weights\nif best_model_state is not None:\n    model.load_state_dict(best_model_state)\n    print(f\"\\nâœ… Loaded best model (Val Loss: {best_val_loss:.6f})\")\n\n# ============================================================\n# ëª¨ë¸ í‰ê°€ / Model Evaluation\n# ============================================================\nmodel.eval()\nval_predictions = []\nval_targets = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in val_loader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        val_predictions.append(outputs.cpu())\n        val_targets.append(y_batch)\n\n# ì˜ˆì¸¡ ê²°í•© / Concatenate predictions\nval_pred_tensor = torch.cat(val_predictions).numpy()\nval_target_tensor = torch.cat(val_targets).numpy()\n\n# ì—­ë³€í™˜í•˜ì—¬ ì‹¤ì œ ê°’ ì–»ê¸° / Inverse transform to get actual values\nn_features = scaler.n_features_in_\ndummy_pred = np.zeros((len(val_pred_tensor), n_features))\ndummy_target = np.zeros((len(val_target_tensor), n_features))\ndummy_pred[:, -1] = val_pred_tensor.flatten()\ndummy_target[:, -1] = val_target_tensor.flatten()\n\nval_pred_actual = scaler.inverse_transform(dummy_pred)[:, -1]\nval_target_actual = scaler.inverse_transform(dummy_target)[:, -1]\n\n# RMSE ê³„ì‚° / Calculate RMSE\ngru_rmse = np.sqrt(mean_squared_error(val_target_actual, val_pred_actual))\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[GRU Model Results | GRU ëª¨ë¸ ê²°ê³¼]\")\nprint(\"=\" * 70)\nprint(f\"âœ… Validation RMSE: {gru_rmse:.6f}\")\n\nif gru_rmse < 0.5:\n    print(\"âœ… ëª©í‘œ ë‹¬ì„±! / Target achieved! RMSE < 0.5\")\nelse:\n    print(f\"âš ï¸ RMSEê°€ ëª©í‘œ(0.5)ë³´ë‹¤ ë†’ìŒ. ì¶”ê°€ íŠœë‹ í•„ìš”.\")\n    print(f\"âš ï¸ RMSE is higher than target (0.5). Additional tuning needed.\")\n\n# ============================================================\n# í•™ìŠµ ê³¡ì„  ì‹œê°í™” / Visualize Learning Curves\n# ============================================================\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# 1. ì†ì‹¤ ê³¡ì„  | Loss curves\naxes[0].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\naxes[0].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss (MSE)')\naxes[0].set_title('Training and Validation Loss | í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# 2. í•™ìŠµë¥  ë³€í™” | Learning rate changes\naxes[1].plot(lr_history, color='green', linewidth=2)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Learning Rate')\naxes[1].set_title('Learning Rate Schedule | í•™ìŠµë¥  ìŠ¤ì¼€ì¤„')\naxes[1].grid(True, alpha=0.3)\n\n# 3. ê³¼ì í•© ê°­ | Overfitting gap\ngap = np.array(val_losses) - np.array(train_losses)\naxes[2].plot(gap, color='purple', linewidth=2)\naxes[2].axhline(0, color='gray', linestyle='--')\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('Validation - Train Loss')\naxes[2].set_title('Overfitting Gap | ê³¼ì í•© ê°­')\naxes[2].fill_between(range(len(gap)), gap, 0, where=(gap > 0), alpha=0.3, color='red', label='Overfitting')\naxes[2].fill_between(range(len(gap)), gap, 0, where=(gap <= 0), alpha=0.3, color='green', label='Good')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ì˜ˆì¸¡ vs ì‹¤ì œ ì‹œê°í™” / Prediction vs Actual visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# ì‹œê³„ì—´ ë¹„êµ (ë§ˆì§€ë§‰ 500 ìƒ˜í”Œ) | Time series comparison (last 500 samples)\nsample_size = min(500, len(val_target_actual))\naxes[0].plot(val_target_actual[-sample_size:], label='Actual', alpha=0.8, linewidth=1)\naxes[0].plot(val_pred_actual[-sample_size:], label='Predicted', alpha=0.8, linewidth=1)\naxes[0].set_xlabel('Time Step')\naxes[0].set_ylabel('OT (Oil Temperature)')\naxes[0].set_title(f'GRU Prediction vs Actual (Last {sample_size}) | GRU ì˜ˆì¸¡ vs ì‹¤ì œ')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# ì”ì°¨ ë¶„í¬ | Residual distribution\nresiduals = val_target_actual - val_pred_actual\naxes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\naxes[1].axvline(0, color='red', linestyle='--', label=f'Mean: {residuals.mean():.4f}')\naxes[1].set_xlabel('Residual (Actual - Predicted)')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('GRU Residual Distribution | GRU ì”ì°¨ ë¶„í¬')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzebHrqS6Tdz"
   },
   "source": [
    "### Q9. ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ test ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”. train ë°ì´í„°ì—ì„œ ì‚¬ìš©í–ˆë˜ ì „ì²˜ë¦¬ë¥¼ ë™ì¼í•˜ê²Œ ì ìš©í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S40s8fXM6Tdz"
   },
   "outputs": [],
   "source": "# ============================================================\n# A9. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ (ë°˜ë³µ ì˜ˆì¸¡ êµ¬í˜„)\n# A9. Test Data Preprocessing (Iterative Forecasting Implementation)\n# ============================================================\n# ğŸ”§ CRITICAL FIX: ë°˜ë³µ ì˜ˆì¸¡(Iterative Forecasting) ì™„ì „ êµ¬í˜„\n# ğŸ”§ CRITICAL FIX: Complete Iterative Forecasting Implementation\n#\n# âš ï¸ ë¬¸ì œì  / Issue:\n# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” OT ê°’ì´ ì—†ìœ¼ë¯€ë¡œ lag/rolling íŠ¹ì„±ì„ ì§ì ‘ ê³„ì‚°í•  ìˆ˜ ì—†ìŒ\n# Test data doesn't have OT values, so lag/rolling features cannot be computed directly\n#\n# âœ… í•´ê²°ì±…: ë°˜ë³µ ì˜ˆì¸¡ (Iterative Forecasting)\n# âœ… Solution: Iterative Forecasting\n# 1. ì²« ì˜ˆì¸¡ì— í•™ìŠµ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ OT ê°’ë“¤ ì‚¬ìš©\n#    Use last OT values from training data for first prediction\n# 2. ì´í›„ ì˜ˆì¸¡ì— ì´ì „ ì˜ˆì¸¡ê°’ë“¤ì„ ì‚¬ìš©í•˜ì—¬ lag/rolling íŠ¹ì„± ì—…ë°ì´íŠ¸\n#    Update lag/rolling features using previous predictions\n\nprint(\"=\" * 70)\nprint(\"[Test Data Preprocessing with Iterative Forecasting]\")\nprint(\"[ë°˜ë³µ ì˜ˆì¸¡ì„ í™œìš©í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬]\")\nprint(\"=\" * 70)\n\n# ============================================================\n# ë°˜ë³µ ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ | Define Iterative Forecasting Function\n# ============================================================\n\ndef iterative_forecasting(test_df, train_df, model, lag_hours, rolling_hours, \n                          feature_columns, batch_size=100):\n    \"\"\"\n    ë°˜ë³µ ì˜ˆì¸¡ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n    Preprocess test data and make predictions using iterative forecasting.\n    \n    ë°˜ë³µ ì˜ˆì¸¡ ì›ë¦¬ / Iterative Forecasting Principle:\n    1. ê° í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰\n       Make predictions sequentially for each test sample\n    2. ì´ì „ ì˜ˆì¸¡ê°’ì„ ë‹¤ìŒ ìƒ˜í”Œì˜ lag/rolling íŠ¹ì„± ê³„ì‚°ì— ì‚¬ìš©\n       Use previous predictions to calculate lag/rolling features for next sample\n    3. ì˜ˆì¸¡ ì˜¤ì°¨ê°€ ëˆ„ì ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¨ê¸° ì˜ˆì¸¡ì— ë” ì í•©\n       More suitable for short-term predictions as errors may accumulate\n    \n    Args:\n        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ / Test DataFrame\n        train_df: í•™ìŠµ ë°ì´í„°í”„ë ˆì„ (ë§ˆì§€ë§‰ OT ê°’ë“¤ í•„ìš”) / Training DataFrame\n        model: í•™ìŠµëœ ì˜ˆì¸¡ ëª¨ë¸ / Trained prediction model\n        lag_hours: ì§€ì—° ì‹œê°„ ë¦¬ìŠ¤íŠ¸ / List of lag hours\n        rolling_hours: ë¡¤ë§ ìœˆë„ìš° ë¦¬ìŠ¤íŠ¸ / List of rolling window hours\n        feature_columns: ëª¨ë¸ ì…ë ¥ íŠ¹ì„± ì—´ ëª©ë¡ / Model input feature columns\n        batch_size: ì§„í–‰ ìƒí™© ì¶œë ¥ ê°„ê²© / Progress output interval\n    \n    Returns:\n        test_final: ì „ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° / Preprocessed test data\n        predictions: ì˜ˆì¸¡ê°’ ë¦¬ìŠ¤íŠ¸ / List of predictions\n    \"\"\"\n    test_processed = test_df.copy()\n    \n    # í•™ìŠµ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ OT ê°’ë“¤ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ lag ë§Œí¼)\n    # Get last OT values from training data (up to max lag)\n    max_lag_steps = max(lag_hours) * INTERVALS_PER_HOUR\n    train_ot_history = list(train_df['OT'].values[-max_lag_steps:])\n    \n    # ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ / List to store predictions\n    predictions = []\n    \n    # ëª¨ë“  íŠ¹ì„± ì—´ì´ test_processedì— ìˆëŠ”ì§€ í™•ì¸\n    # Ensure all feature columns exist in test_processed\n    missing_cols = set(feature_columns) - set(test_processed.columns)\n    if missing_cols:\n        print(f\"âš ï¸ ëˆ„ë½ëœ ì—´ ìƒì„± ì¤‘ / Creating missing columns: {missing_cols}\")\n        for col in missing_cols:\n            test_processed[col] = 0.0\n    \n    print(f\"\\nğŸ“Š ë°˜ë³µ ì˜ˆì¸¡ ì‹œì‘ / Starting Iterative Forecasting\")\n    print(f\"   í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜ / Test samples: {len(test_processed)}\")\n    print(f\"   OT íˆìŠ¤í† ë¦¬ ê¸¸ì´ / OT history length: {len(train_ot_history)}\")\n    \n    # ê° í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì— ëŒ€í•´ ë°˜ë³µ ì˜ˆì¸¡\n    # Iterative prediction for each test sample\n    for idx in tqdm(range(len(test_processed)), desc=\"Iterative Forecasting\"):\n        # í˜„ì¬ ì‹œì ê¹Œì§€ì˜ OT íˆìŠ¤í† ë¦¬ (í•™ìŠµ + ì´ì „ ì˜ˆì¸¡)\n        # OT history up to current point (training + previous predictions)\n        current_history = train_ot_history + predictions\n        \n        # ------------------------------------------------------------\n        # Lag íŠ¹ì„± ê³„ì‚° | Calculate Lag Features\n        # ------------------------------------------------------------\n        for lag_h in lag_hours:\n            shift_steps = lag_h * INTERVALS_PER_HOUR\n            if len(current_history) >= shift_steps:\n                # shift_stepsë§Œí¼ ì´ì „ì˜ ê°’ ì‚¬ìš©\n                # Use value from shift_steps ago\n                test_processed.loc[test_processed.index[idx], f'OT_lag_{lag_h}h'] = current_history[-shift_steps]\n            else:\n                # íˆìŠ¤í† ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ ê°€ì¥ ì˜¤ë˜ëœ ê°’ ì‚¬ìš©\n                # Use oldest value if history is insufficient\n                test_processed.loc[test_processed.index[idx], f'OT_lag_{lag_h}h'] = current_history[0]\n        \n        # ------------------------------------------------------------\n        # Rolling íŠ¹ì„± ê³„ì‚° | Calculate Rolling Features\n        # ------------------------------------------------------------\n        for window_h in rolling_hours:\n            window_steps = window_h * INTERVALS_PER_HOUR\n            if len(current_history) >= window_steps:\n                window_data = current_history[-window_steps:]\n            else:\n                window_data = current_history\n            \n            test_processed.loc[test_processed.index[idx], f'OT_rolling_mean_{window_h}h'] = np.mean(window_data)\n            if len(window_data) > 1:\n                test_processed.loc[test_processed.index[idx], f'OT_rolling_std_{window_h}h'] = np.std(window_data)\n            else:\n                test_processed.loc[test_processed.index[idx], f'OT_rolling_std_{window_h}h'] = 0.0\n        \n        # ------------------------------------------------------------\n        # ì˜ˆì¸¡ ìˆ˜í–‰ | Make Prediction\n        # ------------------------------------------------------------\n        # í˜„ì¬ í–‰ì˜ íŠ¹ì„± ì¶”ì¶œ / Extract features for current row\n        row_features = test_processed.loc[test_processed.index[idx], feature_columns].values.reshape(1, -1)\n        \n        # ì˜ˆì¸¡ / Predict\n        pred_ot = model.predict(row_features)[0]\n        predictions.append(pred_ot)\n        \n        # ì§„í–‰ ìƒí™© ì¶œë ¥ / Print progress\n        if (idx + 1) % batch_size == 0:\n            print(f\"   Progress: {idx + 1}/{len(test_processed)} \"\n                  f\"(Last pred: {pred_ot:.4f})\")\n    \n    # íŠ¹ì„± ì—´ ìˆœì„œ ì •ë ¬ / Align feature column order\n    test_final = test_processed[feature_columns].copy()\n    \n    return test_final, predictions\n\n# ============================================================\n# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ | Execute Test Data Preprocessing\n# ============================================================\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³µì‚¬ / Copy test data\ntest_processed = test.copy()\n\n# ì‹œê°„ íŠ¹ì„± ì¶”ê°€ (trainê³¼ ë™ì¼í•˜ê²Œ) / Add time features (same as train)\n# ì´ë¯¸ Q2ì—ì„œ ì¶”ê°€ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í™•ì¸ë§Œ ìˆ˜í–‰\n# Already added in Q2, just verify\nprint(\"\\n[ì‹œê°„ íŠ¹ì„± í™•ì¸ / Verify Time Features]\")\ntime_features = ['hour', 'minute', 'dayofweek', 'month', 'dayofyear',\n                 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos',\n                 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos',\n                 'dayofyear_sin', 'dayofyear_cos']\n\nfor feat in time_features:\n    if feat not in test_processed.columns:\n        print(f\"   âš ï¸ ëˆ„ë½ëœ íŠ¹ì„±: {feat}\")\n\n# Lag/Rolling ì—´ ì´ˆê¸°í™” / Initialize Lag/Rolling columns\nfor lag_h in lag_hours:\n    test_processed[f'OT_lag_{lag_h}h'] = 0.0\nfor window_h in rolling_hours:\n    test_processed[f'OT_rolling_mean_{window_h}h'] = 0.0\n    test_processed[f'OT_rolling_std_{window_h}h'] = 0.0\n\n# date ì—´ ì œê±° / Remove date column\nif 'date' in test_processed.columns:\n    test_processed = test_processed.drop(columns=['date'])\n\n# íŠ¹ì„± ì—´ ëª©ë¡ (X_trainê³¼ ë™ì¼í•œ ìˆœì„œ)\n# Feature column list (same order as X_train)\nfeature_columns = list(X_train.columns)\n\nprint(f\"\\n[íŠ¹ì„± ì—´ ìˆ˜ / Feature columns: {len(feature_columns)}]\")\n\n# ============================================================\n# ë°˜ë³µ ì˜ˆì¸¡ ì‹¤í–‰ | Execute Iterative Forecasting\n# ============================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Iterative Forecasting Execution | ë°˜ë³µ ì˜ˆì¸¡ ì‹¤í–‰]\")\nprint(\"=\" * 70)\n\n# LightGBM ëª¨ë¸ë¡œ ë°˜ë³µ ì˜ˆì¸¡ ìˆ˜í–‰\n# Execute iterative forecasting with LightGBM model\ntest_final, test_predictions = iterative_forecasting(\n    test_processed, \n    train,  # ì›ë³¸ train ë°ì´í„° (OT ê°’ í¬í•¨)\n    best_model,  # Q6ì—ì„œ íŠœë‹ëœ LightGBM ëª¨ë¸\n    lag_hours, \n    rolling_hours,\n    feature_columns,\n    batch_size=1000\n)\n\n# ============================================================\n# ê²°ê³¼ í™•ì¸ | Verify Results\n# ============================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Iterative Forecasting Results | ë°˜ë³µ ì˜ˆì¸¡ ê²°ê³¼]\")\nprint(\"=\" * 70)\n\nprint(f\"\\nâœ… ì „ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° shape / Preprocessed test data shape: {test_final.shape}\")\nprint(f\"âœ… ì˜ˆì¸¡ ìˆ˜ / Number of predictions: {len(test_predictions)}\")\n\n# ì˜ˆì¸¡ê°’ í†µê³„ / Prediction statistics\npredictions_array = np.array(test_predictions)\nprint(f\"\\n[ì˜ˆì¸¡ê°’ í†µê³„ / Prediction Statistics]\")\nprint(f\"   Mean: {predictions_array.mean():.4f}\")\nprint(f\"   Std: {predictions_array.std():.4f}\")\nprint(f\"   Min: {predictions_array.min():.4f}\")\nprint(f\"   Max: {predictions_array.max():.4f}\")\n\n# Lag íŠ¹ì„± ë³€í™” í™•ì¸ / Verify lag feature changes\nprint(f\"\\n[Lag íŠ¹ì„± ë³€í™” í™•ì¸ / Lag Feature Variation Check]\")\nfor lag_h in lag_hours[:3]:  # ì²˜ìŒ 3ê°œë§Œ í™•ì¸\n    col = f'OT_lag_{lag_h}h'\n    unique_vals = test_final[col].nunique()\n    print(f\"   {col}: {unique_vals} unique values (should be > 1)\")\n\n# ============================================================\n# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™” | Visualize Prediction Results\n# ============================================================\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. ì˜ˆì¸¡ê°’ ì‹œê³„ì—´ / Prediction time series\naxes[0, 0].plot(test_predictions, linewidth=0.5, alpha=0.8)\naxes[0, 0].set_xlabel('Time Step')\naxes[0, 0].set_ylabel('Predicted OT')\naxes[0, 0].set_title('Test Predictions (Iterative Forecasting) | í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (ë°˜ë³µ ì˜ˆì¸¡)')\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. ì˜ˆì¸¡ê°’ ë¶„í¬ / Prediction distribution\naxes[0, 1].hist(test_predictions, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0, 1].axvline(np.mean(test_predictions), color='red', linestyle='--', \n                   label=f'Mean: {np.mean(test_predictions):.2f}')\naxes[0, 1].set_xlabel('Predicted OT')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].set_title('Prediction Distribution | ì˜ˆì¸¡ê°’ ë¶„í¬')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Lag íŠ¹ì„± ë³€í™” / Lag feature variation\nsample_indices = range(min(1000, len(test_final)))\naxes[1, 0].plot(sample_indices, test_final['OT_lag_1h'].iloc[:1000], \n                label='OT_lag_1h', alpha=0.7)\naxes[1, 0].plot(sample_indices, test_final['OT_lag_6h'].iloc[:1000], \n                label='OT_lag_6h', alpha=0.7)\naxes[1, 0].plot(sample_indices, test_final['OT_lag_24h'].iloc[:1000], \n                label='OT_lag_24h', alpha=0.7)\naxes[1, 0].set_xlabel('Time Step (first 1000)')\naxes[1, 0].set_ylabel('Lag Feature Value')\naxes[1, 0].set_title('Lag Features (Dynamic Update) | Lag íŠ¹ì„± (ë™ì  ì—…ë°ì´íŠ¸)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Rolling íŠ¹ì„± ë³€í™” / Rolling feature variation\naxes[1, 1].plot(sample_indices, test_final['OT_rolling_mean_6h'].iloc[:1000], \n                label='Rolling Mean 6h', alpha=0.7)\naxes[1, 1].plot(sample_indices, test_final['OT_rolling_mean_24h'].iloc[:1000], \n                label='Rolling Mean 24h', alpha=0.7)\naxes[1, 1].fill_between(sample_indices, \n                        test_final['OT_rolling_mean_6h'].iloc[:1000] - test_final['OT_rolling_std_6h'].iloc[:1000],\n                        test_final['OT_rolling_mean_6h'].iloc[:1000] + test_final['OT_rolling_std_6h'].iloc[:1000],\n                        alpha=0.2, label='Â±1 Std (6h)')\naxes[1, 1].set_xlabel('Time Step (first 1000)')\naxes[1, 1].set_ylabel('Rolling Feature Value')\naxes[1, 1].set_title('Rolling Features (Dynamic Update) | Rolling íŠ¹ì„± (ë™ì  ì—…ë°ì´íŠ¸)')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================\n# í•œê³„ì  ë¬¸ì„œí™” | Document Limitations\n# ============================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Iterative Forecasting Limitations | ë°˜ë³µ ì˜ˆì¸¡ì˜ í•œê³„ì ]\")\nprint(\"=\" * 70)\nprint(\"\"\"\nâš ï¸ ì¤‘ìš”í•œ í•œê³„ì  / Important Limitations:\n\n1. ì˜ˆì¸¡ ì˜¤ì°¨ ëˆ„ì  / Prediction Error Accumulation:\n   - ê° ì˜ˆì¸¡ì—ì„œ ë°œìƒí•œ ì˜¤ì°¨ê°€ ë‹¤ìŒ ì˜ˆì¸¡ì˜ ì…ë ¥(lag íŠ¹ì„±)ìœ¼ë¡œ ì‚¬ìš©ë¨\n   - Errors from each prediction are used as input (lag features) for next prediction\n   - ì¥ê¸° ì˜ˆì¸¡ì¼ìˆ˜ë¡ ì˜¤ì°¨ê°€ ëˆ„ì ë˜ì–´ ì •í™•ë„ ê°ì†Œ\n   - Accuracy decreases for longer forecasts due to error accumulation\n\n2. ê³„ì‚° ë¹„ìš© / Computational Cost:\n   - ê° ìƒ˜í”Œì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•´ì•¼ í•˜ë¯€ë¡œ ë³‘ë ¬í™” ë¶ˆê°€\n   - Cannot parallelize as predictions must be made sequentially\n   - ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼\n   - Time-consuming for large test datasets\n\n3. ê¶Œì¥ ì‚¬í•­ / Recommendations:\n   - ë‹¨ê¸° ì˜ˆì¸¡ (1-24ì‹œê°„)ì— ì§‘ì¤‘ / Focus on short-term predictions\n   - ì‹¤ì œ ìš´ì˜ ì‹œ ì£¼ê¸°ì  ëª¨ë¸ ì¬í•™ìŠµ / Periodic model retraining in production\n   - ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ëª¨ë‹ˆí„°ë§ / Monitor prediction uncertainty\n   - ì‹¤ì œ OT ê°’ì„ ì–»ìœ¼ë©´ lag íŠ¹ì„± ì—…ë°ì´íŠ¸ / Update lag features when actual OT is available\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwY0Tl7XNW3K"
   },
   "source": [
    "### Q10. ë§ˆì§€ë§‰ìœ¼ë¡œ, LightGBM ëª¨ë¸ê³¼ GRU ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ì•™ìƒë¸”í•˜ì—¬ ê²€ì¦ ì„¸íŠ¸ì— ëŒ€í•œ RMSEë¥¼ ê³„ì‚°í•˜ì„¸ìš”. ì•™ìƒë¸” ë°©ë²•ìœ¼ë¡œ ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì˜ í‰ê· ì„ ì‚¬ìš©í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVSNH1XmO53B"
   },
   "outputs": [],
   "source": "# ============================================================\n# A10. ê°œì„ ëœ ì•™ìƒë¸” ì˜ˆì¸¡ (ë‹¤ì–‘í•œ ë°©ë²• ë° ë¶„ì„)\n# A10. Improved Ensemble Predictions (Various Methods & Analysis)\n# ============================================================\n# ğŸ”§ ê°œì„ : ë‹¤ì–‘í•œ ì•™ìƒë¸” ë°©ë²•, ëª¨ë¸ ë‹¤ì–‘ì„± ë¶„ì„, ê°œì„ ëœ ì‹œê°í™”\n# ğŸ”§ Improvement: Various ensemble methods, model diversity analysis, improved visualization\n\nprint(\"=\" * 70)\nprint(\"[Ensemble Predictions | ì•™ìƒë¸” ì˜ˆì¸¡]\")\nprint(\"=\" * 70)\n\n# ============================================================\n# 1. ì˜ˆì¸¡ ì •ë ¬ | Align Predictions\n# ============================================================\n# GRUëŠ” ì‹œí€€ìŠ¤ ìƒì„±ìœ¼ë¡œ ì¸í•´ ì˜ˆì¸¡ì´ ë” ì ìŒ\n# GRU has fewer predictions due to sequence creation\nmin_len = min(len(y_pred_best), len(val_pred_actual))\n\n# ë§ˆì§€ë§‰ min_len ì˜ˆì¸¡ ì‚¬ìš© (ì‹œê°„ ìˆœì„œ ìœ ì§€)\n# Use last min_len predictions (maintain time order)\nlgb_pred_aligned = y_pred_best[-min_len:]\ngru_pred_aligned = val_pred_actual[-min_len:]\ny_val_aligned = y_val.values[-min_len:]\n\nprint(f\"\\nğŸ“Š ì˜ˆì¸¡ ì •ë ¬ / Prediction Alignment:\")\nprint(f\"   ì •ë ¬ëœ ìƒ˜í”Œ ìˆ˜ / Aligned samples: {min_len:,}\")\n\n# ============================================================\n# 2. ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ | Individual Model Performance\n# ============================================================\nlgb_rmse_final = np.sqrt(mean_squared_error(y_val_aligned, lgb_pred_aligned))\ngru_rmse_final = np.sqrt(mean_squared_error(y_val_aligned, gru_pred_aligned))\n\nprint(f\"\\n[Individual Model Performance | ê°œë³„ ëª¨ë¸ ì„±ëŠ¥]\")\nprint(f\"   ğŸ“Š LightGBM RMSE: {lgb_rmse_final:.6f}\")\nprint(f\"   ğŸ“Š GRU RMSE: {gru_rmse_final:.6f}\")\n\n# ============================================================\n# 3. ë‹¤ì–‘í•œ ì•™ìƒë¸” ë°©ë²• | Various Ensemble Methods\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Ensemble Methods | ì•™ìƒë¸” ë°©ë²•]\")\nprint(\"=\" * 70)\n\n# ë°©ë²• 1: ë‹¨ìˆœ í‰ê·  | Method 1: Simple Average\nensemble_simple = (lgb_pred_aligned + gru_pred_aligned) / 2\nrmse_simple = np.sqrt(mean_squared_error(y_val_aligned, ensemble_simple))\n\n# ë°©ë²• 2: ê°€ì¤‘ í‰ê·  (RMSE ê¸°ë°˜) | Method 2: Weighted Average (RMSE-based)\ninv_lgb_rmse = 1.0 / lgb_rmse_final\ninv_gru_rmse = 1.0 / gru_rmse_final\ntotal_inv = inv_lgb_rmse + inv_gru_rmse\nlgb_weight = inv_lgb_rmse / total_inv\ngru_weight = inv_gru_rmse / total_inv\nensemble_weighted = lgb_weight * lgb_pred_aligned + gru_weight * gru_pred_aligned\nrmse_weighted = np.sqrt(mean_squared_error(y_val_aligned, ensemble_weighted))\n\n# ğŸ”§ NEW: ë°©ë²• 3: ì¤‘ì•™ê°’ ì•™ìƒë¸” | Method 3: Median Ensemble\n# ì´ìƒì¹˜ì— ë” ê°•ê±´í•¨ | More robust to outliers\nensemble_median = np.median([lgb_pred_aligned, gru_pred_aligned], axis=0)\nrmse_median = np.sqrt(mean_squared_error(y_val_aligned, ensemble_median))\n\n# ğŸ”§ NEW: ë°©ë²• 4: ê¸°í•˜ í‰ê·  | Method 4: Geometric Mean\n# ì˜ˆì¸¡ê°’ì´ ëª¨ë‘ ì–‘ìˆ˜ì¼ ë•Œ ì‚¬ìš© ê°€ëŠ¥ | Usable when all predictions are positive\nif np.all(lgb_pred_aligned > 0) and np.all(gru_pred_aligned > 0):\n    ensemble_geometric = np.sqrt(lgb_pred_aligned * gru_pred_aligned)\n    rmse_geometric = np.sqrt(mean_squared_error(y_val_aligned, ensemble_geometric))\nelse:\n    ensemble_geometric = None\n    rmse_geometric = float('inf')\n    print(\"   âš ï¸ ê¸°í•˜ í‰ê· : ìŒìˆ˜ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì¸í•´ ê³„ì‚° ë¶ˆê°€\")\n\n# ğŸ”§ NEW: ë°©ë²• 5: ì ì‘í˜• ê°€ì¤‘ì¹˜ (MAE ê¸°ë°˜) | Method 5: Adaptive Weights (MAE-based)\nfrom sklearn.metrics import mean_absolute_error\nmae_lgb = mean_absolute_error(y_val_aligned, lgb_pred_aligned)\nmae_gru = mean_absolute_error(y_val_aligned, gru_pred_aligned)\ninv_mae_lgb = 1.0 / mae_lgb\ninv_mae_gru = 1.0 / mae_gru\ntotal_inv_mae = inv_mae_lgb + inv_mae_gru\nlgb_weight_mae = inv_mae_lgb / total_inv_mae\ngru_weight_mae = inv_mae_gru / total_inv_mae\nensemble_mae_weighted = lgb_weight_mae * lgb_pred_aligned + gru_weight_mae * gru_pred_aligned\nrmse_mae_weighted = np.sqrt(mean_squared_error(y_val_aligned, ensemble_mae_weighted))\n\nprint(\"\\nğŸ“Š ì•™ìƒë¸” ë°©ë²•ë³„ RMSE / Ensemble Method RMSE:\")\nprint(f\"   1. ë‹¨ìˆœ í‰ê·  / Simple Average: {rmse_simple:.6f}\")\nprint(f\"   2. ê°€ì¤‘ í‰ê·  (RMSE) / Weighted (RMSE): {rmse_weighted:.6f}\")\nprint(f\"      (LightGBM: {lgb_weight:.2%}, GRU: {gru_weight:.2%})\")\nprint(f\"   3. ì¤‘ì•™ê°’ / Median: {rmse_median:.6f}\")\nif rmse_geometric != float('inf'):\n    print(f\"   4. ê¸°í•˜ í‰ê·  / Geometric Mean: {rmse_geometric:.6f}\")\nprint(f\"   5. ê°€ì¤‘ í‰ê·  (MAE) / Weighted (MAE): {rmse_mae_weighted:.6f}\")\nprint(f\"      (LightGBM: {lgb_weight_mae:.2%}, GRU: {gru_weight_mae:.2%})\")\n\n# ============================================================\n# 4. ëª¨ë¸ ë‹¤ì–‘ì„± ë¶„ì„ | Model Diversity Analysis\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[Model Diversity Analysis | ëª¨ë¸ ë‹¤ì–‘ì„± ë¶„ì„]\")\nprint(\"=\" * 70)\n\n# ì˜ˆì¸¡ ìƒê´€ê´€ê³„ | Prediction correlation\npred_corr = np.corrcoef(lgb_pred_aligned, gru_pred_aligned)[0, 1]\nprint(f\"\\nğŸ“Š ì˜ˆì¸¡ ìƒê´€ê´€ê³„ / Prediction Correlation: {pred_corr:.4f}\")\n\nif pred_corr > 0.95:\n    print(\"   âš ï¸ ë†’ì€ ìƒê´€ê´€ê³„: ëª¨ë¸ë“¤ì´ ë§¤ìš° ìœ ì‚¬í•œ ì˜ˆì¸¡ì„ í•¨\")\n    print(\"   âš ï¸ High correlation: Models make very similar predictions\")\n    print(\"      ì•™ìƒë¸” íš¨ê³¼ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŒ / Ensemble effect may be limited\")\nelif pred_corr > 0.8:\n    print(\"   âœ“ ì ë‹¹í•œ ìƒê´€ê´€ê³„: ì•™ìƒë¸”ì— ì í•©\")\n    print(\"   âœ“ Moderate correlation: Suitable for ensemble\")\nelse:\n    print(\"   âœ“ ë‚®ì€ ìƒê´€ê´€ê³„: ì•™ìƒë¸” íš¨ê³¼ ê·¹ëŒ€í™” ê°€ëŠ¥\")\n    print(\"   âœ“ Low correlation: Ensemble effect maximized\")\n\n# ì˜ˆì¸¡ ì°¨ì´ ë¶„ì„ | Prediction difference analysis\npred_diff = lgb_pred_aligned - gru_pred_aligned\nprint(f\"\\nğŸ“Š ì˜ˆì¸¡ ì°¨ì´ í†µê³„ / Prediction Difference Statistics:\")\nprint(f\"   Mean: {pred_diff.mean():.4f}\")\nprint(f\"   Std: {pred_diff.std():.4f}\")\nprint(f\"   Max absolute diff: {np.abs(pred_diff).max():.4f}\")\n\n# ê° ëª¨ë¸ì´ ë” ì •í™•í•œ ë¹„ìœ¨ | Rate where each model is more accurate\nlgb_better = np.sum(np.abs(y_val_aligned - lgb_pred_aligned) < \n                    np.abs(y_val_aligned - gru_pred_aligned))\ngru_better = len(y_val_aligned) - lgb_better\n\nprint(f\"\\nğŸ“Š ì •í™•ë„ ë¹„êµ / Accuracy Comparison:\")\nprint(f\"   LightGBMì´ ë” ì •í™•í•œ ìƒ˜í”Œ: {lgb_better:,} ({lgb_better/len(y_val_aligned)*100:.1f}%)\")\nprint(f\"   GRUê°€ ë” ì •í™•í•œ ìƒ˜í”Œ: {gru_better:,} ({gru_better/len(y_val_aligned)*100:.1f}%)\")\n\n# ============================================================\n# 5. ìµœì  ì•™ìƒë¸” ì„ íƒ | Select Best Ensemble\n# ============================================================\nresults = {\n    'LightGBM': lgb_rmse_final,\n    'GRU': gru_rmse_final,\n    'Simple Avg': rmse_simple,\n    'Weighted (RMSE)': rmse_weighted,\n    'Median': rmse_median,\n    'Weighted (MAE)': rmse_mae_weighted\n}\nif rmse_geometric != float('inf'):\n    results['Geometric'] = rmse_geometric\n\nbest_method = min(results, key=results.get)\nbest_rmse = results[best_method]\n\nprint(\"\\n\" + \"=\" * 70)\nprint(f\"[Best Method | ìµœì  ë°©ë²•]: {best_method}\")\nprint(f\"[Best RMSE | ìµœì  RMSE]: {best_rmse:.6f}\")\nprint(\"=\" * 70)\n\nif best_rmse < 0.5:\n    print(\"âœ… ëª©í‘œ ë‹¬ì„±! / Target achieved! RMSE < 0.5\")\nelse:\n    print(f\"âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„± / Target not achieved: RMSE = {best_rmse:.6f} >= 0.5\")\n\n# ============================================================\n# 6. ê°œì„ ëœ ì‹œê°í™” | Improved Visualization\n# ============================================================\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. ëª¨ë¸ë³„ RMSE ë¹„êµ | RMSE comparison by model\ncolors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6', '#f39c12', '#1abc9c', '#34495e']\nmodels = list(results.keys())\nrmses = list(results.values())\nbars = axes[0, 0].bar(models, rmses, color=colors[:len(models)], edgecolor='black', alpha=0.8)\naxes[0, 0].axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Target (0.5)')\naxes[0, 0].set_ylabel('RMSE')\naxes[0, 0].set_title('Model & Ensemble RMSE Comparison | ëª¨ë¸ ë° ì•™ìƒë¸” RMSE ë¹„êµ')\naxes[0, 0].legend()\naxes[0, 0].tick_params(axis='x', rotation=30)\naxes[0, 0].grid(True, alpha=0.3, axis='y')\n\n# ìµœì  ë°©ë²• ê°•ì¡° | Highlight best method\nfor bar, val, name in zip(bars, rmses, models):\n    color = 'green' if name == best_method else 'black'\n    weight = 'bold' if name == best_method else 'normal'\n    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n                    f'{val:.4f}', ha='center', va='bottom', fontsize=9, \n                    color=color, fontweight=weight)\n\n# 2. ì˜ˆì¸¡ ë¶„í¬ ë¹„êµ | Prediction distribution comparison\naxes[0, 1].hist(lgb_pred_aligned, bins=50, alpha=0.5, label='LightGBM', color='blue')\naxes[0, 1].hist(gru_pred_aligned, bins=50, alpha=0.5, label='GRU', color='red')\naxes[0, 1].hist(y_val_aligned, bins=50, alpha=0.5, label='Actual', color='green')\naxes[0, 1].set_xlabel('OT Value')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].set_title('Prediction Distribution Comparison | ì˜ˆì¸¡ ë¶„í¬ ë¹„êµ')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. ì‹œê³„ì—´ ë¹„êµ (ë§ˆì§€ë§‰ 500 ìƒ˜í”Œ) | Time series comparison (last 500 samples)\nsample_size = min(500, len(y_val_aligned))\naxes[1, 0].plot(y_val_aligned[-sample_size:], label='Actual', alpha=0.8, linewidth=1)\naxes[1, 0].plot(lgb_pred_aligned[-sample_size:], label='LightGBM', alpha=0.6, linewidth=1)\naxes[1, 0].plot(gru_pred_aligned[-sample_size:], label='GRU', alpha=0.6, linewidth=1)\naxes[1, 0].plot(ensemble_weighted[-sample_size:], label='Weighted Ensemble', \n                alpha=0.8, linewidth=1.5, linestyle='--')\naxes[1, 0].set_xlabel('Time Step')\naxes[1, 0].set_ylabel('OT')\naxes[1, 0].set_title(f'Predictions vs Actual (Last {sample_size}) | ì˜ˆì¸¡ vs ì‹¤ì œ')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. ì”ì°¨ ë¹„êµ ë°•ìŠ¤í”Œë¡¯ | Residual comparison boxplot\nlgb_residuals = y_val_aligned - lgb_pred_aligned\ngru_residuals = y_val_aligned - gru_pred_aligned\nensemble_residuals = y_val_aligned - ensemble_weighted\n\nbp = axes[1, 1].boxplot([lgb_residuals, gru_residuals, ensemble_residuals],\n                        labels=['LightGBM', 'GRU', 'Weighted\\nEnsemble'], \n                        patch_artist=True)\nbp['boxes'][0].set_facecolor('lightblue')\nbp['boxes'][1].set_facecolor('lightcoral')\nbp['boxes'][2].set_facecolor('lightgreen')\naxes[1, 1].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[1, 1].set_ylabel('Residual (Actual - Predicted)')\naxes[1, 1].set_title('Residual Distribution by Model | ëª¨ë¸ë³„ ì”ì°¨ ë¶„í¬')\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================\n# 7. ìµœì¢… ìš”ì•½ | Final Summary\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ğŸ“‹ Final Summary | ìµœì¢… ìš”ì•½\")\nprint(\"=\" * 70)\n\nprint(f\"\\n[Model Results | ëª¨ë¸ ê²°ê³¼]\")\nprint(f\"  Q5  - LightGBM Baseline RMSE: {rmse_baseline:.6f}\")\nprint(f\"  Q6  - Optuna Tuned LightGBM RMSE: {rmse_best:.6f}\")\nprint(f\"  Q8  - Production GRU Model RMSE: {gru_rmse:.6f}\")\nprint(f\"  Q10 - Simple Ensemble RMSE: {rmse_simple:.6f}\")\nprint(f\"  Q10 - Weighted Ensemble (RMSE) RMSE: {rmse_weighted:.6f}\")\nprint(f\"  Q10 - Median Ensemble RMSE: {rmse_median:.6f}\")\nprint(f\"  Q10 - Weighted Ensemble (MAE) RMSE: {rmse_mae_weighted:.6f}\")\n\nprint(f\"\\n[Best Result | ìµœì  ê²°ê³¼]\")\nprint(f\"  ğŸ† Method: {best_method}\")\nprint(f\"  ğŸ† RMSE: {best_rmse:.6f}\")\n\nprint(f\"\\n[Ensemble Weights (RMSE-based) | ì•™ìƒë¸” ê°€ì¤‘ì¹˜]\")\nprint(f\"  LightGBM: {lgb_weight:.4f} ({lgb_weight * 100:.1f}%)\")\nprint(f\"  GRU: {gru_weight:.4f} ({gru_weight * 100:.1f}%)\")\n\nprint(f\"\\n[Model Diversity | ëª¨ë¸ ë‹¤ì–‘ì„±]\")\nprint(f\"  Prediction Correlation: {pred_corr:.4f}\")\nprint(f\"  LightGBM better rate: {lgb_better/len(y_val_aligned)*100:.1f}%\")\nprint(f\"  GRU better rate: {gru_better/len(y_val_aligned)*100:.1f}%\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"âœ… ë¶„ì„ ì™„ë£Œ / Analysis Completed\")\nprint(\"=\" * 70)\n\n# ============================================================\n# 8. ì œì¶œ íŒŒì¼ ìƒì„± (ì„ íƒì ) | Generate Submission File (Optional)\n# ============================================================\n# submission íŒŒì¼ ìƒì„±ì´ í•„ìš”í•œ ê²½ìš° ì•„ë˜ ì½”ë“œ ì‚¬ìš©\n# Use the code below if submission file generation is needed\n\n# submission = pd.read_csv(DATA_PATH + 'submission.csv')\n# submission['OT'] = test_predictions  # ë°˜ë³µ ì˜ˆì¸¡ ê²°ê³¼ ì‚¬ìš©\n# submission.to_csv('submission_final.csv', index=False)\n# print(\"\\nğŸ“ Submission file saved: submission_final.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBLnx0tl6Tdz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}