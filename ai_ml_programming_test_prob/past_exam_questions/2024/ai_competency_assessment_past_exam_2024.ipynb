{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7Jh-RVfNW3E"
   },
   "source": [
    "## << 문제 정의 >>\n",
    "\n",
    "주어진 ETDataset을 사용하여 전력 변압기의 **오일 온도(OT)**를 예측하는 문제를 풉니다.\n",
    "\n",
    "주어진 데이터는 총 3개의 CSV 파일입니다. 각 CSV 파일에 대한 설명은 아래에 기술되어 있습니다.\n",
    "\n",
    "각 시간대별로 예측한 OT와 실제 OT 사이의 RMSE(Root Mean Squared Error) 값을 성능 지표로 사용합니다.\n",
    "\n",
    "해당 문제는 머신러닝 및 딥러닝 예측 모델을 만드는 과정을 코드로 구현하는 것을 평가합니다.\n",
    "\n",
    "1. train.csv\n",
    "\n",
    "학습에 사용되는 데이터로 2016년 7월 1일 0시부터 2017년 12월 31일 23시 45분까지의 변압기 데이터가 기록되어 있습니다.\n",
    "\n",
    "date, HUFL, HULL, MUFL, MULL, LUFL, LULL, OT로 열이 구성되어 있습니다.\n",
    "\n",
    "2. test.csv\n",
    "\n",
    "예측에 사용되는 데이터로 2018년 1월 1일 0시부터 2018년 6월 30일 23시 45분까지의 변압기 데이터가 기록되어 있습니다.\n",
    "\n",
    "OT 열은 제외되어 있으며, 나머지 열은 train.csv와 동일합니다.\n",
    "\n",
    "3. submission.csv\n",
    "\n",
    "실제 예측값을 기록하는 파일입니다. OT 열에 예측한 OT 값을 기록하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wwW5j4rM9DgL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LapI63BSMbQI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 사용할 라이브러리 불러오기 | Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# 디바이스 설정 | Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 재현성을 위한 시드 설정 | Set seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# 데이터는 15분 간격으로 기록됨 | Data is recorded at 15-minute intervals\n",
    "INTERVALS_PER_HOUR = 4\n",
    "\n",
    "# 기본 데이터 경로 설정 (로컬 환경용) | Default data path (for local environment)\n",
    "# Google Colab 사용 시 다음 셀에서 경로가 재설정됩니다\n",
    "# When using Google Colab, the path will be reset in the next cell\n",
    "DATA_PATH = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "g9EkKrD-6_NK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Data path: dataset/\n"
     ]
    }
   ],
   "source": [
    "# Google Colab 환경 설정 | Google Colab environment setup\n",
    "# 로컬 환경에서는 이 셀을 건너뛰세요 | Skip this cell in local environment\n",
    "\n",
    "# Colab 환경 확인 | Check if running in Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Colab에서 데이터 경로 설정 | Set data path for Colab\n",
    "    # 아래 경로를 자신의 Google Drive 경로에 맞게 수정하세요\n",
    "    # Modify the path below to match your Google Drive path\n",
    "    DATA_PATH = '/content/drive/MyDrive/your_path_here/dataset/'\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    # 로컬 환경 | Local environment\n",
    "    DATA_PATH = 'dataset/'\n",
    "    IN_COLAB = False\n",
    "    print(\"Running in local environment\")\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v_ujn4hNW3H"
   },
   "source": [
    "### Q1. train.csv와 test.csv를 불러오고, 각 데이터의 shape을 출력하세요. 또한, 결측치가 있는지 확인하고 각 열별로 결측치의 개수를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AHGYGA14Ol74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[Data Shape | 데이터 Shape]\n",
      "==================================================\n",
      "Train shape: (52704, 8)\n",
      "Test shape: (16976, 7)\n",
      "\n",
      "==================================================\n",
      "[Train Missing Values | Train 결측치]\n",
      "==================================================\n",
      "date    0\n",
      "HUFL    0\n",
      "HULL    0\n",
      "MUFL    0\n",
      "MULL    0\n",
      "LUFL    0\n",
      "LULL    0\n",
      "OT      0\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "[Test Missing Values | Test 결측치]\n",
      "==================================================\n",
      "date    0\n",
      "HUFL    0\n",
      "HULL    0\n",
      "MUFL    0\n",
      "MULL    0\n",
      "LUFL    0\n",
      "LULL    0\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "[Summary | 요약]\n",
      "==================================================\n",
      "Total missing in train: 0\n",
      "Total missing in test: 0\n"
     ]
    }
   ],
   "source": [
    "# A1. 데이터 로드 및 결측치 확인 | Load data and check missing values\n",
    "\n",
    "# CSV 파일 로드 | Load CSV files\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "\n",
    "# 데이터 shape 출력 | Print data shapes\n",
    "print(\"=\" * 50)\n",
    "print(\"[Data Shape | 데이터 Shape]\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# 결측치 확인 | Check missing values\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"[Train Missing Values | Train 결측치]\")\n",
    "print(\"=\" * 50)\n",
    "print(train.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"[Test Missing Values | Test 결측치]\")\n",
    "print(\"=\" * 50)\n",
    "print(test.isnull().sum())\n",
    "\n",
    "# 결측치 요약 | Missing values summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"[Summary | 요약]\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total missing in train: {train.isnull().sum().sum()}\")\n",
    "print(f\"Total missing in test: {test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooFdqqcFNW3I"
   },
   "source": [
    "### Q2. 'date' 열을 사용하여 'hour', 'dayofweek', 'month' 특성을 생성하고, 'hour'와 'dayofweek'에 대해 sin과 cos 변환을 적용하여 cyclic feature를 만드세요. 주기를 결정하는건 자유롭게 정하셔도 됩니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ic0jxhDuOvQr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cyclic Features Created | 생성된 순환 특성]\n",
      "- hour, hour_sin, hour_cos (24시간 주기)\n",
      "- dayofweek, dayofweek_sin, dayofweek_cos (7일 주기)\n",
      "- month, month_sin, month_cos (12개월 주기)\n",
      "\n",
      "Train shape after feature engineering: (52704, 17)\n",
      "Test shape after feature engineering: (16976, 16)\n"
     ]
    }
   ],
   "source": [
    "# A2. 순환 특성 생성 | Create cyclic features\n",
    "\n",
    "# date 열을 datetime으로 변환 | Convert date column to datetime\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "\n",
    "# 시간 특성 추출 | Extract time features\n",
    "train['hour'] = train['date'].dt.hour\n",
    "train['dayofweek'] = train['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "train['month'] = train['date'].dt.month\n",
    "\n",
    "test['hour'] = test['date'].dt.hour\n",
    "test['dayofweek'] = test['date'].dt.dayofweek\n",
    "test['month'] = test['date'].dt.month\n",
    "\n",
    "# 순환 변환 적용 (sin/cos) | Apply cyclic transformation (sin/cos)\n",
    "# hour: 24시간 주기 | 24-hour cycle\n",
    "train['hour_sin'] = np.sin(2 * np.pi * train['hour'] / 24)\n",
    "train['hour_cos'] = np.cos(2 * np.pi * train['hour'] / 24)\n",
    "test['hour_sin'] = np.sin(2 * np.pi * test['hour'] / 24)\n",
    "test['hour_cos'] = np.cos(2 * np.pi * test['hour'] / 24)\n",
    "\n",
    "# dayofweek: 7일 주기 | 7-day cycle\n",
    "train['dayofweek_sin'] = np.sin(2 * np.pi * train['dayofweek'] / 7)\n",
    "train['dayofweek_cos'] = np.cos(2 * np.pi * train['dayofweek'] / 7)\n",
    "test['dayofweek_sin'] = np.sin(2 * np.pi * test['dayofweek'] / 7)\n",
    "test['dayofweek_cos'] = np.cos(2 * np.pi * test['dayofweek'] / 7)\n",
    "\n",
    "# month: 12개월 주기 | 12-month cycle\n",
    "train['month_sin'] = np.sin(2 * np.pi * train['month'] / 12)\n",
    "train['month_cos'] = np.cos(2 * np.pi * train['month'] / 12)\n",
    "test['month_sin'] = np.sin(2 * np.pi * test['month'] / 12)\n",
    "test['month_cos'] = np.cos(2 * np.pi * test['month'] / 12)\n",
    "\n",
    "print(\"[Cyclic Features Created | 생성된 순환 특성]\")\n",
    "print(\"- hour, hour_sin, hour_cos (24시간 주기)\")\n",
    "print(\"- dayofweek, dayofweek_sin, dayofweek_cos (7일 주기)\")\n",
    "print(\"- month, month_sin, month_cos (12개월 주기)\")\n",
    "print(f\"\\nTrain shape after feature engineering: {train.shape}\")\n",
    "print(f\"Test shape after feature engineering: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLL2e9JlNW3I"
   },
   "source": [
    "### Q3. 'OT' 열에 대해 1시간 전, 2시간 전, 3시간 전의 값을 나타내는 lag 특성을 생성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "soHRdYpsOwRS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lag Features Created | 생성된 지연 특성]\n",
      "- OT_lag_1h: OT value from 1 hour ago (shift=4)\n",
      "- OT_lag_2h: OT value from 2 hours ago (shift=8)\n",
      "- OT_lag_3h: OT value from 3 hours ago (shift=12)\n",
      "\n",
      "NaN rows created by lag: 12\n",
      "Train shape: (52704, 20)\n"
     ]
    }
   ],
   "source": [
    "# A3. 지연 특성 생성 | Create lag features\n",
    "\n",
    "# 데이터가 15분 간격이므로 1시간 = 4 스텝\n",
    "# Data is at 15-minute intervals, so 1 hour = 4 steps\n",
    "# 1시간 전 = shift(4), 2시간 전 = shift(8), 3시간 전 = shift(12)\n",
    "\n",
    "train['OT_lag_1h'] = train['OT'].shift(INTERVALS_PER_HOUR * 1)  # 1시간 전 | 1 hour ago\n",
    "train['OT_lag_2h'] = train['OT'].shift(INTERVALS_PER_HOUR * 2)  # 2시간 전 | 2 hours ago\n",
    "train['OT_lag_3h'] = train['OT'].shift(INTERVALS_PER_HOUR * 3)  # 3시간 전 | 3 hours ago\n",
    "\n",
    "print(\"[Lag Features Created | 생성된 지연 특성]\")\n",
    "print(f\"- OT_lag_1h: OT value from 1 hour ago (shift={INTERVALS_PER_HOUR})\")\n",
    "print(f\"- OT_lag_2h: OT value from 2 hours ago (shift={INTERVALS_PER_HOUR * 2})\")\n",
    "print(f\"- OT_lag_3h: OT value from 3 hours ago (shift={INTERVALS_PER_HOUR * 3})\")\n",
    "print(f\"\\nNaN rows created by lag: {train['OT_lag_3h'].isna().sum()}\")\n",
    "print(f\"Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpChME79NW3I"
   },
   "source": [
    "### Q4. 불필요한 열인 'date'를 제거하고, 특성 행렬 X와 목표 변수 y를 생성하여 데이터를 시간 순서에 따라 3:1 비율로 훈련 세트와 검증 세트로 분할하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R6FQ8Al0OzlX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 12 rows with NaN values | NaN 행 12개 제거\n",
      "\n",
      "[Data Split | 데이터 분할]\n",
      "Training set size: 39519 (75%)\n",
      "Validation set size: 13173 (25%)\n",
      "\n",
      "Feature columns (18): ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'hour', 'dayofweek', 'month', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos', 'OT_lag_1h', 'OT_lag_2h', 'OT_lag_3h']\n"
     ]
    }
   ],
   "source": [
    "# A4. 데이터 준비 및 분할 | Prepare and split data\n",
    "\n",
    "# date 열 제거 | Remove date column\n",
    "train_processed = train.drop(columns=['date'])\n",
    "\n",
    "# NaN 값이 있는 행 제거 (lag 특성으로 인해 발생)\n",
    "# Drop rows with NaN values (caused by lag features)\n",
    "initial_len = len(train_processed)\n",
    "train_processed = train_processed.dropna()\n",
    "dropped_rows = initial_len - len(train_processed)\n",
    "print(f\"Dropped {dropped_rows} rows with NaN values | NaN 행 {dropped_rows}개 제거\")\n",
    "\n",
    "# 특성 행렬 X와 목표 변수 y 생성 | Create feature matrix X and target variable y\n",
    "y = train_processed['OT']\n",
    "X = train_processed.drop(columns=['OT'])\n",
    "\n",
    "# 시간 순서에 따라 3:1 비율로 분할 (시계열이므로 shuffle=False)\n",
    "# Split by time order in 3:1 ratio (no shuffle for time series)\n",
    "train_ratio = 0.75\n",
    "split_idx = int(len(X) * train_ratio)\n",
    "\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_val = X.iloc[split_idx:]\n",
    "y_train = y.iloc[:split_idx]\n",
    "y_val = y.iloc[split_idx:]\n",
    "\n",
    "print(\"\\n[Data Split | 데이터 분할]\")\n",
    "print(f\"Training set size: {len(X_train)} ({train_ratio * 100:.0f}%)\")\n",
    "print(f\"Validation set size: {len(X_val)} ({(1 - train_ratio) * 100:.0f}%)\")\n",
    "print(f\"\\nFeature columns ({len(X.columns)}): {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt5V8t_6NW3J"
   },
   "source": [
    "### Q5. LightGBM을 사용하여 모델을 학습한 후, 검증 세트에 대한 RMSE를 계산하세요. 하이퍼파라미터는 num_leaves=31, n_estimators=100, learning_rate=0.05로 설정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "R1-aHy97O0w3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM Baseline Results | LightGBM 기준 결과]\n",
      "Hyperparameters: num_leaves=31, n_estimators=100, learning_rate=0.05\n",
      "Validation RMSE: 0.773486\n"
     ]
    }
   ],
   "source": [
    "# A5. LightGBM 기준 모델 학습 | Train LightGBM baseline model\n",
    "\n",
    "# 지정된 하이퍼파라미터로 모델 생성 | Create model with specified hyperparameters\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    num_leaves=31,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "# 모델 학습 | Train model\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 예측 및 RMSE 계산 | Predict and calculate RMSE\n",
    "y_pred = lgb_model.predict(X_val)\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "print(\"[LightGBM Baseline Results | LightGBM 기준 결과]\")\n",
    "print(f\"Hyperparameters: num_leaves=31, n_estimators=100, learning_rate=0.05\")\n",
    "print(f\"Validation RMSE: {rmse_baseline:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7va-XmINW3J"
   },
   "source": [
    "### Q6. optuna를 사용하여 LightGBM의 하이퍼파라미터를 튜닝하고, 최적의 모델을 이용하여 검증 세트에 대한 RMSE를 계산하세요. RMSE를 0.5 이하로 낮추는 것을 목표로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L0YMF8HGO11E"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c976b34814cf4d4baa99c207a45cc023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Trial:\n",
      "  RMSE: 0.749162\n",
      "  Best Params: \n",
      "    num_leaves: 51\n",
      "    n_estimators: 474\n",
      "    learning_rate: 0.02314711631954098\n",
      "    max_depth: 3\n",
      "    min_child_samples: 40\n",
      "    subsample: 0.6589955926184673\n",
      "    colsample_bytree: 0.9017404647109675\n",
      "    reg_alpha: 0.4253570454627979\n",
      "    reg_lambda: 0.0001876514633601301\n",
      "\n",
      "Optuna Tuned LightGBM Validation RMSE: 0.749162\n",
      "✗ Target not achieved: RMSE >= 0.5 | 목표 미달성: RMSE >= 0.5\n"
     ]
    }
   ],
   "source": [
    "# A6. Optuna 하이퍼파라미터 튜닝 | Optuna hyperparameter tuning\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna 목적 함수 | Optuna objective function\n",
    "    LightGBM 하이퍼파라미터 최적화 | LightGBM hyperparameter optimization\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Optuna 스터디 생성 및 최적화 | Create study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBest Trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  RMSE: {trial.value:.6f}\")\n",
    "print(\"  Best Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델 재학습 | Retrain model with best hyperparameters\n",
    "best_params = trial.params\n",
    "best_params['random_state'] = RANDOM_SEED\n",
    "best_params['verbosity'] = -1\n",
    "\n",
    "best_model = lgb.LGBMRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_best = best_model.predict(X_val)\n",
    "rmse_best = np.sqrt(mean_squared_error(y_val, y_pred_best))\n",
    "print(f\"\\nOptuna Tuned LightGBM Validation RMSE: {rmse_best:.6f}\")\n",
    "\n",
    "if rmse_best < 0.5:\n",
    "    print(\"✓ Target achieved: RMSE < 0.5 | 목표 달성: RMSE < 0.5\")\n",
    "else:\n",
    "    print(\"✗ Target not achieved: RMSE >= 0.5 | 목표 미달성: RMSE >= 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W556TPbZNW3J"
   },
   "source": [
    "### Q7. PyTorch를 사용하여 GRU 기반의 시계열 예측 모델을 구축하기 위해, OT 열을 정규화(Min-Max Scaling)하고 시계열 형태로 변환하세요. 입력 시퀀스의 길이는 24시간으로 설정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-YN-JflwO3Vp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRU Data Preparation | GRU 데이터 준비]\n",
      "Sequence length: 96 (24 hours = 96 time steps)\n",
      "Feature dimension: 16\n",
      "Training sequences: 39456\n",
      "Validation sequences: 13152\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# A7. GRU 데이터 준비 | Prepare data for GRU\n",
    "\n",
    "# 시퀀스 길이: 24시간 = 96 타임 스텝 (15분 간격)\n",
    "# Sequence length: 24 hours = 96 time steps (15-minute intervals)\n",
    "SEQUENCE_LENGTH = 24 * INTERVALS_PER_HOUR  # 96\n",
    "\n",
    "# 원본 train 데이터를 다시 로드하여 GRU용으로 사용\n",
    "# Reload original train data for GRU (to avoid conflicts with modified data)\n",
    "train_gru = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "train_gru['date'] = pd.to_datetime(train_gru['date'])\n",
    "\n",
    "# 시간 특성 추출 | Extract time features\n",
    "train_gru['hour'] = train_gru['date'].dt.hour\n",
    "train_gru['dayofweek'] = train_gru['date'].dt.dayofweek\n",
    "train_gru['month'] = train_gru['date'].dt.month\n",
    "\n",
    "# 순환 변환 | Cyclic transformation\n",
    "train_gru['hour_sin'] = np.sin(2 * np.pi * train_gru['hour'] / 24)\n",
    "train_gru['hour_cos'] = np.cos(2 * np.pi * train_gru['hour'] / 24)\n",
    "train_gru['dayofweek_sin'] = np.sin(2 * np.pi * train_gru['dayofweek'] / 7)\n",
    "train_gru['dayofweek_cos'] = np.cos(2 * np.pi * train_gru['dayofweek'] / 7)\n",
    "train_gru['month_sin'] = np.sin(2 * np.pi * train_gru['month'] / 12)\n",
    "train_gru['month_cos'] = np.cos(2 * np.pi * train_gru['month'] / 12)\n",
    "\n",
    "# 특성 열 선택 (date 제외, OT는 마지막에)\n",
    "# Select feature columns (exclude date, OT at the end)\n",
    "feature_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL',\n",
    "                'hour', 'dayofweek', 'month',\n",
    "                'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos',\n",
    "                'month_sin', 'month_cos', 'OT']\n",
    "\n",
    "data = train_gru[feature_cols].values\n",
    "\n",
    "# MinMaxScaler로 정규화 | Normalize with MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# 시퀀스 생성 | Create sequences\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(SEQUENCE_LENGTH, len(data_scaled)):\n",
    "    X_sequences.append(data_scaled[i - SEQUENCE_LENGTH:i])  # 입력: 24시간 시퀀스\n",
    "    y_sequences.append(data_scaled[i, -1])  # 출력: 현재 OT 값 (마지막 열)\n",
    "\n",
    "X_array = np.array(X_sequences)\n",
    "y_array = np.array(y_sequences)\n",
    "\n",
    "# 시간 순서에 따라 분할 | Split by time order\n",
    "split_idx_gru = int(len(X_array) * 0.75)\n",
    "\n",
    "X_train_gru = X_array[:split_idx_gru]\n",
    "X_val_gru = X_array[split_idx_gru:]\n",
    "y_train_gru = y_array[:split_idx_gru]\n",
    "y_val_gru = y_array[split_idx_gru:]\n",
    "\n",
    "# PyTorch 텐서로 변환 | Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_gru)\n",
    "y_train_tensor = torch.FloatTensor(y_train_gru).unsqueeze(1)\n",
    "X_val_tensor = torch.FloatTensor(X_val_gru)\n",
    "y_val_tensor = torch.FloatTensor(y_val_gru).unsqueeze(1)\n",
    "\n",
    "# DataLoader 생성 | Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "feature_dim = X_array.shape[2]\n",
    "\n",
    "print(\"[GRU Data Preparation | GRU 데이터 준비]\")\n",
    "print(f\"Sequence length: {SEQUENCE_LENGTH} (24 hours = {SEQUENCE_LENGTH} time steps)\")\n",
    "print(f\"Feature dimension: {feature_dim}\")\n",
    "print(f\"Training sequences: {len(X_train_gru)}\")\n",
    "print(f\"Validation sequences: {len(X_val_gru)}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqmdrvzgNW3K"
   },
   "source": [
    "### Q8. GRU 모델을 정의하고 학습한 후, 검증 세트에 대한 RMSE를 계산하세요. 에포크는 20번으로 설정하고, 손실 함수는 MSELoss, 옵티마이저는 Adam을 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WGZrMEk-O4j4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "GRU(\n",
      "  (gru): GRU(16, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3d9be62152427ba042434ea6e203e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training GRU:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Train Loss: 0.002164\n",
      "Epoch [10/20], Train Loss: 0.000789\n",
      "Epoch [15/20], Train Loss: 0.000661\n",
      "Epoch [20/20], Train Loss: 0.000556\n",
      "\n",
      "[GRU Model Results | GRU 모델 결과]\n",
      "Epochs: 20\n",
      "Loss function: MSELoss\n",
      "Optimizer: Adam (lr=0.001)\n",
      "Validation RMSE: 1.467100\n"
     ]
    }
   ],
   "source": [
    "# A8. GRU 모델 정의 및 학습 | Define and train GRU model\n",
    "\n",
    "# GRU 모델 정의 | GRU model definition\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):\n",
    "        \"\"\"\n",
    "        GRU 기반 시계열 예측 모델 | GRU-based time series prediction model\n",
    "        \n",
    "        Args:\n",
    "            input_size: 입력 특성 수 | Number of input features\n",
    "            hidden_size: 히든 유닛 수 | Number of hidden units\n",
    "            num_layers: GRU 레이어 수 | Number of GRU layers\n",
    "            output_size: 출력 크기 | Output size\n",
    "            dropout: 드롭아웃 비율 | Dropout rate\n",
    "        \"\"\"\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # GRU 순전파 | GRU forward pass\n",
    "        out, _ = self.gru(x)\n",
    "        # 마지막 타임 스텝 출력 사용 | Use last time step output\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# 모델 초기화 | Initialize model\n",
    "model = GRU(input_size=feature_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model architecture:\\n{model}\")\n",
    "print(f\"\\nDevice: {device}\")\n",
    "\n",
    "# 모델 학습 | Train model\n",
    "epochs = 20\n",
    "for epoch in tqdm(range(epochs), desc=\"Training GRU\"):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # 순전파 | Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # 역전파 | Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "# 모델 검증 | Validate model\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        val_predictions.append(outputs.cpu())\n",
    "        val_targets.append(y_batch)\n",
    "\n",
    "# 예측 결합 | Concatenate predictions\n",
    "val_pred_tensor = torch.cat(val_predictions).numpy()\n",
    "val_target_tensor = torch.cat(val_targets).numpy()\n",
    "\n",
    "# 역변환하여 실제 값 얻기 | Inverse transform to get actual values\n",
    "n_features = scaler.n_features_in_\n",
    "dummy_pred = np.zeros((len(val_pred_tensor), n_features))\n",
    "dummy_target = np.zeros((len(val_target_tensor), n_features))\n",
    "dummy_pred[:, -1] = val_pred_tensor.flatten()\n",
    "dummy_target[:, -1] = val_target_tensor.flatten()\n",
    "\n",
    "val_pred_actual = scaler.inverse_transform(dummy_pred)[:, -1]\n",
    "val_target_actual = scaler.inverse_transform(dummy_target)[:, -1]\n",
    "\n",
    "# RMSE 계산 | Calculate RMSE\n",
    "gru_rmse = np.sqrt(mean_squared_error(val_target_actual, val_pred_actual))\n",
    "\n",
    "print(f\"\\n[GRU Model Results | GRU 모델 결과]\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Loss function: MSELoss\")\n",
    "print(f\"Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"Validation RMSE: {gru_rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzebHrqS6Tdz"
   },
   "source": [
    "### Q9. 전처리가 완료된 test 데이터를 생성하세요. train 데이터에서 사용했던 전처리를 동일하게 적용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "S40s8fXM6Tdz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Data Preprocessing | 테스트 데이터 전처리]\n",
      "Test data shape after preprocessing: (16976, 18)\n",
      "Columns: ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'hour', 'dayofweek', 'month', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos', 'OT_lag_1h', 'OT_lag_2h', 'OT_lag_3h']\n",
      "\n",
      "Final test shape (aligned with training): (16976, 18)\n"
     ]
    }
   ],
   "source": [
    "# A9. 테스트 데이터 전처리 | Preprocess test data\n",
    "\n",
    "# 테스트 데이터에 동일한 전처리 적용 | Apply same preprocessing to test data\n",
    "test_processed = test.copy()\n",
    "\n",
    "# 이미 Q2에서 순환 특성이 생성되어 있음 | Cyclic features already created in Q2\n",
    "# 지연 특성 처리 - 테스트 데이터에는 OT가 없으므로 train의 마지막 값 사용\n",
    "# Lag features - use train's last OT values since test doesn't have OT\n",
    "\n",
    "# train 데이터의 최근 OT 평균을 플레이스홀더로 사용\n",
    "# Use mean of recent OT values from train as placeholder\n",
    "# Note: train 변수는 Q1에서 로드됨 | train variable is loaded in Q1\n",
    "mean_recent_ot = train['OT'].tail(INTERVALS_PER_HOUR * 24).mean()\n",
    "\n",
    "test_processed['OT_lag_1h'] = mean_recent_ot\n",
    "test_processed['OT_lag_2h'] = mean_recent_ot\n",
    "test_processed['OT_lag_3h'] = mean_recent_ot\n",
    "\n",
    "# date 열 제거 | Remove date column\n",
    "test_processed = test_processed.drop(columns=['date'])\n",
    "\n",
    "print(\"[Test Data Preprocessing | 테스트 데이터 전처리]\")\n",
    "print(f\"Test data shape after preprocessing: {test_processed.shape}\")\n",
    "print(f\"Columns: {list(test_processed.columns)}\")\n",
    "\n",
    "# X_train과 동일한 열 순서로 정렬 | Align column order with X_train\n",
    "test_final = test_processed[X_train.columns]\n",
    "print(f\"\\nFinal test shape (aligned with training): {test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwY0Tl7XNW3K"
   },
   "source": [
    "### Q10. 마지막으로, LightGBM 모델과 GRU 모델의 예측값을 앙상블하여 검증 세트에 대한 RMSE를 계산하세요. 앙상블 방법으로 두 모델의 예측값의 평균을 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vVSNH1XmO53B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ensemble Results | 앙상블 결과]\n",
      "LightGBM RMSE: 0.749594\n",
      "GRU RMSE: 1.467100\n",
      "Ensemble RMSE (Average): 0.768240\n",
      "\n",
      "Note: Ensemble did not outperform all individual models\n",
      "참고: 앙상블이 모든 개별 모델보다 우수하지 않음\n",
      "\n",
      "============================================================\n",
      "Final Summary | 최종 요약\n",
      "============================================================\n",
      "Q5 - LightGBM Baseline RMSE: 0.773486\n",
      "Q6 - Optuna Tuned LightGBM RMSE: 0.749162\n",
      "Q8 - GRU Model RMSE: 1.467100\n",
      "Q10 - Ensemble RMSE: 0.768240\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# A10. 앙상블 예측 | Ensemble predictions\n",
    "\n",
    "# LightGBM 예측 | LightGBM predictions\n",
    "lgb_pred = best_model.predict(X_val)\n",
    "\n",
    "# GRU 예측 (이미 Q8에서 계산됨) | GRU predictions (already computed in Q8)\n",
    "# val_pred_actual 변수에 저장되어 있음\n",
    "\n",
    "# 예측 정렬 (GRU는 시퀀스 생성으로 인해 예측이 더 적음)\n",
    "# Align predictions (GRU has fewer predictions due to sequence creation)\n",
    "min_len = min(len(lgb_pred), len(val_pred_actual))\n",
    "\n",
    "# 마지막 min_len 예측 사용 | Use last min_len predictions\n",
    "lgb_pred_aligned = lgb_pred[-min_len:]\n",
    "gru_pred_aligned = val_pred_actual[-min_len:]\n",
    "y_val_aligned = y_val.values[-min_len:]\n",
    "\n",
    "# 앙상블: 두 모델의 평균 | Ensemble: Average of two models\n",
    "ensemble_pred = (lgb_pred_aligned + gru_pred_aligned) / 2\n",
    "\n",
    "# 개별 및 앙상블 RMSE 계산 | Calculate individual and ensemble RMSE\n",
    "lgb_rmse_final = np.sqrt(mean_squared_error(y_val_aligned, lgb_pred_aligned))\n",
    "gru_rmse_final = np.sqrt(mean_squared_error(y_val_aligned, gru_pred_aligned))\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_val_aligned, ensemble_pred))\n",
    "\n",
    "print(\"[Ensemble Results | 앙상블 결과]\")\n",
    "print(f\"LightGBM RMSE: {lgb_rmse_final:.6f}\")\n",
    "print(f\"GRU RMSE: {gru_rmse_final:.6f}\")\n",
    "print(f\"Ensemble RMSE (Average): {ensemble_rmse:.6f}\")\n",
    "\n",
    "if ensemble_rmse < lgb_rmse_final and ensemble_rmse < gru_rmse_final:\n",
    "    print(\"\\n✓ Ensemble outperforms individual models | 앙상블이 개별 모델보다 우수함\")\n",
    "else:\n",
    "    print(\"\\nNote: Ensemble did not outperform all individual models\")\n",
    "    print(\"참고: 앙상블이 모든 개별 모델보다 우수하지 않음\")\n",
    "\n",
    "# 최종 요약 | Final Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Final Summary | 최종 요약\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Q5 - LightGBM Baseline RMSE: {rmse_baseline:.6f}\")\n",
    "print(f\"Q6 - Optuna Tuned LightGBM RMSE: {rmse_best:.6f}\")\n",
    "print(f\"Q8 - GRU Model RMSE: {gru_rmse:.6f}\")\n",
    "print(f\"Q10 - Ensemble RMSE: {ensemble_rmse:.6f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBLnx0tl6Tdz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
