{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. 환경 설정 및 전처리 (Setup & Preprocessing)\n",
    "# Environment Setup and Preprocessing Utilities\n",
    "\n",
    "---\n",
    "\n",
    "## 목차 / Table of Contents\n",
    "1. 라이브러리 임포트 / Library Imports\n",
    "2. 토이 데이터 생성 / Toy Data Generation\n",
    "3. 결측치 처리 / Missing Value Handling\n",
    "4. 이상치 탐지 및 처리 / Outlier Detection & Handling\n",
    "5. 스케일링 / Scaling\n",
    "6. 인코딩 / Encoding\n",
    "7. 데이터 분할 / Train-Test Split\n",
    "8. 불균형 데이터 처리 / Imbalanced Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 라이브러리 임포트 / Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 필수 라이브러리 임포트 / Essential Library Imports\n",
    "# ============================================================\n",
    "# 복사하여 사용 / Copy and use\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 시각화 설정 / Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# 랜덤 시드 고정 / Fix random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"✅ 기본 라이브러리 로드 완료 / Basic libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 사이킷런 라이브러리 / Scikit-learn Libraries\n",
    "# ============================================================\n",
    "\n",
    "# 전처리 / Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# 데이터 분할 / Data splitting\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n",
    "\n",
    "# 회귀 모델 / Regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# 분류 모델 / Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 평가 지표 / Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    # 회귀 / Regression\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    # 분류 / Classification\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "print(\"✅ Scikit-learn 라이브러리 로드 완료 / Scikit-learn libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# XGBoost / XGBoost\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    print(\"✅ XGBoost 로드 완료 / XGBoost loaded\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ XGBoost 미설치. 설치: !pip install xgboost\")\n",
    "    print(\"⚠️ XGBoost not installed. Install: !pip install xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 불균형 데이터 처리 / Imbalanced Data Handling\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    print(\"✅ imbalanced-learn 로드 완료 / imbalanced-learn loaded\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ imbalanced-learn 미설치. 설치: !pip install imbalanced-learn\")\n",
    "    print(\"⚠️ imbalanced-learn not installed. Install: !pip install imbalanced-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 토이 데이터 생성 / Toy Data Generation\n",
    "\n",
    "시험에서 제공되는 데이터를 대신하여 연습용 토이 데이터를 생성합니다.\n",
    "Generate toy data for practice instead of exam-provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 회귀용 토이 데이터 생성 / Toy Data for Regression\n",
    "# ============================================================\n",
    "\n",
    "def create_regression_data(n_samples=1000, n_features=5, noise=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    회귀 문제용 토이 데이터 생성 / Create toy data for regression\n",
    "    \n",
    "    Args:\n",
    "        n_samples: 샘플 수 / Number of samples\n",
    "        n_features: 피처 수 / Number of features\n",
    "        noise: 노이즈 수준 / Noise level\n",
    "        random_state: 랜덤 시드 / Random seed\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with features and target\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # 피처 생성 / Generate features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # 타겟 생성 (선형 조합 + 노이즈) / Generate target (linear combination + noise)\n",
    "    coefficients = np.random.randn(n_features)\n",
    "    y = X @ coefficients + noise * np.random.randn(n_samples)\n",
    "    \n",
    "    # DataFrame 생성 / Create DataFrame\n",
    "    feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['target'] = y\n",
    "    \n",
    "    # 일부 결측치 추가 (현실적 데이터) / Add some missing values (realistic)\n",
    "    mask = np.random.random(df.shape) < 0.02  # 2% 결측치\n",
    "    df = df.mask(mask)\n",
    "    df['target'] = y  # 타겟은 결측 없음 / Target has no missing\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "df_reg = create_regression_data(n_samples=1000, n_features=5)\n",
    "print(f\"회귀 데이터 shape: {df_reg.shape}\")\n",
    "print(f\"Regression data shape: {df_reg.shape}\")\n",
    "df_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 분류용 토이 데이터 생성 (불균형 포함) / Toy Data for Classification (Imbalanced)\n",
    "# ============================================================\n",
    "\n",
    "def create_classification_data(n_samples=1000, n_features=5, imbalance_ratio=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    분류 문제용 토이 데이터 생성 (불균형 데이터)\n",
    "    Create toy data for classification (imbalanced)\n",
    "    \n",
    "    Args:\n",
    "        n_samples: 샘플 수 / Number of samples\n",
    "        n_features: 피처 수 / Number of features\n",
    "        imbalance_ratio: 양성 클래스 비율 (0.1 = 10%) / Positive class ratio\n",
    "        random_state: 랜덤 시드 / Random seed\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with features and target\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # 클래스별 샘플 수 / Samples per class\n",
    "    n_positive = int(n_samples * imbalance_ratio)\n",
    "    n_negative = n_samples - n_positive\n",
    "    \n",
    "    # 음성 클래스 (정상) / Negative class (normal)\n",
    "    X_neg = np.random.randn(n_negative, n_features)\n",
    "    y_neg = np.zeros(n_negative)\n",
    "    \n",
    "    # 양성 클래스 (이상) - 약간 다른 분포 / Positive class (anomaly) - slightly different distribution\n",
    "    X_pos = np.random.randn(n_positive, n_features) + 1.5\n",
    "    y_pos = np.ones(n_positive)\n",
    "    \n",
    "    # 합치기 / Combine\n",
    "    X = np.vstack([X_neg, X_pos])\n",
    "    y = np.hstack([y_neg, y_pos])\n",
    "    \n",
    "    # 셔플 / Shuffle\n",
    "    shuffle_idx = np.random.permutation(n_samples)\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    \n",
    "    # DataFrame 생성 / Create DataFrame\n",
    "    feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['target'] = y.astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "df_clf = create_classification_data(n_samples=1000, imbalance_ratio=0.1)\n",
    "print(f\"분류 데이터 shape: {df_clf.shape}\")\n",
    "print(f\"Classification data shape: {df_clf.shape}\")\n",
    "print(f\"\\n클래스 분포 / Class distribution:\")\n",
    "print(df_clf['target'].value_counts())\n",
    "df_clf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 시계열용 토이 데이터 생성 / Toy Data for Time Series\n",
    "# ============================================================\n",
    "\n",
    "def create_timeseries_data(n_samples=500, freq='D', trend=0.01, seasonality=True, random_state=42):\n",
    "    \"\"\"\n",
    "    시계열 문제용 토이 데이터 생성\n",
    "    Create toy data for time series analysis\n",
    "    \n",
    "    Args:\n",
    "        n_samples: 샘플 수 (일 수) / Number of samples (days)\n",
    "        freq: 주기 ('D'=일, 'H'=시간) / Frequency\n",
    "        trend: 추세 강도 / Trend strength\n",
    "        seasonality: 계절성 포함 여부 / Include seasonality\n",
    "        random_state: 랜덤 시드 / Random seed\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with datetime index and value column\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # 날짜 인덱스 생성 / Create date index\n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_samples, freq=freq)\n",
    "    \n",
    "    # 기본값 / Base value\n",
    "    t = np.arange(n_samples)\n",
    "    \n",
    "    # 추세 / Trend\n",
    "    trend_component = trend * t\n",
    "    \n",
    "    # 계절성 (주간 패턴) / Seasonality (weekly pattern)\n",
    "    if seasonality:\n",
    "        seasonal_component = 5 * np.sin(2 * np.pi * t / 7)\n",
    "    else:\n",
    "        seasonal_component = 0\n",
    "    \n",
    "    # 노이즈 / Noise\n",
    "    noise = np.random.randn(n_samples) * 2\n",
    "    \n",
    "    # 최종 값 / Final value\n",
    "    values = 100 + trend_component + seasonal_component + noise\n",
    "    \n",
    "    # DataFrame 생성 / Create DataFrame\n",
    "    df = pd.DataFrame({'date': dates, 'value': values})\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "df_ts = create_timeseries_data(n_samples=365)\n",
    "print(f\"시계열 데이터 shape: {df_ts.shape}\")\n",
    "print(f\"Time series data shape: {df_ts.shape}\")\n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 제조 데이터 (이상 탐지용) / Manufacturing Data (for Anomaly Detection)\n",
    "# ============================================================\n",
    "\n",
    "def create_manufacturing_data(n_samples=1000, n_sensors=10, anomaly_ratio=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    제조 이상 탐지용 토이 데이터 생성\n",
    "    Create toy data for manufacturing anomaly detection\n",
    "    \n",
    "    Args:\n",
    "        n_samples: 샘플 수 / Number of samples\n",
    "        n_sensors: 센서 수 / Number of sensors\n",
    "        anomaly_ratio: 이상 비율 / Anomaly ratio\n",
    "        random_state: 랜덤 시드 / Random seed\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sensor readings and label\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # 정상 샘플 / Normal samples\n",
    "    n_normal = int(n_samples * (1 - anomaly_ratio))\n",
    "    n_anomaly = n_samples - n_normal\n",
    "    \n",
    "    # 정상 데이터 (각 센서별 정상 범위) / Normal data\n",
    "    normal_data = []\n",
    "    for i in range(n_sensors):\n",
    "        mean = 50 + i * 5\n",
    "        std = 5\n",
    "        normal_data.append(np.random.normal(mean, std, n_normal))\n",
    "    X_normal = np.array(normal_data).T\n",
    "    \n",
    "    # 이상 데이터 (일부 센서에서 비정상 값) / Anomaly data\n",
    "    anomaly_data = []\n",
    "    for i in range(n_sensors):\n",
    "        mean = 50 + i * 5\n",
    "        std = 5\n",
    "        values = np.random.normal(mean, std, n_anomaly)\n",
    "        # 일부 센서에서 이상값 / Abnormal values in some sensors\n",
    "        if i < 3:  # 처음 3개 센서에서 이상 발생\n",
    "            values += np.random.choice([-20, 20], n_anomaly) * np.random.random(n_anomaly)\n",
    "        anomaly_data.append(values)\n",
    "    X_anomaly = np.array(anomaly_data).T\n",
    "    \n",
    "    # 합치기 / Combine\n",
    "    X = np.vstack([X_normal, X_anomaly])\n",
    "    y = np.hstack([np.zeros(n_normal), np.ones(n_anomaly)])\n",
    "    \n",
    "    # 셔플 / Shuffle\n",
    "    shuffle_idx = np.random.permutation(n_samples)\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    \n",
    "    # DataFrame 생성 / Create DataFrame\n",
    "    sensor_names = [f'sensor_{i}' for i in range(n_sensors)]\n",
    "    df = pd.DataFrame(X, columns=sensor_names)\n",
    "    df['label'] = y.astype(int)  # 0: 정상, 1: 이상 / 0: Normal, 1: Anomaly\n",
    "    \n",
    "    # 일부 결측치 추가 / Add some missing values\n",
    "    mask = np.random.random((n_samples, n_sensors)) < 0.01\n",
    "    df.iloc[:, :n_sensors] = df.iloc[:, :n_sensors].mask(mask)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "df_mfg = create_manufacturing_data(n_samples=1000, anomaly_ratio=0.05)\n",
    "print(f\"제조 데이터 shape: {df_mfg.shape}\")\n",
    "print(f\"Manufacturing data shape: {df_mfg.shape}\")\n",
    "print(f\"\\n레이블 분포 / Label distribution:\")\n",
    "print(df_mfg['label'].value_counts())\n",
    "df_mfg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 결측치 처리 / Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 결측치 확인 / Check Missing Values\n",
    "# ============================================================\n",
    "\n",
    "def check_missing(df):\n",
    "    \"\"\"\n",
    "    결측치 현황 확인 / Check missing value status\n",
    "    \"\"\"\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'missing_count': missing,\n",
    "        'missing_pct': missing_pct\n",
    "    })\n",
    "    result = result[result['missing_count'] > 0].sort_values('missing_count', ascending=False)\n",
    "    \n",
    "    print(f\"전체 결측치 수: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
    "    return result\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "check_missing(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 결측치 처리 방법들 / Missing Value Imputation Methods\n",
    "# ============================================================\n",
    "\n",
    "# 방법 1: 평균/중앙값/최빈값 대체 / Method 1: Mean/Median/Mode imputation\n",
    "def impute_simple(df, strategy='mean', columns=None):\n",
    "    \"\"\"\n",
    "    단순 대체법으로 결측치 처리\n",
    "    Impute missing values with simple strategy\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        strategy: 'mean', 'median', 'most_frequent'\n",
    "        columns: 처리할 컬럼 (None이면 수치형 전체) / Columns to impute\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df_copy.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    df_copy[columns] = imputer.fit_transform(df_copy[columns])\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 방법 2: KNN 대체 / Method 2: KNN imputation\n",
    "def impute_knn(df, n_neighbors=5, columns=None):\n",
    "    \"\"\"\n",
    "    KNN 대체법으로 결측치 처리\n",
    "    Impute missing values with KNN\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df_copy.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_copy[columns] = imputer.fit_transform(df_copy[columns])\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 방법 3: 앞/뒤 값으로 채우기 (시계열용) / Method 3: Forward/Backward fill (for time series)\n",
    "def impute_ffill_bfill(df, columns=None):\n",
    "    \"\"\"\n",
    "    앞/뒤 값으로 결측치 채우기 (시계열 데이터에 적합)\n",
    "    Fill missing values with forward/backward fill\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df_copy.columns\n",
    "    \n",
    "    df_copy[columns] = df_copy[columns].ffill().bfill()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "print(\"원본 결측치 / Original missing:\")\n",
    "print(df_reg.isnull().sum().sum())\n",
    "\n",
    "df_imputed = impute_simple(df_reg, strategy='mean')\n",
    "print(\"\\n대체 후 결측치 / After imputation:\")\n",
    "print(df_imputed.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 이상치 탐지 및 처리 / Outlier Detection & Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IQR 방식 이상치 탐지 / IQR-based Outlier Detection\n",
    "# ============================================================\n",
    "\n",
    "def detect_outliers_iqr(df, columns=None, k=1.5):\n",
    "    \"\"\"\n",
    "    IQR 방식으로 이상치 탐지\n",
    "    Detect outliers using IQR method\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        columns: 검사할 컬럼 / Columns to check\n",
    "        k: IQR 배수 (기본 1.5) / IQR multiplier\n",
    "    \n",
    "    Returns:\n",
    "        이상치 마스크 DataFrame / Outlier mask DataFrame\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    outlier_mask = pd.DataFrame(False, index=df.index, columns=columns)\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - k * IQR\n",
    "        upper_bound = Q3 + k * IQR\n",
    "        \n",
    "        outlier_mask[col] = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "    \n",
    "    # 결과 출력 / Print results\n",
    "    print(\"컬럼별 이상치 수 / Outliers per column:\")\n",
    "    print(outlier_mask.sum())\n",
    "    print(f\"\\n전체 이상치 행 수: {outlier_mask.any(axis=1).sum()}\")\n",
    "    \n",
    "    return outlier_mask\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "outlier_mask = detect_outliers_iqr(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Z-score 방식 이상치 탐지 / Z-score based Outlier Detection\n",
    "# ============================================================\n",
    "\n",
    "def detect_outliers_zscore(df, columns=None, threshold=3):\n",
    "    \"\"\"\n",
    "    Z-score 방식으로 이상치 탐지\n",
    "    Detect outliers using Z-score method\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        columns: 검사할 컬럼 / Columns to check\n",
    "        threshold: Z-score 임계값 (기본 3) / Z-score threshold\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    outlier_mask = pd.DataFrame(False, index=df.index, columns=columns)\n",
    "    \n",
    "    for col in columns:\n",
    "        z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "        outlier_mask.loc[df[col].dropna().index, col] = z_scores > threshold\n",
    "    \n",
    "    print(\"컬럼별 이상치 수 / Outliers per column:\")\n",
    "    print(outlier_mask.sum())\n",
    "    \n",
    "    return outlier_mask\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "outlier_mask_z = detect_outliers_zscore(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 이상치 처리 방법 / Outlier Handling Methods\n",
    "# ============================================================\n",
    "\n",
    "def handle_outliers(df, columns=None, method='clip', k=1.5):\n",
    "    \"\"\"\n",
    "    이상치 처리\n",
    "    Handle outliers\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        columns: 처리할 컬럼 / Columns to handle\n",
    "        method: 'clip' (경계값으로 대체), 'remove' (제거), 'nan' (NaN으로 대체)\n",
    "        k: IQR 배수 / IQR multiplier\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df_copy.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df_copy[col].quantile(0.25)\n",
    "        Q3 = df_copy[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - k * IQR\n",
    "        upper_bound = Q3 + k * IQR\n",
    "        \n",
    "        if method == 'clip':\n",
    "            # 경계값으로 클리핑 / Clip to bounds\n",
    "            df_copy[col] = df_copy[col].clip(lower_bound, upper_bound)\n",
    "        elif method == 'nan':\n",
    "            # NaN으로 대체 / Replace with NaN\n",
    "            mask = (df_copy[col] < lower_bound) | (df_copy[col] > upper_bound)\n",
    "            df_copy.loc[mask, col] = np.nan\n",
    "        elif method == 'remove':\n",
    "            # 이상치 행 제거는 루프 외부에서 처리 / Remove rows outside loop\n",
    "            pass\n",
    "    \n",
    "    if method == 'remove':\n",
    "        # 이상치가 있는 행 제거 / Remove rows with outliers\n",
    "        outlier_mask = detect_outliers_iqr(df, columns, k)\n",
    "        df_copy = df_copy[~outlier_mask.any(axis=1)]\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "print(f\"원본 shape: {df_reg.shape}\")\n",
    "df_clipped = handle_outliers(df_reg, method='clip')\n",
    "print(f\"클리핑 후 shape: {df_clipped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 스케일링 / Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 스케일링 방법 비교 / Scaling Methods Comparison\n",
    "# ============================================================\n",
    "\n",
    "def scale_data(X_train, X_test, method='standard'):\n",
    "    \"\"\"\n",
    "    데이터 스케일링\n",
    "    Scale data\n",
    "    \n",
    "    Args:\n",
    "        X_train: 훈련 데이터 / Training data\n",
    "        X_test: 테스트 데이터 / Test data\n",
    "        method: 'standard', 'minmax', 'robust'\n",
    "    \n",
    "    Returns:\n",
    "        스케일링된 X_train, X_test, scaler 객체\n",
    "    \n",
    "    Note:\n",
    "        ⚠️ 반드시 훈련 데이터로만 fit하고 테스트 데이터는 transform만!\n",
    "        ⚠️ Always fit on training data only, then transform test data!\n",
    "    \"\"\"\n",
    "    if method == 'standard':\n",
    "        # 평균 0, 표준편차 1로 변환 / Transform to mean=0, std=1\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        # 0~1 범위로 변환 / Transform to 0-1 range\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'robust':\n",
    "        # 중앙값과 IQR 사용 (이상치에 강건) / Use median and IQR (robust to outliers)\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # 훈련 데이터로 fit, 훈련/테스트 모두 transform\n",
    "    # Fit on training data, transform both\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "# 데이터 준비 / Prepare data\n",
    "df_clean = impute_simple(df_reg, strategy='mean')\n",
    "X = df_clean.drop('target', axis=1)\n",
    "y = df_clean['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링 / Scaling\n",
    "X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test, method='standard')\n",
    "\n",
    "print(\"스케일링 전 훈련 데이터 통계 / Before scaling:\")\n",
    "print(f\"Mean: {X_train.mean().mean():.4f}, Std: {X_train.std().mean():.4f}\")\n",
    "print(\"\\n스케일링 후 훈련 데이터 통계 / After scaling:\")\n",
    "print(f\"Mean: {X_train_scaled.mean():.4f}, Std: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 인코딩 / Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 범주형 변수 인코딩 / Categorical Variable Encoding\n",
    "# ============================================================\n",
    "\n",
    "# 테스트용 범주형 데이터 생성 / Create categorical test data\n",
    "df_cat = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'red', 'blue'] * 20,\n",
    "    'size': ['S', 'M', 'L', 'M', 'S'] * 20,\n",
    "    'value': np.random.randn(100)\n",
    "})\n",
    "\n",
    "print(\"원본 데이터 / Original data:\")\n",
    "print(df_cat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Label Encoding (순서가 있는 범주형) / Label Encoding (ordinal)\n",
    "# ============================================================\n",
    "\n",
    "def encode_labels(df, columns):\n",
    "    \"\"\"\n",
    "    레이블 인코딩 (순서가 있는 범주형에 적합)\n",
    "    Label encoding (suitable for ordinal categories)\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        df_copy[col] = le.fit_transform(df_copy[col])\n",
    "        encoders[col] = le\n",
    "    \n",
    "    return df_copy, encoders\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "df_encoded, encoders = encode_labels(df_cat, ['color', 'size'])\n",
    "print(\"레이블 인코딩 후 / After label encoding:\")\n",
    "print(df_encoded.head())\n",
    "print(f\"\\ncolor 매핑: {list(encoders['color'].classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# One-Hot Encoding (순서가 없는 범주형) / One-Hot Encoding (nominal)\n",
    "# ============================================================\n",
    "\n",
    "def encode_onehot(df, columns):\n",
    "    \"\"\"\n",
    "    원-핫 인코딩 (순서가 없는 범주형에 적합)\n",
    "    One-hot encoding (suitable for nominal categories)\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy = pd.get_dummies(df_copy, columns=columns, drop_first=True)\n",
    "    return df_copy\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "df_onehot = encode_onehot(df_cat, ['color', 'size'])\n",
    "print(\"원-핫 인코딩 후 / After one-hot encoding:\")\n",
    "print(df_onehot.head())\n",
    "print(f\"\\n컬럼 수 변화: {len(df_cat.columns)} -> {len(df_onehot.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. 데이터 분할 / Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 기본 Train-Test Split / Basic Train-Test Split\n",
    "# ============================================================\n",
    "\n",
    "def split_data(df, target_col, test_size=0.2, val_size=0.1, random_state=42, stratify=False):\n",
    "    \"\"\"\n",
    "    데이터를 Train/Validation/Test로 분할\n",
    "    Split data into Train/Validation/Test\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        target_col: 타겟 컬럼명 / Target column name\n",
    "        test_size: 테스트 비율 / Test ratio\n",
    "        val_size: 검증 비율 (0이면 없음) / Validation ratio\n",
    "        stratify: 층화 샘플링 여부 (분류에 사용) / Stratified sampling\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # 층화 샘플링 설정 / Stratified sampling setting\n",
    "    strat = y if stratify else None\n",
    "    \n",
    "    # Train + (Val+Test) 분할 / Split Train + (Val+Test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=(test_size + val_size), random_state=random_state, stratify=strat\n",
    "    )\n",
    "    \n",
    "    if val_size > 0:\n",
    "        # Val과 Test 분할 / Split Val and Test\n",
    "        val_ratio = val_size / (test_size + val_size)\n",
    "        strat_temp = y_temp if stratify else None\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=(1-val_ratio), random_state=random_state, stratify=strat_temp\n",
    "        )\n",
    "    else:\n",
    "        X_val, y_val = None, None\n",
    "        X_test, y_test = X_temp, y_temp\n",
    "    \n",
    "    print(f\"Train: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    if val_size > 0:\n",
    "        print(f\"Val:   {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"Test:  {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "df_clean = impute_simple(df_reg, strategy='mean')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    df_clean, 'target', test_size=0.2, val_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 시계열 데이터 분할 / Time Series Split\n",
    "# ============================================================\n",
    "\n",
    "def split_timeseries(df, target_col, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    시계열 데이터 분할 (시간 순서 유지)\n",
    "    Time series split (maintaining temporal order)\n",
    "    \n",
    "    ⚠️ 시계열은 랜덤 분할하면 안 됨! 시간 순서대로 분할해야 함\n",
    "    ⚠️ Time series must not be randomly split! Must split by time order\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    \n",
    "    # 분할 지점 계산 / Calculate split points\n",
    "    train_end = int(n * (1 - test_size - val_size))\n",
    "    val_end = int(n * (1 - test_size))\n",
    "    \n",
    "    # 분할 / Split\n",
    "    train = df.iloc[:train_end]\n",
    "    val = df.iloc[train_end:val_end] if val_size > 0 else None\n",
    "    test = df.iloc[val_end:]\n",
    "    \n",
    "    print(f\"Train: {len(train)} samples (시작: {train.index[0]}, 끝: {train.index[-1]})\")\n",
    "    if val_size > 0:\n",
    "        print(f\"Val:   {len(val)} samples (시작: {val.index[0]}, 끝: {val.index[-1]})\")\n",
    "    print(f\"Test:  {len(test)} samples (시작: {test.index[0]}, 끝: {test.index[-1]})\")\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "train_ts, val_ts, test_ts = split_timeseries(df_ts, 'value', test_size=0.2, val_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. 불균형 데이터 처리 / Imbalanced Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 불균형 데이터 확인 / Check Data Imbalance\n",
    "# ============================================================\n",
    "\n",
    "def check_imbalance(y, plot=True):\n",
    "    \"\"\"\n",
    "    클래스 불균형 확인\n",
    "    Check class imbalance\n",
    "    \"\"\"\n",
    "    value_counts = pd.Series(y).value_counts()\n",
    "    imbalance_ratio = value_counts.min() / value_counts.max()\n",
    "    \n",
    "    print(\"클래스 분포 / Class distribution:\")\n",
    "    print(value_counts)\n",
    "    print(f\"\\n불균형 비율 (소수/다수): {imbalance_ratio:.4f}\")\n",
    "    print(f\"Imbalance ratio (minority/majority): {imbalance_ratio:.4f}\")\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        value_counts.plot(kind='bar')\n",
    "        plt.title('Class Distribution / 클래스 분포')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return imbalance_ratio\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "check_imbalance(df_clf['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 불균형 처리 방법들 / Imbalanced Data Handling Methods\n",
    "# ============================================================\n",
    "\n",
    "def handle_imbalance(X, y, method='smote', random_state=42):\n",
    "    \"\"\"\n",
    "    불균형 데이터 처리\n",
    "    Handle imbalanced data\n",
    "    \n",
    "    Args:\n",
    "        X: 피처 / Features\n",
    "        y: 타겟 / Target\n",
    "        method: 'smote', 'oversample', 'undersample', 'smote_tomek'\n",
    "    \n",
    "    Returns:\n",
    "        리샘플링된 X, y / Resampled X, y\n",
    "    \n",
    "    ⚠️ 주의: 반드시 훈련 데이터에만 적용! 테스트 데이터에는 적용하지 않음!\n",
    "    ⚠️ Warning: Apply only to training data! Never apply to test data!\n",
    "    \"\"\"\n",
    "    print(f\"처리 전 / Before: {pd.Series(y).value_counts().to_dict()}\")\n",
    "    \n",
    "    if method == 'smote':\n",
    "        # SMOTE: 합성 소수 클래스 오버샘플링\n",
    "        # SMOTE: Synthetic Minority Oversampling Technique\n",
    "        sampler = SMOTE(random_state=random_state)\n",
    "    elif method == 'oversample':\n",
    "        # 랜덤 오버샘플링 / Random oversampling\n",
    "        sampler = RandomOverSampler(random_state=random_state)\n",
    "    elif method == 'undersample':\n",
    "        # 랜덤 언더샘플링 / Random undersampling\n",
    "        sampler = RandomUnderSampler(random_state=random_state)\n",
    "    elif method == 'smote_tomek':\n",
    "        # SMOTE + Tomek links (하이브리드)\n",
    "        sampler = SMOTETomek(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"처리 후 / After:  {pd.Series(y_resampled).value_counts().to_dict()}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# 사용 예시 / Usage example\n",
    "X_clf = df_clf.drop('target', axis=1)\n",
    "y_clf = df_clf['target']\n",
    "\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
    ")\n",
    "\n",
    "print(\"\\n=== SMOTE 적용 ===\")\n",
    "X_train_resampled, y_train_resampled = handle_imbalance(X_train_clf, y_train_clf, method='smote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# class_weight 사용법 (모델 내부 가중치 조정)\n",
    "# Using class_weight (internal model weighting)\n",
    "# ============================================================\n",
    "\n",
    "# 방법 1: 직접 가중치 계산 / Method 1: Calculate weights manually\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train_clf)\n",
    "weights = compute_class_weight('balanced', classes=classes, y=y_train_clf)\n",
    "class_weight_dict = dict(zip(classes, weights))\n",
    "\n",
    "print(\"계산된 클래스 가중치 / Computed class weights:\")\n",
    "print(class_weight_dict)\n",
    "\n",
    "# 방법 2: 모델에 직접 적용 / Method 2: Apply directly to model\n",
    "# ⚠️ XGBoost는 scale_pos_weight 사용\n",
    "# ⚠️ XGBoost uses scale_pos_weight instead\n",
    "\n",
    "# RandomForest 예시 / RandomForest example\n",
    "rf_weighted = RandomForestClassifier(\n",
    "    class_weight='balanced',  # 또는 class_weight_dict\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# XGBoost 예시 / XGBoost example\n",
    "# scale_pos_weight = 다수 클래스 수 / 소수 클래스 수\n",
    "# scale_pos_weight = majority count / minority count\n",
    "neg_count = (y_train_clf == 0).sum()\n",
    "pos_count = (y_train_clf == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"\\nXGBoost scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "xgb_weighted = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 전처리 파이프라인 요약 / Preprocessing Pipeline Summary\n",
    "\n",
    "```python\n",
    "# ============================================================\n",
    "# 전체 전처리 파이프라인 예시 / Complete Preprocessing Pipeline\n",
    "# ============================================================\n",
    "\n",
    "# 1. 데이터 로드 / Load data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# 2. 결측치 확인 및 처리 / Check and handle missing values\n",
    "check_missing(df)\n",
    "df = impute_simple(df, strategy='mean')\n",
    "\n",
    "# 3. 이상치 탐지 및 처리 / Detect and handle outliers\n",
    "detect_outliers_iqr(df)\n",
    "df = handle_outliers(df, method='clip')\n",
    "\n",
    "# 4. 데이터 분할 / Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, 'target')\n",
    "\n",
    "# 5. 스케일링 / Scaling\n",
    "X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test, method='standard')\n",
    "\n",
    "# 6. (분류) 불균형 처리 / (Classification) Handle imbalance\n",
    "X_train_balanced, y_train_balanced = handle_imbalance(X_train_scaled, y_train, method='smote')\n",
    "\n",
    "# 7. 모델 학습 / Train model\n",
    "model.fit(X_train_balanced, y_train_balanced)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ 전처리 노트북 완료 / Preprocessing notebook complete!\")\n",
    "print(\"\\n다음 노트북: 01_regression_models.ipynb\")\n",
    "print(\"Next notebook: 01_regression_models.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
