{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. ë”¥ëŸ¬ë‹ (Deep Learning - DNN, CNN, RNN)\n",
    "\n",
    "---\n",
    "\n",
    "## ëª©ì°¨ / Table of Contents\n",
    "1. PyTorch ê¸°ë³¸ ì„¤ì • / PyTorch Setup\n",
    "2. DNN (Deep Neural Network) - ì •í˜• ë°ì´í„°\n",
    "3. CNN (Convolutional Neural Network) - ì´ë¯¸ì§€/ì‹œí€€ìŠ¤\n",
    "4. RNN/LSTM - ì‹œê³„ì—´ ë°ì´í„°\n",
    "5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ / Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ / Library Imports\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# ì‚¬ì´í‚·ëŸ° / Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, classification_report\n",
    "\n",
    "# ì„¤ì • / Settings\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì • / Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ… Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. PyTorch ê¸°ë³¸ ì„¤ì • / PyTorch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ë°ì´í„° ìƒì„± í•¨ìˆ˜ / Data Generation Functions\n",
    "# ============================================================\n",
    "\n",
    "def create_tabular_data(n_samples=1000, n_features=20, task='regression', random_state=42):\n",
    "    \"\"\"\n",
    "    ì •í˜• ë°ì´í„° ìƒì„± (DNNìš©)\n",
    "    Create tabular data for DNN\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    if task == 'regression':\n",
    "        y = X[:, :5].sum(axis=1) + 0.5 * X[:, 0]**2 + np.random.randn(n_samples) * 0.1\n",
    "    else:  # classification\n",
    "        y = (X[:, :3].sum(axis=1) > 0).astype(int)\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.float32 if task == 'regression' else np.int64)\n",
    "\n",
    "def create_sequence_data(n_samples=500, seq_length=50, n_features=1, random_state=42):\n",
    "    \"\"\"\n",
    "    ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (RNNìš©)\n",
    "    Create sequence data for RNN\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # ì‚¬ì¸íŒŒ + ë…¸ì´ì¦ˆ / Sine wave + noise\n",
    "        t = np.linspace(0, 4*np.pi, seq_length + 1)\n",
    "        signal = np.sin(t) + np.random.randn(seq_length + 1) * 0.1\n",
    "        \n",
    "        X.append(signal[:-1].reshape(-1, n_features))\n",
    "        y.append(signal[-1])  # ë‹¤ìŒ ê°’ ì˜ˆì¸¡ / Predict next value\n",
    "    \n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ / Data generation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# í•™ìŠµ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ / Training Utility Functions\n",
    "# ============================================================\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=50, val_loader=None, verbose=True):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜\n",
    "    Model training function\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = criterion(outputs.squeeze(), y_batch)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            if val_loader:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_regression(model, test_loader):\n",
    "    \"\"\"\n",
    "    íšŒê·€ ëª¨ë¸ í‰ê°€\n",
    "    Evaluate regression model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    return predictions, actuals\n",
    "\n",
    "def evaluate_classification(model, test_loader):\n",
    "    \"\"\"\n",
    "    ë¶„ë¥˜ ëª¨ë¸ í‰ê°€\n",
    "    Evaluate classification model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    f1 = f1_score(actuals, predictions, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return predictions, actuals\n",
    "\n",
    "print(\"âœ… í•™ìŠµ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ / Training utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. DNN (Deep Neural Network) - ì •í˜• ë°ì´í„°\n",
    "\n",
    "**ì í•©í•œ ë°ì´í„°**: í…Œì´ë¸” í˜•íƒœì˜ ì •í˜• ë°ì´í„°\n",
    "**Suitable for**: Tabular/structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DNN ëª¨ë¸ ì •ì˜ (íšŒê·€ìš©) / DNN Model Definition (Regression)\n",
    "# ============================================================\n",
    "\n",
    "class DNNRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    DNN íšŒê·€ ëª¨ë¸\n",
    "    DNN Regression Model\n",
    "    \n",
    "    êµ¬ì¡°: Input -> Hidden1 -> Hidden2 -> Hidden3 -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.2):\n",
    "        super(DNNRegressor, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))  # ë°°ì¹˜ ì •ê·œí™” / Batch normalization\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))  # ì¶œë ¥ì¸µ / Output layer\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# ëª¨ë¸ í™•ì¸ / Check model\n",
    "sample_model = DNNRegressor(input_dim=20)\n",
    "print(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DNN íšŒê·€ í•™ìŠµ / DNN Regression Training\n",
    "# ============================================================\n",
    "\n",
    "# ë°ì´í„° ìƒì„± / Generate data\n",
    "X, y = create_tabular_data(n_samples=1000, n_features=20, task='regression')\n",
    "\n",
    "# ë¶„í•  / Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§ / Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test_scaled = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "# DataLoader ìƒì„± / Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_scaled), torch.tensor(y_train))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test_scaled), torch.tensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± / Create model\n",
    "dnn_reg = DNNRegressor(input_dim=20, hidden_dims=[128, 64, 32], dropout=0.2)\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € / Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dnn_reg.parameters(), lr=0.001)\n",
    "\n",
    "# í•™ìŠµ / Train\n",
    "print(\"DNN íšŒê·€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "train_losses, _ = train_model(dnn_reg, train_loader, criterion, optimizer, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DNN íšŒê·€ í‰ê°€ / DNN Regression Evaluation\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nğŸ“Š DNN íšŒê·€ í‰ê°€ ê²°ê³¼:\")\n",
    "predictions, actuals = evaluate_regression(dnn_reg, test_loader)\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™” / Visualize learning curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "\n",
    "axes[1].scatter(actuals, predictions, alpha=0.5)\n",
    "axes[1].plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'r--')\n",
    "axes[1].set_xlabel('Actual')\n",
    "axes[1].set_ylabel('Predicted')\n",
    "axes[1].set_title('Actual vs Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DNN ë¶„ë¥˜ ëª¨ë¸ / DNN Classification Model\n",
    "# ============================================================\n",
    "\n",
    "class DNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    DNN ë¶„ë¥˜ ëª¨ë¸\n",
    "    DNN Classification Model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], num_classes=2, dropout=0.2):\n",
    "        super(DNNClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# ë¶„ë¥˜ ë°ì´í„° ìƒì„± / Generate classification data\n",
    "X_clf, y_clf = create_tabular_data(n_samples=1000, n_features=20, task='classification')\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=RANDOM_STATE, stratify=y_clf\n",
    ")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§ / Scaling\n",
    "X_train_clf_scaled = scaler.fit_transform(X_train_clf).astype(np.float32)\n",
    "X_test_clf_scaled = scaler.transform(X_test_clf).astype(np.float32)\n",
    "\n",
    "# DataLoader\n",
    "train_clf_dataset = TensorDataset(torch.tensor(X_train_clf_scaled), torch.tensor(y_train_clf))\n",
    "test_clf_dataset = TensorDataset(torch.tensor(X_test_clf_scaled), torch.tensor(y_test_clf))\n",
    "\n",
    "train_clf_loader = DataLoader(train_clf_dataset, batch_size=32, shuffle=True)\n",
    "test_clf_loader = DataLoader(test_clf_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ / Create and train model\n",
    "dnn_clf = DNNClassifier(input_dim=20, num_classes=2)\n",
    "criterion_clf = nn.CrossEntropyLoss()\n",
    "optimizer_clf = optim.Adam(dnn_clf.parameters(), lr=0.001)\n",
    "\n",
    "print(\"DNN ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "train_model(dnn_clf, train_clf_loader, criterion_clf, optimizer_clf, epochs=50)\n",
    "\n",
    "print(\"\\nğŸ“Š DNN ë¶„ë¥˜ í‰ê°€ ê²°ê³¼:\")\n",
    "evaluate_classification(dnn_clf, test_clf_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. CNN (Convolutional Neural Network)\n",
    "\n",
    "**ì í•©í•œ ë°ì´í„°**: ì´ë¯¸ì§€, 1D ì‹œí€€ìŠ¤ ë°ì´í„°\n",
    "**Suitable for**: Images, 1D sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1D CNN ëª¨ë¸ (ì‹œí€€ìŠ¤/ì‹œê³„ì—´ìš©) / 1D CNN Model (for sequences/time series)\n",
    "# ============================================================\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN ëª¨ë¸ (ì‹œê³„ì—´/ì‹œí€€ìŠ¤ ë°ì´í„°ìš©)\n",
    "    1D CNN Model for time series/sequence data\n",
    "    \n",
    "    ì…ë ¥: (batch_size, seq_length, n_features)\n",
    "    Input: (batch_size, seq_length, n_features)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=1, seq_length=50, num_classes=1):\n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        # Conv ë ˆì´ì–´ë“¤ / Conv layers\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # í’€ë§ í›„ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚° / Calculate sequence length after pooling\n",
    "        # 3ë²ˆì˜ í’€ë§: seq_length -> seq_length/2 -> seq_length/4 -> seq_length/8\n",
    "        final_seq_len = seq_length // 8\n",
    "        \n",
    "        # ì™„ì „ ì—°ê²°ì¸µ / Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * final_seq_len, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features) -> (batch, features, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Conv blocks\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"CNN1D ëª¨ë¸ êµ¬ì¡°:\")\n",
    "cnn_sample = CNN1D(input_channels=1, seq_length=48)  # 48 = 6*8 (8ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ë„ë¡)\n",
    "print(cnn_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CNN í•™ìŠµ ì˜ˆì‹œ / CNN Training Example\n",
    "# ============================================================\n",
    "\n",
    "# ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± / Generate sequence data\n",
    "X_seq, y_seq = create_sequence_data(n_samples=500, seq_length=48, n_features=1)\n",
    "\n",
    "# ë¶„í•  / Split\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "train_seq_dataset = TensorDataset(torch.tensor(X_train_seq), torch.tensor(y_train_seq))\n",
    "test_seq_dataset = TensorDataset(torch.tensor(X_test_seq), torch.tensor(y_test_seq))\n",
    "\n",
    "train_seq_loader = DataLoader(train_seq_dataset, batch_size=32, shuffle=True)\n",
    "test_seq_loader = DataLoader(test_seq_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ / Create and train model\n",
    "cnn_model = CNN1D(input_channels=1, seq_length=48, num_classes=1)\n",
    "criterion_cnn = nn.MSELoss()\n",
    "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"CNN ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "train_model(cnn_model, train_seq_loader, criterion_cnn, optimizer_cnn, epochs=50)\n",
    "\n",
    "print(\"\\nğŸ“Š CNN í‰ê°€ ê²°ê³¼:\")\n",
    "evaluate_regression(cnn_model, test_seq_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. RNN/LSTM - ì‹œê³„ì—´ ë°ì´í„°\n",
    "\n",
    "**ì í•©í•œ ë°ì´í„°**: ì‹œê³„ì—´, ìˆœì°¨ì  ë°ì´í„°\n",
    "**Suitable for**: Time series, sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LSTM ëª¨ë¸ ì •ì˜ / LSTM Model Definition\n",
    "# ============================================================\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM ëª¨ë¸ (ì‹œê³„ì—´ ì˜ˆì¸¡ìš©)\n",
    "    LSTM Model for time series forecasting\n",
    "    \n",
    "    ì…ë ¥: (batch_size, seq_length, input_dim)\n",
    "    Input: (batch_size, seq_length, input_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2, output_dim=1, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM ë ˆì´ì–´ / LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,  # (batch, seq, feature) í˜•íƒœ\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # ì¶œë ¥ì¸µ / Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM ì¶œë ¥ / LSTM output\n",
    "        # lstm_out: (batch, seq_len, hidden_dim)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ë§Œ ì‚¬ìš© / Use only last timestep output\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"LSTM ëª¨ë¸ êµ¬ì¡°:\")\n",
    "lstm_sample = LSTMModel(input_dim=1, hidden_dim=64, num_layers=2)\n",
    "print(lstm_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LSTM í•™ìŠµ / LSTM Training\n",
    "# ============================================================\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± / Create model\n",
    "lstm_model = LSTMModel(input_dim=1, hidden_dim=64, num_layers=2, output_dim=1)\n",
    "criterion_lstm = nn.MSELoss()\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "train_model(lstm_model, train_seq_loader, criterion_lstm, optimizer_lstm, epochs=50)\n",
    "\n",
    "print(\"\\nğŸ“Š LSTM í‰ê°€ ê²°ê³¼:\")\n",
    "predictions_lstm, actuals_lstm = evaluate_regression(lstm_model, test_seq_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GRU ëª¨ë¸ (LSTM ëŒ€ì•ˆ) / GRU Model (LSTM Alternative)\n",
    "# ============================================================\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU ëª¨ë¸\n",
    "    - LSTMë³´ë‹¤ íŒŒë¼ë¯¸í„°ê°€ ì ì–´ ë¹ ë¦„\n",
    "    - ë¹„ìŠ·í•œ ì„±ëŠ¥\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2, output_dim=1, dropout=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, h_n = self.gru(x)\n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# GRU í•™ìŠµ / GRU Training\n",
    "gru_model = GRUModel(input_dim=1, hidden_dim=64, num_layers=2)\n",
    "optimizer_gru = optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"GRU ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "train_model(gru_model, train_seq_loader, criterion_lstm, optimizer_gru, epochs=50)\n",
    "\n",
    "print(\"\\nğŸ“Š GRU í‰ê°€ ê²°ê³¼:\")\n",
    "evaluate_regression(gru_model, test_seq_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ / Model Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ëª¨ë¸ ì €ì¥ / Save Model\n",
    "# ============================================================\n",
    "\n",
    "# ë°©ë²• 1: state_dict ì €ì¥ (ê¶Œì¥) / Method 1: Save state_dict (recommended)\n",
    "# torch.save(lstm_model.state_dict(), 'lstm_model.pth')\n",
    "\n",
    "# ë°©ë²• 2: ì „ì²´ ëª¨ë¸ ì €ì¥ / Method 2: Save entire model\n",
    "# torch.save(lstm_model, 'lstm_model_full.pth')\n",
    "\n",
    "print(\"ëª¨ë¸ ì €ì¥ ë°©ë²•:\")\n",
    "print(\"# ì €ì¥ / Save\")\n",
    "print(\"torch.save(model.state_dict(), 'model.pth')\")\n",
    "print(\"\")\n",
    "print(\"# ë¡œë“œ / Load\")\n",
    "print(\"model = LSTMModel(...)  # ë™ì¼í•œ êµ¬ì¡°ë¡œ ìƒì„±\")\n",
    "print(\"model.load_state_dict(torch.load('model.pth'))\")\n",
    "print(\"model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ìš”ì•½: ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ / Summary: Deep Learning Model Selection Guide\n",
    "\n",
    "| ëª¨ë¸ | ì í•©í•œ ë°ì´í„° | íŠ¹ì§• |\n",
    "|------|---------------|------|\n",
    "| **DNN** | ì •í˜• ë°ì´í„° (í…Œì´ë¸”) | ê°€ì¥ ê¸°ë³¸, ë¹ ë¦„ |\n",
    "| **CNN** | ì´ë¯¸ì§€, 1D ì‹œí€€ìŠ¤ | ì§€ì—­ì  íŒ¨í„´ í•™ìŠµ |\n",
    "| **LSTM** | ì‹œê³„ì—´, ìˆœì°¨ ë°ì´í„° | ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ |\n",
    "| **GRU** | ì‹œê³„ì—´, ìˆœì°¨ ë°ì´í„° | LSTMë³´ë‹¤ ë¹ ë¦„ |\n",
    "\n",
    "### í•µì‹¬ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n",
    "```python\n",
    "# 1. ë°ì´í„° ì¤€ë¹„\n",
    "X_train, X_test = train_test_split(X, y)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 2. DataLoader ìƒì„±\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 3. ëª¨ë¸ ì •ì˜\n",
    "model = LSTMModel(input_dim=1, hidden_dim=64).to(device)\n",
    "\n",
    "# 4. ì†ì‹¤í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €\n",
    "criterion = nn.MSELoss()  # íšŒê·€\n",
    "criterion = nn.CrossEntropyLoss()  # ë¶„ë¥˜\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 6. í‰ê°€\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ… ë”¥ëŸ¬ë‹ ë…¸íŠ¸ë¶ ì™„ë£Œ / Deep Learning notebook complete!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ ëª¨ë“  ì¹˜íŠ¸ì‹œíŠ¸ ë…¸íŠ¸ë¶ ì‘ì„± ì™„ë£Œ!\")\n",
    "print(\"ğŸ‰ All cheatsheet notebooks completed!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\níŒŒì¼ ëª©ë¡ / File List:\")\n",
    "print(\"  00_setup_and_preprocessing.ipynb - í™˜ê²½ ì„¤ì • ë° ì „ì²˜ë¦¬\")\n",
    "print(\"  01_regression_models.ipynb - íšŒê·€ ëª¨ë¸\")\n",
    "print(\"  02_classification_models.ipynb - ë¶„ë¥˜ ëª¨ë¸\")\n",
    "print(\"  03_timeseries_arima.ipynb - ì‹œê³„ì—´ ARIMA\")\n",
    "print(\"  04_anomaly_detection.ipynb - ì´ìƒ íƒì§€\")\n",
    "print(\"  05_deep_learning.ipynb - ë”¥ëŸ¬ë‹ (DNN/CNN/RNN)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
