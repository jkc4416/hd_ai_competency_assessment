## < ì¸ì¦í‰ê°€ë¥¼ ì§„í–‰í•˜ê¸° ìœ„í•´ì„  ì•„ë˜ 2ê°€ì§€ ì„¸íŒ…ì´ í•„ìš”í•©ë‹ˆë‹¤. >


1. Google Colab ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ êµ¬ê¸€ ê³„ì •(ì—¬ë¶„ í•˜ë‚˜ ë” ìˆìœ¼ë©´ ì¢‹ìŠµë‹ˆë‹¤)
- Colabì—ì„œ GPUë¥¼ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ë•Œ, ëŒ€ëµ 2~3ì‹œê°„ì •ë„ í•œë²ˆì— ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

- ë¬´ë£Œ ê³„ì •ì˜ ëª…í™•í•œ ì‚¬ìš©ì œí•œì´ ì •í•´ì ¸ ìˆì§€ ì•Šì•„, ì–´ë–¤ ê³„ì •ì€ ì´ì „ ì‚¬ìš©ë¥ ì˜ ë”°ë¼ì„œ ì¼ì° ëŠê¸¸ ìˆ˜ ìˆì–´ ì—¬ë¶„ì˜ ê³„ì •ì´ ìˆìœ¼ë©´ ì¢‹ìŠµë‹ˆë‹¤.


2. Google Drive ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ êµ¬ê¸€ ê³„ì • (1ë²ˆê³¼ ê°™ì€ ê³„ì •)
- Google Driveì— ë°ì´í„°ë¥¼ ì˜¬ë ¤ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. (colabì— ì§ì ‘ ì—…ë¡œë“œí•˜ëŠ” ê²ƒê³¼ ë¹„êµì˜ˆì •)

## << ë¬¸ì œ ì •ì˜ >>

- ì£¼ì–´ì§„ ë°ì´í„°ë¡œ ê±´ë¬¼ë³„ ì‹œê°„ë‹¹ ì „ë ¥ì†Œë¹„ëŸ‰ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ í’‰ë‹ˆë‹¤.

- ì£¼ì–´ì§„ ë°ì´í„°ëŠ” ì´ 4ê°œì˜ csvì…ë‹ˆë‹¤. ê° csv íŒŒì¼ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ ê¸°ìˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- ê±´ë¬¼ë³„ ì‹œê°„ë‹¹ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ **RMSE(Root Mean Squared Error)**ê°’ì„ ì„±ëŠ¥ ì§€í‘œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

- í•´ë‹¹ ë¬¸ì œëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“œëŠ” ê³¼ì •ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ëŠ” ê²ƒì„ í‰ê°€í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì²˜ìŒì— ì•„ë˜ ë¬¸ì œë“¤ì„ ë¨¼ì € ì½ì–´ë³¸ ë’¤ ìœ„ì—ì„œë¶€í„° ì°¨ë¡€ëŒ€ë¡œ êµ¬í˜„í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.

- ë¬¸ì œ í’€ì´ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì¶”ê°€ ì½”ë“œë“¤ì€ ì–¼ë§ˆë“ ì§€ ì‘ì„±í•˜ì…”ë„ ë˜ë‚˜, ì´ ì½”ë“œ ìì²´ê°€ ì œì¶œ íŒŒì¼ì´ë¯€ë¡œ ì œì¶œì‹œ ë‹¤ë¥¸ ì¶”ê°€ ì½”ë“œë“¤ì€ ì‚­ì œí•˜ê³  ì œì¶œí•´ì£¼ì„¸ìš”.


**1. train.csv**

- í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” íŒŒì¼ë¡œ 2022ë…„ 6ì›” 1ì¼ 0ì‹œë¶€í„° 2022ë…„ 8ì›” 17ì¼ 23ì‹œê¹Œì§€ì˜ 100ê°œ ê±´ë¬¼ì— ëŒ€í•œ ì‹œê°„ë‹¹ ì „ë ¥ì†Œë¹„ëŸ‰ê³¼ ê´€ë ¨ëœ ì •ë³´ë“¤ì´ ê¸°ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- num_date_time, ê±´ë¬¼ë²ˆí˜¸, ì¼ì‹œ, ê¸°ì˜¨(C), ê°•ìˆ˜ëŸ‰(mm), í’ì†(m/s), ìŠµë„(%), ì „ë ¥ì†Œë¹„ëŸ‰(kWh)ìœ¼ë¡œ ì—´ì´ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- ê° columnë“¤ì˜ ì´ë¦„ì— ë‹¨ìœ„ê°€ í•¨ê»˜ ê¸°ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê±´ë¬¼ë²ˆí˜¸ì˜ ê²½ìš° 1 ~ 100ê¹Œì§€ 100ê°€ì§€ì˜ ì„œë¡œ ë‹¤ë¥¸ ê±´ë¬¼ ì •ë³´ë¥¼ ì˜ë¯¸í•˜ë©°, num_date_timeì€ "ê±´ë¬¼ë²ˆí˜¸_ì¼ì‹œ"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

- **ì „ë ¥ì†Œë¹„ëŸ‰(kWh)ê°€ target variable**ì— í•´ë‹¹í•©ë‹ˆë‹¤.


**2. test.csv**

- ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ëŠ” íŒŒì¼ë¡œ 2022ë…„ 8ì›” 18ì¼ 0ì‹œë¶€í„° 2022ë…„ 8ì›” 24ì¼ 23ì‹œê¹Œì§€ì˜ 100ê°œ ê±´ë¬¼ì— ëŒ€í•œ ì •ë³´ê°€ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.

- ***ì „ë ¥ì†Œë¹„ëŸ‰(kWh)***ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•´ë‹¹ columnê°’ì„ ë§ì¶°ì•¼í•©ë‹ˆë‹¤.


- num_date_time, ê±´ë¬¼ë²ˆí˜¸, ì¼ì‹œ, ê¸°ì˜¨(C), ê°•ìˆ˜ëŸ‰(mm), í’ì†(m/s), ìŠµë„(%)ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.


- ê° rowë§ˆë‹¤ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ í•´ë‹¹ ì‹œê°„, í•´ë‹¹ ê±´ë¬¼ì˜ ì „ë ¥ ì†Œë¹„ëŸ‰ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”.


**3. building_info.csv**

- ê° ê±´ë¬¼ë³„ ì •ë³´ê°€ ë“¤ì–´ìˆëŠ” íŒŒì¼ë¡œ, 100ê°œì˜ ê±´ë¬¼ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ê³¼ ê´€ë ¨ëœ ì •ë³´ë“¤ì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.


- ê±´ë¬¼ë²ˆí˜¸, ê±´ë¬¼ìœ í˜•, ì—°ë©´ì (m2), ëƒ‰ë°©ë©´ì (m2), íƒœì–‘ê´‘ìš©ëŸ‰(kW), ESSì €ì¥ìš©ëŸ‰(kWh), PCSìš©ëŸ‰(kW)ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.


- í•´ë‹¹ ë°ì´í„°ëŠ” í•„ìš”ì‹œ, train.csvë‚˜ test.csvì— í•©ì³ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



**4. sample_submission.csv**

- ì‹¤ì œ ì˜ˆì¸¡ê°’ì„ ê¸°ë¡í•˜ëŠ” íŒŒì¼ì…ë‹ˆë‹¤. ì±„ì ì— ì‚¬ìš©ë˜ë©°, sample_submissionì˜ answer columnì— ì˜ˆì¸¡ê°’ì„ ê¸°ë¡í•˜ë©´ ë©ë‹ˆë‹¤.


- num_date_timeì€ test.csvì˜ ê° rowì™€ ë§¤ì¹­ë©ë‹ˆë‹¤. ìˆœì„œ ë˜í•œ ê°™ê¸° ë•Œë¬¸ì—, ëª¨ë¸ì— test.csvë¥¼ ì˜ˆì¸¡í•œ ê°’ì„ answer columnì— ê¸°ë¡í•˜ë©´ ë©ë‹ˆë‹¤.


- ì‹¤ì œ ì„±ëŠ¥ì— ëŒ€í•œ ì±„ì  ê²°ê³¼ëŠ” ì´ íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. **(ë§ˆì§€ë§‰ì— í•´ë‹¹ íŒŒì¼ì— ëŒ€í•œ ìƒì„± ì½”ë“œëŠ” ì£¼ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.)**


```python
# from google.colab import drive
# drive.mount('/content/drive')
```

[ì˜µì…˜] ë°ì´í„° ì‹œê°í™”ë¥¼ ìœ„í•œ í•œê¸€ê¸€ê¼´ ì„¤ì¹˜ (1ë¶„ ì •ë„ ì†Œìš”)

- ì„¤ì¹˜ í›„ ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘ ë° ì¬ì‹¤í–‰ í•„ìš”


```python
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf
```

    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    fonts-nanum is already the newest version (20180306-3).
    0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.
    /usr/share/fonts: caching, new cache contents: 0 fonts, 2 dirs
    /usr/share/fonts/X11: caching, new cache contents: 0 fonts, 3 dirs
    /usr/share/fonts/X11/encodings: caching, new cache contents: 0 fonts, 1 dirs
    /usr/share/fonts/X11/encodings/large: caching, new cache contents: 0 fonts, 0 dirs
    /usr/share/fonts/X11/misc: caching, new cache contents: 89 fonts, 0 dirs
    /usr/share/fonts/X11/util: caching, new cache contents: 0 fonts, 0 dirs
    /usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 2 dirs
    /usr/share/fonts/truetype/dejavu: caching, new cache contents: 22 fonts, 0 dirs
    /usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs
    /usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs
    /root/.local/share/fonts: skipping, no such directory
    /root/.fonts: skipping, no such directory
    /usr/share/fonts/X11: skipping, looped directory detected
    /usr/share/fonts/truetype: skipping, looped directory detected
    /usr/share/fonts/X11/encodings: skipping, looped directory detected
    /usr/share/fonts/X11/misc: skipping, looped directory detected
    /usr/share/fonts/X11/util: skipping, looped directory detected
    /usr/share/fonts/truetype/dejavu: skipping, looped directory detected
    /usr/share/fonts/truetype/nanum: skipping, looped directory detected
    /usr/share/fonts/X11/encodings/large: skipping, looped directory detected
    /var/cache/fontconfig: cleaning cache directory
    /root/.cache/fontconfig: not cleaning non-existent cache directory
    /root/.fontconfig: not cleaning non-existent cache directory
    fc-cache: succeeded



```python
!pip install tslearn
```

    Requirement already satisfied: tslearn in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (0.7.0)
    Requirement already satisfied: scikit-learn>=1.4 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from tslearn) (1.7.2)
    Requirement already satisfied: numpy>=1.24.3 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from tslearn) (2.3.5)
    Requirement already satisfied: scipy>=1.10.1 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from tslearn) (1.16.3)
    Requirement already satisfied: numba>=0.58.1 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from tslearn) (0.62.1)
    Requirement already satisfied: joblib>=1.2 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from tslearn) (1.5.2)
    Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from numba>=0.58.1->tslearn) (0.45.1)
    Requirement already satisfied: threadpoolctl>=3.1.0 in /root/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from scikit-learn>=1.4->tslearn) (3.6.0)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
    [0m
    [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m24.0[0m[39;49m -> [0m[32;49m25.3[0m
    [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m



```python
# ë°ì´í„°ë¶„ì„ 4ì¢… ì„¸íŠ¸
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

# Colab ì˜ í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rc('font', family='NanumBarunGothic')
# ìœ ë‹ˆì½”ë“œì—ì„œ  ìŒìˆ˜ ë¶€í˜¸ì„¤ì •
mpl.rc('axes', unicode_minus=False)

import time
import os
import random

# ì‚¬ìš© ëª¨ë¸ (ì›í•˜ëŠ” ëª¨ë¸ ì¶”ê°€ ë° ë³€ê²½í•˜ì…”ë„ ë©ë‹ˆë‹¤)
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm.sklearn import LGBMRegressor
from tslearn.clustering import TimeSeriesKMeans, silhouette_score

# ì „ì²˜ë¦¬ê¸°
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# ì„±ëŠ¥ ì§€í‘œ
from sklearn.metrics import mean_squared_error

# ë°ì´í„° ë¶„í•  ë° hyper-param tuning
from sklearn.model_selection import train_test_split, GridSearchCV
```

    /home/claude-dev-kcj/project/hd/ai_competency_assessment/ai_competency_assessment/.venv/lib/python3.13/site-packages/tslearn/bases/bases.py:16: UserWarning: h5py not installed, hdf5 features will not be supported.
    Install h5py to use hdf5 features: http://docs.h5py.org/
      warn(h5py_msg)



```python
def seed_everything(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)

seed_everything(42) ## ë¬´ì¡°ê±´ 42ë¡œ ì„¸íŒ…!! ë°”ê¾¸ë©´ debuggingì´ í˜ë“­ë‹ˆë‹¤.
```

### Q1. í˜„ì¬ ê²½ë¡œì— ìœ„ì¹˜í•˜ê³  ìˆëŠ” train.csv, test.csv, building_info.csvë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , í•´ë‹¹ íŒŒì¼ë“¤ì˜ shapeì„ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.


```python
# Q1
# Q1. CSV íŒŒì¼ë“¤ì„ ë¶ˆëŸ¬ì˜¤ê³  shape ì¶œë ¥
# Q1. Load CSV files and print shapes

# ë°ì´í„° ê²½ë¡œ ì„¤ì • (Set data path)
DATA_PATH = "dataset/"

# CSV íŒŒì¼ ë¡œë“œ (Load CSV files)
train = pd.read_csv(f"{DATA_PATH}train.csv")
test = pd.read_csv(f"{DATA_PATH}test.csv")
building_info = pd.read_csv(f"{DATA_PATH}building_info.csv")

# ê° ë°ì´í„°í”„ë ˆì„ì˜ shape ì¶œë ¥ (Print shape of each dataframe)
print(f"train.csv shape: {train.shape}")
print(f"test.csv shape: {test.shape}")
print(f"building_info.csv shape: {building_info.shape}")
```

    train.csv shape: (187200, 8)
    test.csv shape: (16800, 7)
    building_info.csv shape: (100, 7)



```python
## optional - ì»¬ëŸ¼ëª… ì˜ë¬¸í™” (Rename columns to English)
train.columns = ['num_date_time', 'num', 'date_time', 'temperature', 'precipitation',
                 'windspeed', 'humidity', 'target']
test.columns = ['num_date_time', 'num', 'date_time', 'temperature', 'precipitation',
                 'windspeed', 'humidity']

# building_info ì»¬ëŸ¼ëª…ë„ ì˜ë¬¸í™” (Also rename building_info columns)
building_info.columns = ['num', 'building_type', 'total_area', 'cooling_area', 
                         'solar_capacity', 'ess_capacity', 'pcs_capacity']

print("ì»¬ëŸ¼ëª… ì˜ë¬¸í™” ì™„ë£Œ (Column renaming completed)")
print(f"train columns: {train.columns.tolist()}")
print(f"test columns: {test.columns.tolist()}")
print(f"building_info columns: {building_info.columns.tolist()}")
```

    ì»¬ëŸ¼ëª… ì˜ë¬¸í™” ì™„ë£Œ (Column renaming completed)
    train columns: ['num_date_time', 'num', 'date_time', 'temperature', 'precipitation', 'windspeed', 'humidity', 'target']
    test columns: ['num_date_time', 'num', 'date_time', 'temperature', 'precipitation', 'windspeed', 'humidity']
    building_info columns: ['num', 'building_type', 'total_area', 'cooling_area', 'solar_capacity', 'ess_capacity', 'pcs_capacity']


### Q2. train, test ë°ì´í„°ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•´ë‹¹ columnë³„ë¡œ ê²°ì¸¡ì¹˜ê°€ ëª‡ ê°œì”© ìˆëŠ”ì§€ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.


```python
# Q2
# Q2. train, test ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜ í™•ì¸ ë° columnë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ ì¶œë ¥
# Q2. Check missing values in train/test data and print count per column

print("=" * 50)
print("Train ë°ì´í„° ê²°ì¸¡ì¹˜ (Missing values in Train data):")
print("=" * 50)
print(train.isnull().sum())

print("\n" + "=" * 50)
print("Test ë°ì´í„° ê²°ì¸¡ì¹˜ (Missing values in Test data):")
print("=" * 50)
print(test.isnull().sum())
```

    ==================================================
    Train ë°ì´í„° ê²°ì¸¡ì¹˜ (Missing values in Train data):
    ==================================================
    num_date_time         0
    num                   0
    date_time             0
    temperature           0
    precipitation    145963
    windspeed            19
    humidity              9
    target                0
    dtype: int64
    
    ==================================================
    Test ë°ì´í„° ê²°ì¸¡ì¹˜ (Missing values in Test data):
    ==================================================
    num_date_time        0
    num                  0
    date_time            0
    temperature          0
    precipitation    14106
    windspeed            0
    humidity             0
    dtype: int64


### Q3. building_infoì— ìˆëŠ” '-'ì„ ëª¨ë‘ 0.0ìœ¼ë¡œ ë³€ê²½í•˜ê³  '-'ê°€ ìˆì—ˆë˜ columnì˜ dtypeì„ ëª¨ë‘ floatë¡œ ë°”ê¾¸ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.


```python
# Q3
# Q3. building_infoì˜ '-'ë¥¼ 0.0ìœ¼ë¡œ ë³€ê²½í•˜ê³  í•´ë‹¹ columnì˜ dtypeì„ floatë¡œ ë³€í™˜
# Q3. Replace '-' with 0.0 in building_info and convert dtype to float

# '-' ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ ì°¾ê¸° (Find columns with '-' values)
columns_with_dash = []
for col in building_info.columns:
    if building_info[col].dtype == "object":
        if (building_info[col] == "-").any():
            columns_with_dash.append(col)
            
print(f"'-' ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ (Columns with '-'): {columns_with_dash}")

# '-'ë¥¼ 0.0ìœ¼ë¡œ ë³€ê²½í•˜ê³  floatë¡œ ë³€í™˜ (Replace '-' with 0.0 and convert to float)
for col in columns_with_dash:
    building_info[col] = building_info[col].replace("-", 0.0).astype(float)

# ë³€í™˜ ê²°ê³¼ í™•ì¸ (Check conversion result)
print("\në³€í™˜ í›„ ë°ì´í„° íƒ€ì… (Data types after conversion):")
print(building_info.dtypes)
```

    '-' ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ (Columns with '-'): ['solar_capacity', 'ess_capacity', 'pcs_capacity']
    
    ë³€í™˜ í›„ ë°ì´í„° íƒ€ì… (Data types after conversion):
    num                 int64
    building_type      object
    total_area        float64
    cooling_area      float64
    solar_capacity    float64
    ess_capacity      float64
    pcs_capacity      float64
    dtype: object


### Q4. building_infoë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ, train, test ë°ì´í„°ì™€ í•©ì¹˜ë ¤ê³  í•©ë‹ˆë‹¤. ê±´ë¬¼ë²ˆí˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‘ ê°œì˜ DataFrameì„ mergeí•´ì£¼ì„¸ìš”. INNER JOIN ì—°ì‚°ì„ ì‚¬ìš©í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.


```python
# Q4
# Q4. building_infoë¥¼ train, test ë°ì´í„°ì™€ INNER JOINìœ¼ë¡œ ë³‘í•©
# Q4. Merge building_info with train/test using INNER JOIN

# train ë°ì´í„°ì™€ building_info ë³‘í•© (Merge train with building_info)
train = pd.merge(train, building_info, on="num", how="inner")

# test ë°ì´í„°ì™€ building_info ë³‘í•© (Merge test with building_info)
test = pd.merge(test, building_info, on="num", how="inner")

# ë³‘í•© ê²°ê³¼ í™•ì¸ (Check merge result)
print(f"ë³‘í•© í›„ train shape (Train shape after merge): {train.shape}")
print(f"ë³‘í•© í›„ test shape (Test shape after merge): {test.shape}")
print(f"\ntrain columns: {train.columns.tolist()}")
print(f"test columns: {test.columns.tolist()}")
```

    ë³‘í•© í›„ train shape (Train shape after merge): (187200, 14)
    ë³‘í•© í›„ test shape (Test shape after merge): (16800, 13)
    
    train columns: ['num_date_time', 'num', 'date_time', 'temperature', 'precipitation', 'windspeed', 'humidity', 'target', 'building_type', 'total_area', 'cooling_area', 'solar_capacity', 'ess_capacity', 'pcs_capacity']
    test columns: ['num_date_time', 'num', 'date_time', 'temperature', 'precipitation', 'windspeed', 'humidity', 'building_type', 'total_area', 'cooling_area', 'solar_capacity', 'ess_capacity', 'pcs_capacity']


### Q5. ì „ë ¥ì‚¬ìš©ëŸ‰ì€ ê±´ë¬¼ìœ í˜•ë³„ë¡œ ë‹¤ë¥¸ íŒ¨í„´ì„ ê°€ì§ˆ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. train ë°ì´í„°ì— ê±´ë¬¼ìœ í˜•ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ì˜ í‰ê· ê°’ì„ ê³„ì‚°í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.


```python
# Q5
# Q5. ê±´ë¬¼ìœ í˜•ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ í‰ê· ê°’ ê³„ì‚°
# Q5. Calculate mean power consumption by building type

# ê±´ë¬¼ìœ í˜• ì»¬ëŸ¼ëª… í™•ì¸ (Check building type column name)
building_type_col = "building_type" if "building_type" in train.columns else "ê±´ë¬¼ìœ í˜•"
target_col = "target" if "target" in train.columns else "ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"

# ê±´ë¬¼ìœ í˜•ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ í‰ê·  ê³„ì‚° (Calculate mean power by building type)
mean_power_by_type = train.groupby(building_type_col)[target_col].mean()

print("ê±´ë¬¼ìœ í˜•ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ í‰ê·  (Mean power consumption by building type):")
print("=" * 60)
print(mean_power_by_type.sort_values(ascending=False))
```

    ê±´ë¬¼ìœ í˜•ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ í‰ê·  (Mean power consumption by building type):
    ============================================================
    building_type
    ëŒ€í•™êµ        5448.553112
    ë°ì´í„°ì„¼í„°      5394.410769
    ë³‘ì›         2903.033558
    ì—°êµ¬ì†Œ        2796.301564
    ìƒìš©         2293.394483
    ë°±í™”ì ë°ì•„ìš¸ë ›    2256.816266
    ê±´ë¬¼ê¸°íƒ€       1967.538615
    ì§€ì‹ì‚°ì—…ì„¼í„°     1925.575172
    ê³µê³µ         1676.714868
    í˜¸í…”ë°ë¦¬ì¡°íŠ¸     1473.405397
    í• ì¸ë§ˆíŠ¸       1408.020505
    ì•„íŒŒíŠ¸        1306.512626
    Name: target, dtype: float64


### Q6. ê±´ë¬¼ìœ í˜•ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ì„ ì‹œê°„ì— ë”°ë¥¸ ì„ ê·¸ë˜í”„ë¡œ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
(figureëŠ” í•˜ë‚˜ì— ê·¸ë ¤ë„ ë˜ê³ , ì—¬ëŸ¬ê°œì˜ subplotìœ¼ë¡œ ê·¸ë ¤ë„ ë©ë‹ˆë‹¤. ê·¸ë˜í”„ì˜ ìƒ‰ìƒ ì—­ì‹œ ë§ˆìŒëŒ€ë¡œ í•˜ì…”ë„ ë˜ì§€ë§Œ, í•˜ë‚˜ì˜ figureì˜ ê·¸ë¦°ë‹¤ë©´ ê° plotì„ ì„œë¡œ ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ ì§€ì •í•´ì£¼ì„¸ìš”)


```python
# Q6
# Q6. ê±´ë¬¼ìœ í˜•ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ì„ ì‹œê°„ì— ë”°ë¥¸ ì„ ê·¸ë˜í”„ë¡œ ì¶œë ¥
# Q6. Plot power consumption by building type over time

# ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜ (Convert date column)
date_col = "date_time" if "date_time" in train.columns else "ì¼ì‹œ"
train[date_col] = pd.to_datetime(train[date_col])

# ê±´ë¬¼ìœ í˜•ë³„ ì‹œê°„ì— ë”°ë¥¸ ì „ë ¥ì‚¬ìš©ëŸ‰ í‰ê·  ê³„ì‚°
# Calculate mean power consumption over time by building type
power_by_type_time = train.groupby([date_col, building_type_col])[target_col].mean().reset_index()

# ê±´ë¬¼ìœ í˜• ëª©ë¡ (List of building types)
building_types = power_by_type_time[building_type_col].unique()

# ì‹œê°í™” (Visualization)
fig, ax = plt.subplots(figsize=(16, 8))

# ê° ê±´ë¬¼ìœ í˜•ë³„ë¡œ ì„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (Draw line plot for each building type)
colors = plt.cm.tab20(np.linspace(0, 1, len(building_types)))
for i, btype in enumerate(building_types):
    data = power_by_type_time[power_by_type_time[building_type_col] == btype]
    ax.plot(data[date_col], data[target_col], label=btype, color=colors[i], alpha=0.8)

ax.set_xlabel("ì‹œê°„ (Time)", fontsize=12)
ax.set_ylabel("ì „ë ¥ì†Œë¹„ëŸ‰ (kWh) (Power Consumption)", fontsize=12)
ax.set_title("ê±´ë¬¼ìœ í˜•ë³„ ì‹œê°„ì— ë”°ë¥¸ ì „ë ¥ì‚¬ìš©ëŸ‰ (Power Consumption by Building Type over Time)", fontsize=14)
ax.legend(loc="upper right", fontsize=8, ncol=2)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```


    
![png](ai_competency_assessment_past_exam_2023_solutions_files/ai_competency_assessment_past_exam_2023_solutions_20_0.png)
    


### Q7. ë¹„ê°€ ì•ˆ ì™€ì„œ ì¸¡ì •ì´ ì•ˆëœ ê²ƒìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ê°•ìˆ˜ëŸ‰ì€ 0ìœ¼ë¡œ ì±„ìš°ê³ , í’ì†, ìŠµë„ëŠ” linear interpolation methodë¥¼ ì´ìš©í•˜ì—¬ ì±„ì›Œì£¼ì„¸ìš”.


```python
# Q7
# Q7. ê°•ìˆ˜ëŸ‰ì€ 0ìœ¼ë¡œ ì±„ìš°ê³ , í’ì†/ìŠµë„ëŠ” linear interpolationìœ¼ë¡œ ì±„ì›€
# Q7. Fill precipitation with 0, interpolate wind speed and humidity

# ì»¬ëŸ¼ëª… ì„¤ì • (Set column names)
precip_col = "precipitation" if "precipitation" in train.columns else "ê°•ìˆ˜ëŸ‰(mm)"
wind_col = "windspeed" if "windspeed" in train.columns else "í’ì†(m/s)"
humid_col = "humidity" if "humidity" in train.columns else "ìŠµë„(%)"

# Train ë°ì´í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Handle missing values in train data)
# ê°•ìˆ˜ëŸ‰: 0ìœ¼ë¡œ ì±„ì›€ (Precipitation: fill with 0)
train[precip_col] = train[precip_col].fillna(0)
# í’ì†, ìŠµë„: linear interpolation (Wind speed, humidity: linear interpolation)
train[wind_col] = train[wind_col].interpolate(method="linear")
train[humid_col] = train[humid_col].interpolate(method="linear")

# Test ë°ì´í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Handle missing values in test data)
test[precip_col] = test[precip_col].fillna(0)
test[wind_col] = test[wind_col].interpolate(method="linear")
test[humid_col] = test[humid_col].interpolate(method="linear")

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ê²°ê³¼ í™•ì¸ (Check result)
print("Train ê²°ì¸¡ì¹˜ í™•ì¸ (Check train missing values):")
print(train[[precip_col, wind_col, humid_col]].isnull().sum())
print("\nTest ê²°ì¸¡ì¹˜ í™•ì¸ (Check test missing values):")
print(test[[precip_col, wind_col, humid_col]].isnull().sum())
```

    Train ê²°ì¸¡ì¹˜ í™•ì¸ (Check train missing values):
    precipitation    0
    windspeed        0
    humidity         0
    dtype: int64
    
    Test ê²°ì¸¡ì¹˜ í™•ì¸ (Check test missing values):
    precipitation    0
    windspeed        0
    humidity         0
    dtype: int64


### Q8. ê±´ë¬¼ ìœ í˜• ì •ë³´ëŠ” ëª¨ë¸ì— í° ì˜í–¥ì„ ì¤„ ê²ƒìœ¼ë¡œ ìƒê°ì´ ë©ë‹ˆë‹¤. train, test ë°ì´í„°ì˜ ê±´ë¬¼ìœ í˜• ì •ë³´ë¥¼ One-Hot Encodingí•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.


```python
# Q8
# Q8. ê±´ë¬¼ìœ í˜• ì •ë³´ë¥¼ One-Hot Encoding
# Q8. One-Hot Encode building type information

# One-Hot Encoding ìˆ˜í–‰ (Perform One-Hot Encoding)
train = pd.get_dummies(train, columns=[building_type_col], prefix="building_type")
test = pd.get_dummies(test, columns=[building_type_col], prefix="building_type")

# One-Hot Encoding ê²°ê³¼ í™•ì¸ (Check One-Hot Encoding result)
print(f"Train shape after One-Hot Encoding: {train.shape}")
print(f"Test shape after One-Hot Encoding: {test.shape}")

# ìƒˆë¡œ ìƒì„±ëœ ê±´ë¬¼ìœ í˜• ì»¬ëŸ¼ í™•ì¸ (Check newly created building type columns)
building_type_cols = [col for col in train.columns if col.startswith("building_type_")]
print(f"\nìƒì„±ëœ ê±´ë¬¼ìœ í˜• ì»¬ëŸ¼ ìˆ˜ (Number of building type columns): {len(building_type_cols)}")
print(f"ê±´ë¬¼ìœ í˜• ì»¬ëŸ¼ (Building type columns): {building_type_cols}")
```

    Train shape after One-Hot Encoding: (187200, 25)
    Test shape after One-Hot Encoding: (16800, 24)
    
    ìƒì„±ëœ ê±´ë¬¼ìœ í˜• ì»¬ëŸ¼ ìˆ˜ (Number of building type columns): 12
    ê±´ë¬¼ìœ í˜• ì»¬ëŸ¼ (Building type columns): ['building_type_ê±´ë¬¼ê¸°íƒ€', 'building_type_ê³µê³µ', 'building_type_ëŒ€í•™êµ', 'building_type_ë°ì´í„°ì„¼í„°', 'building_type_ë°±í™”ì ë°ì•„ìš¸ë ›', 'building_type_ë³‘ì›', 'building_type_ìƒìš©', 'building_type_ì•„íŒŒíŠ¸', 'building_type_ì—°êµ¬ì†Œ', 'building_type_ì§€ì‹ì‚°ì—…ì„¼í„°', 'building_type_í• ì¸ë§ˆíŠ¸', 'building_type_í˜¸í…”ë°ë¦¬ì¡°íŠ¸']


### Q9. ëª…ì‹œì ìœ¼ë¡œ ë‚˜ì™€ìˆëŠ” ê±´ë¬¼íƒ€ì…ë§ê³ , ì‹¤ì œ ì‹œê°„ì— ë”°ë¥¸ ì „ë ¥ì‚¬ìš©ëŸ‰ íŒ¨í„´ì´ ë¹„ìŠ·í•œ ê±´ë¬¼ë“¤ë¼ë¦¬ ê°™ì€ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ì£¼ëŠ” ë°©ì‹ì„ featureë¡œ ì‚¬ìš©í•˜ë ¤ê³  í•©ë‹ˆë‹¤. TimeSeriesKMeansë¥¼ í™œìš©í•˜ì—¬ train ë°ì´í„°ì…‹ì˜ ì‹œê°„ë‹¹ ì „ë ¥ì‚¬ìš©ëŸ‰ì´ ë¹„ìŠ·í•œ ê±´ë¬¼ ê·¸ë£¹ì„ ì°¾ì•„ì„œ "cluster" columnìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ë‹¨, ì•„ë˜ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•´ì•¼í•©ë‹ˆë‹¤.


> ì¡°ê±´1 : TimeSerisKMeansì˜ êµ¬í˜„ì²´ì—ì„œ metricì€ euclideanì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (dtwê°€ ì„±ëŠ¥ì€ ë” ì¢‹ì„ìˆ˜ ìˆìœ¼ë‚˜, ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¼)

> ì¡°ê±´2 : tslearn.clustering.silhouette_scoreë¥¼ í™œìš©í•˜ì—¬, ìµœì ì˜ Kë¥¼ ì°¾ìŠµë‹ˆë‹¤. íƒìƒ‰í•˜ëŠ” Kì˜ ë²”ìœ„ëŠ” 2ë¶€í„° 10ê¹Œì§€ì…ë‹ˆë‹¤. (ìœ„ì—ì„œ random seedê°€ ê³ ì •ë˜ì–´ ìˆì–´, ì½”ë“œë‚´ì—ì„œëŠ” deterministicí•˜ê²Œ ê²°ì •ë©ë‹ˆë‹¤)

> ì¡°ê±´3 : í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” timestepì€ ì‹œê°„ë‹¹ìœ¼ë¡œ ì •í•©ë‹ˆë‹¤. ì¦‰, ê±´ë¬¼ë³„ë¡œ 78ì¼ x 24ì‹œê°„ = 1872ì‹œê°„ì— ëŒ€í•´ì„œ TimeSeriesKMeansë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì¦‰, ìµœì¢… í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” feature matrixì˜ shapeì€ (100, 1872)ê°€ ë©ë‹ˆë‹¤.

> ì¡°ê±´4 : í´ëŸ¬ìŠ¤í„°ë§ì´ ì œëŒ€ë¡œ ìˆ˜í–‰ë˜ê¸° ìœ„í•´ì„œ ì£¼ì–´ì§„ ì „ë ¥ì‚¬ìš©ëŸ‰ì„ MinMaxScalerë¡œ ì²˜ë¦¬í•˜ì—¬, ì‹œê°„ì— ë”°ë¥¸ ê²½í–¥ì„±ë§Œ í•™ìŠµì‹œí‚¤ë„ë¡ í•©ë‹ˆë‹¤.


```python
# Q9
# Q9. TimeSeriesKMeansë¥¼ í™œìš©í•˜ì—¬ ì‹œê°„ë‹¹ ì „ë ¥ì‚¬ìš©ëŸ‰ì´ ë¹„ìŠ·í•œ ê±´ë¬¼ ê·¸ë£¹ ì°¾ê¸°
# Q9. Use TimeSeriesKMeans to find building groups with similar hourly power consumption

# ê±´ë¬¼ë²ˆí˜¸ ì»¬ëŸ¼ëª… í™•ì¸ (Check building number column name)
num_col = "num" if "num" in train.columns else "ê±´ë¬¼ë²ˆí˜¸"

# ê±´ë¬¼ë³„ ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (78ì¼ x 24ì‹œê°„ = 1872ì‹œê°„)
# Create time series data per building (78 days x 24 hours = 1872 hours)
building_power_matrix = train.pivot_table(
    index=num_col, 
    columns=date_col, 
    values=target_col,
    aggfunc="mean"
).values

print(f"ê±´ë¬¼ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ í–‰ë ¬ shape (Building power matrix shape): {building_power_matrix.shape}")

# MinMaxScalerë¡œ ì •ê·œí™”í•˜ì—¬ ê²½í–¥ì„±ë§Œ í•™ìŠµ (Normalize with MinMaxScaler to learn trends only)
scaler = MinMaxScaler()
building_power_scaled = scaler.fit_transform(building_power_matrix.T).T

# ìµœì ì˜ K ì°¾ê¸° (K ë²”ìœ„: 2~10) (Find optimal K in range 2-10)
best_k = 2
best_score = -1
silhouette_scores = []

print("\nKë³„ Silhouette Score ê³„ì‚° ì¤‘... (Calculating Silhouette Scores for each K...)")
for k in range(2, 11):
    # TimeSeriesKMeans í•™ìŠµ (Train TimeSeriesKMeans)
    km = TimeSeriesKMeans(n_clusters=k, metric="euclidean", random_state=42)
    labels = km.fit_predict(building_power_scaled)
    
    # Silhouette Score ê³„ì‚° (Calculate Silhouette Score)
    score = silhouette_score(building_power_scaled, labels, metric="euclidean")
    silhouette_scores.append(score)
    
    print(f"K={k}: Silhouette Score = {score:.4f}")
    
    if score > best_score:
        best_score = score
        best_k = k

print(f"\nìµœì ì˜ K (Optimal K): {best_k}, Silhouette Score: {best_score:.4f}")

# ìµœì ì˜ Kë¡œ ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (Perform final clustering with optimal K)
final_km = TimeSeriesKMeans(n_clusters=best_k, metric="euclidean", random_state=42)
final_labels = final_km.fit_predict(building_power_scaled)

# í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ìƒì„± (Create DataFrame with cluster results)
cluster_df = pd.DataFrame({num_col: range(1, 101), "cluster": final_labels})

# train, test ë°ì´í„°ì— cluster ì»¬ëŸ¼ ì¶”ê°€ (Add cluster column to train, test data)
train = pd.merge(train, cluster_df, on=num_col, how="left")
test = pd.merge(test, cluster_df, on=num_col, how="left")

print(f"\ní´ëŸ¬ìŠ¤í„°ë³„ ê±´ë¬¼ ìˆ˜ (Number of buildings per cluster):")
print(cluster_df["cluster"].value_counts().sort_index())
```

    ê±´ë¬¼ë³„ ì „ë ¥ì‚¬ìš©ëŸ‰ í–‰ë ¬ shape (Building power matrix shape): (100, 1872)
    
    Kë³„ Silhouette Score ê³„ì‚° ì¤‘... (Calculating Silhouette Scores for each K...)
    K=2: Silhouette Score = 0.2061
    K=3: Silhouette Score = 0.2682
    K=4: Silhouette Score = 0.3268
    K=5: Silhouette Score = 0.2669
    K=6: Silhouette Score = 0.2224
    K=7: Silhouette Score = 0.2049
    K=8: Silhouette Score = 0.2040
    K=9: Silhouette Score = 0.2023
    K=10: Silhouette Score = 0.2221
    
    ìµœì ì˜ K (Optimal K): 4, Silhouette Score: 0.3268
    
    í´ëŸ¬ìŠ¤í„°ë³„ ê±´ë¬¼ ìˆ˜ (Number of buildings per cluster):
    cluster
    0    32
    1    41
    2    16
    3    11
    Name: count, dtype: int64


### Q10. train, testì—ì„œ í•™ìŠµì— í•„ìš”ì—†ëŠ” ëª‡ ê°œì˜ columnì„ ì œì™¸í•˜ê³  í•™ìŠµì— ì‚¬ìš©í•  X(feature vector)ì™€ y(target value)ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ê·¸ ë‹¤ìŒ X, yë¥¼ ì‚¬ìš©í•˜ì—¬ train ë°ì´í„°ë¥¼ 8:2ë¡œ ë¶„í• í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

> ì´ ë•Œ 8ì€ trainìœ¼ë¡œ 2ëŠ” validationìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¶„í•  ì „ random shuffleì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

> ì œê±°í•  columnì€ num_date_time, num, date_timeì…ë‹ˆë‹¤.


```python
# Q10
# Q10. í•™ìŠµì— í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì œì™¸ í›„ X, y ìƒì„± ë° 8:2 ë¶„í• 
# Q10. Remove unnecessary columns, create X, y and split 80/20

# ì œê±°í•  ì»¬ëŸ¼ ì •ì˜ (Define columns to remove)
drop_cols = ["num_date_time", num_col, date_col]

# X (feature vector) ìƒì„± (Create X - feature vector)
X = train.drop(columns=drop_cols + [target_col], errors="ignore")

# y (target value) ìƒì„± (Create y - target value)
y = train[target_col]

print(f"X shape: {X.shape}")
print(f"y shape: {y.shape}")
print(f"\nFeature columns: {X.columns.tolist()}")

# Train/Validation 8:2 ë¶„í•  (random shuffle í¬í•¨)
# Split Train/Validation 80:20 (with random shuffle)
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"\nX_train shape: {X_train.shape}")
print(f"X_valid shape: {X_valid.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_valid shape: {y_valid.shape}")
```

    X shape: (187200, 22)
    y shape: (187200,)
    
    Feature columns: ['temperature', 'precipitation', 'windspeed', 'humidity', 'total_area', 'cooling_area', 'solar_capacity', 'ess_capacity', 'pcs_capacity', 'building_type_ê±´ë¬¼ê¸°íƒ€', 'building_type_ê³µê³µ', 'building_type_ëŒ€í•™êµ', 'building_type_ë°ì´í„°ì„¼í„°', 'building_type_ë°±í™”ì ë°ì•„ìš¸ë ›', 'building_type_ë³‘ì›', 'building_type_ìƒìš©', 'building_type_ì•„íŒŒíŠ¸', 'building_type_ì—°êµ¬ì†Œ', 'building_type_ì§€ì‹ì‚°ì—…ì„¼í„°', 'building_type_í• ì¸ë§ˆíŠ¸', 'building_type_í˜¸í…”ë°ë¦¬ì¡°íŠ¸', 'cluster']
    
    X_train shape: (149760, 22)
    X_valid shape: (37440, 22)
    y_train shape: (149760,)
    y_valid shape: (37440,)


### Q11. XGBoostë¥¼ ì´ìš©í•˜ì—¬ ìœ„ì—ì„œ ë§Œë“  ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê³ , í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ì˜ ì˜ˆì¸¡ê°’ì„ ë§Œë“œëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”. hyper-parameterëŠ” max_depth=10, n_estimators=200, learning_rate=0.1, colsample_bynode=0.5ë¡œ ì„¸íŒ…í•´ì£¼ì„¸ìš”. (ì´í›„ì— Grid Searchë¡œ tuning ì˜ˆì •)


```python
# Q11
# Q11. XGBoost í•™ìŠµ ë° ì˜ˆì¸¡
# Q11. Train XGBoost and make predictions

# XGBoost ëª¨ë¸ ìƒì„± (ì§€ì •ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©)
# Create XGBoost model (with specified hyperparameters)
xgb_model = XGBRegressor(
    max_depth=10,
    n_estimators=200,
    learning_rate=0.1,
    colsample_bynode=0.5,
    random_state=42,
    n_jobs=-1
)

# ëª¨ë¸ í•™ìŠµ (Train model)
print("XGBoost ëª¨ë¸ í•™ìŠµ ì¤‘... (Training XGBoost model...)")
xgb_model.fit(X_train, y_train)
print("í•™ìŠµ ì™„ë£Œ! (Training completed!)")

# í•™ìŠµ ë°ì´í„° ì˜ˆì¸¡ (Predict on training data)
y_train_pred = xgb_model.predict(X_train)

# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ (Predict on validation data)
y_valid_pred = xgb_model.predict(X_valid)

print(f"\ní•™ìŠµ ë°ì´í„° ì˜ˆì¸¡ê°’ shape (Train prediction shape): {y_train_pred.shape}")
print(f"ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ê°’ shape (Validation prediction shape): {y_valid_pred.shape}")
```

    XGBoost ëª¨ë¸ í•™ìŠµ ì¤‘... (Training XGBoost model...)
    í•™ìŠµ ì™„ë£Œ! (Training completed!)
    
    í•™ìŠµ ë°ì´í„° ì˜ˆì¸¡ê°’ shape (Train prediction shape): (149760,)
    ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ê°’ shape (Validation prediction shape): (37440,)


### Q12. Q11ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ í‰ê°€í•˜ë ¤ê³  í•©ë‹ˆë‹¤. train ë°ì´í„°ì˜ target valueì˜ í‰ê· ê°’ìœ¼ë¡œ ì˜ˆì¸¡í–ˆì„ ë•Œì˜ RMSE(baseline)ê³¼ í•™ìŠµí•œ ëª¨ë¸ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•œ ì„±ëŠ¥(P_train), í•™ìŠµí•œ ëª¨ë¸ë¡œ ê²€ì¦ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•œ ì„±ëŠ¥(P_valid)ë¥¼ ëª¨ë‘ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.


```python
# Q12
# Q12. ëª¨ë¸ í‰ê°€ - Baseline RMSE, P_train RMSE, P_valid RMSE ì¶œë ¥
# Q12. Model evaluation - Print Baseline RMSE, P_train RMSE, P_valid RMSE

# Baseline: train ë°ì´í„°ì˜ target í‰ê· ê°’ìœ¼ë¡œ ì˜ˆì¸¡
# Baseline: Predict with mean of train target values
baseline_pred = np.full(len(y_train), y_train.mean())
baseline_rmse = np.sqrt(mean_squared_error(y_train, baseline_pred))

# P_train: í•™ìŠµí•œ ëª¨ë¸ë¡œ í•™ìŠµ ë°ì´í„° ì˜ˆì¸¡ ì„±ëŠ¥
# P_train: Prediction performance on training data
p_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))

# P_valid: í•™ìŠµí•œ ëª¨ë¸ë¡œ ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ì„±ëŠ¥
# P_valid: Prediction performance on validation data
p_valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))

print("=" * 60)
print("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (Model Performance Evaluation)")
print("=" * 60)
print(f"Baseline RMSE (í‰ê· ê°’ ì˜ˆì¸¡): {baseline_rmse:.4f}")
print(f"P_train RMSE (í•™ìŠµ ë°ì´í„°): {p_train_rmse:.4f}")
print(f"P_valid RMSE (ê²€ì¦ ë°ì´í„°): {p_valid_rmse:.4f}")
print("=" * 60)
```

    ============================================================
    ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (Model Performance Evaluation)
    ============================================================
    Baseline RMSE (í‰ê· ê°’ ì˜ˆì¸¡): 2442.0425
    P_train RMSE (í•™ìŠµ ë°ì´í„°): 531.8370
    P_valid RMSE (ê²€ì¦ ë°ì´í„°): 686.4979
    ============================================================


### Q13. GridSearchCVë¥¼ ì‚¬ìš©í•˜ì—¬, XGBoostì˜ hyper-parameterë¥¼ tuningí•˜ê³ , best_estimatorë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”. XGBoostì˜ ê³µì‹ ê°€ì´ë“œë¼ì¸ì„ ë³´ë©´ì„œ, ì‹œê°„ë‚´ì— ì—¬ëŸ¬ê°€ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì˜ tuningí•´ì„œ P_validì˜ RMSEê°€ 500 ì´í•˜ê°€ ë˜ë„ë¡ í•˜ì„¸ìš”.


Reference : https://xgboost.readthedocs.io/en/stable/parameter.html#


```python
# Q13
# Q13. GridSearchCVë¥¼ ì‚¬ìš©í•˜ì—¬ XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# Q13. Use GridSearchCV for XGBoost hyperparameter tuning

# íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ (Define hyperparameters to search)
param_grid = {
    "max_depth": [6, 8, 10],
    # "n_estimators": [200, 300, 400],
    # "learning_rate": [0.05, 0.1, 0.15],
    # "colsample_bytree": [0.7, 0.8, 0.9],
    # "subsample": [0.8, 0.9, 1.0],
}

# XGBoost ê¸°ë³¸ ëª¨ë¸ (Base XGBoost model)
xgb_base = XGBRegressor(random_state=42, n_jobs=-1)

# GridSearchCV ì„¤ì • (Configure GridSearchCV)
# cv=3ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì‹œê°„ ë‹¨ì¶• (Set cv=3 to reduce time)
grid_search = GridSearchCV(
    estimator=xgb_base,
    param_grid=param_grid,
    cv=3,
    scoring="neg_root_mean_squared_error",
    verbose=1,
    n_jobs=-1
)

# GridSearchCV ìˆ˜í–‰ (Perform GridSearchCV)
print("GridSearchCV ìˆ˜í–‰ ì¤‘... (Running GridSearchCV...)")
print("(ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ / This may take some time)")
grid_search.fit(X_train, y_train)

# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥ (Print best hyperparameters)
print("\n" + "=" * 60)
print("ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° (Best Hyperparameters):")
print("=" * 60)
for param, value in grid_search.best_params_.items():
    print(f"  {param}: {value}")

# Best estimatorë¡œ ì˜ˆì¸¡ (Predict with best estimator)
best_model = grid_search.best_estimator_

y_train_pred_best = best_model.predict(X_train)
y_valid_pred_best = best_model.predict(X_valid)

# ì„±ëŠ¥ ì¸¡ì • (Measure performance)
p_train_rmse_best = np.sqrt(mean_squared_error(y_train, y_train_pred_best))
p_valid_rmse_best = np.sqrt(mean_squared_error(y_valid, y_valid_pred_best))

print("\n" + "=" * 60)
print("GridSearchCV í›„ ì„±ëŠ¥ (Performance after GridSearchCV)")
print("=" * 60)
print(f"P_train RMSE: {p_train_rmse_best:.4f}")
print(f"P_valid RMSE: {p_valid_rmse_best:.4f}")
print("=" * 60)
```

    GridSearchCV ìˆ˜í–‰ ì¤‘... (Running GridSearchCV...)
    (ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ / This may take some time)
    Fitting 3 folds for each of 3 candidates, totalling 9 fits


    
    ============================================================
    ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° (Best Hyperparameters):
    ============================================================
      max_depth: 10
    
    ============================================================
    GridSearchCV í›„ ì„±ëŠ¥ (Performance after GridSearchCV)
    ============================================================
    P_train RMSE: 433.4877
    P_valid RMSE: 679.5075
    ============================================================


### Q14. ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ test ë°ì´í„°ë¥¼ GridSearchCVë¡œ í•™ìŠµí•œ best_estimatorë¡œ ì˜ˆì¸¡í•´ë³´ì„¸ìš”. ì˜ˆì¸¡ê°’ì„ sample_submissionì˜ answer columnì— ì±„ìš°ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.


```python
# Q14
# Q14. Test ë°ì´í„°ë¥¼ best_estimatorë¡œ ì˜ˆì¸¡í•˜ê³  sample_submissionì˜ answerì— ì±„ì›€
# Q14. Predict test data with best_estimator and fill sample_submission answer column

# sample_submission ë¡œë“œ (Load sample_submission)
submission = pd.read_csv(f"{DATA_PATH}sample_submission.csv")
print(f"sample_submission shape: {submission.shape}")
print(f"sample_submission columns: {submission.columns.tolist()}")

# Test ë°ì´í„° ì „ì²˜ë¦¬ (Preprocess test data)
X_test = test.drop(columns=drop_cols, errors="ignore")

# trainê³¼ testì˜ ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸° (Align columns between train and test)
# trainì—ëŠ” ìˆì§€ë§Œ testì—ëŠ” ì—†ëŠ” ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ì›€
# Columns in train but not in test are filled with 0
missing_cols = set(X.columns) - set(X_test.columns)
for col in missing_cols:
    X_test[col] = 0

# testì—ëŠ” ìˆì§€ë§Œ trainì—ëŠ” ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
# Remove columns in test but not in train
extra_cols = set(X_test.columns) - set(X.columns)
X_test = X_test.drop(columns=list(extra_cols), errors="ignore")

# ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸° (Align column order)
X_test = X_test[X.columns]

print(f"X_test shape: {X_test.shape}")

# Best modelë¡œ test ë°ì´í„° ì˜ˆì¸¡ (Predict test data with best model)
test_predictions = best_model.predict(X_test)

# submissionì˜ answer ì»¬ëŸ¼ì— ì˜ˆì¸¡ê°’ ì±„ìš°ê¸° (Fill predictions in answer column)
submission["answer"] = test_predictions

print(f"\nì˜ˆì¸¡ ì™„ë£Œ! (Prediction completed!)")
print(f"ì˜ˆì¸¡ê°’ shape (Prediction shape): {test_predictions.shape}")
print(f"ì˜ˆì¸¡ê°’ ìƒ˜í”Œ (Sample predictions):")
print(submission.head(10))
```

    sample_submission shape: (16800, 2)
    sample_submission columns: ['num_date_time', 'answer']
    X_test shape: (16800, 22)
    
    ì˜ˆì¸¡ ì™„ë£Œ! (Prediction completed!)
    ì˜ˆì¸¡ê°’ shape (Prediction shape): (16800,)
    ì˜ˆì¸¡ê°’ ìƒ˜í”Œ (Sample predictions):
       num_date_time       answer
    0  1_20220818 00  2434.541504
    1  1_20220818 01  2127.760010
    2  1_20220818 02  1987.792969
    3  1_20220818 03  2092.826416
    4  1_20220818 04  2132.575928
    5  1_20220818 05  1680.582642
    6  1_20220818 06  2031.087646
    7  1_20220818 07  1991.968750
    8  1_20220818 08  2412.971191
    9  1_20220818 09  2578.061279


### Q15. ë‹¨ìˆœ íšŒê·€ëª¨ë¸ì´ ì•„ë‹Œ ì‹œê³„ì—´ ì˜ˆì¸¡ë¬¸ì œë¡œ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ í‰ê°€ë¥¼ í•˜ê¸° ìœ„í•´ LSTMì„ êµ¬í˜„í•˜ë ¤ê³  í•©ë‹ˆë‹¤. í•™ìŠµì— ì‚¬ìš©í•  TimeSeries Datasetì„ êµ¬ì¶•í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ë‹¨, ì•„ë˜ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•´ì•¼í•©ë‹ˆë‹¤.

> ì¡°ê±´1 : ì‹œí—˜ íŠ¹ì„±ìƒ, **1ë²ˆ ê±´ë¬¼**ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.


> ì¡°ê±´2 : LSTMì˜ input vectorëŠ” X_trainì—ì„œ 1ë²ˆ ê±´ë¬¼ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ Standard Scalingì„ ì ìš©í•œ ìƒíƒœë¡œ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤. (22ì°¨ì› feature vector)

> ì¡°ê±´3 : í•œë²ˆì— í•™ìŠµí•˜ëŠ” ì‹œê°„ ë‹¨ìœ„ëŠ” ì‹œê°„ë‹¹ì´ë©°, í•œ ë²ˆì— 10steps(10ì‹œê°„ ê°„ê²©)ì”© í•™ìŠµí•©ë‹ˆë‹¤. ì¦‰, hidden_stateê°€ 10ê°œì…ë‹ˆë‹¤.

> ì¡°ê±´4 : ì‹œê³„ì—´ ë°ì´í„°ì—¬ì•¼í•˜ë¯€ë¡œ, ì£¼ì–´ì§„ ë°ì´í„°ëŠ” ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ìˆì–´ì•¼ í•©ë‹ˆë‹¤. train dataëŠ”  2022ë…„ 6ì›” 1ì¼ 0ì‹œë¶€í„° 2022ë…„ 8ì›” 10ì¼ 23ì‹œ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ê³ , validationì˜ ëŒ€ìƒì€ 2022ë…„ 8ì›” 11ì¼ 0ì‹œë¶€í„° 2022ë…„ 8ì›” 16ì¼ 23ì‹œê¹Œì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.



```python
# tensorflowë‚˜ pytorchì—ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
# Load required libraries from TensorFlow or PyTorch

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (Check if GPU is available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤ (Device): {device}")
```

    ì‚¬ìš© ë””ë°”ì´ìŠ¤ (Device): cuda



```python
# Q15
# Q15. LSTM í•™ìŠµì„ ìœ„í•œ TimeSeries Dataset êµ¬ì¶•
# Q15. Build TimeSeries Dataset for LSTM training

# ì¡°ê±´1: 1ë²ˆ ê±´ë¬¼ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡ (Condition 1: Predict only for building 1)
train_b1 = train[train[num_col] == 1].copy()
train_b1 = train_b1.sort_values(by=date_col).reset_index(drop=True)

print(f"1ë²ˆ ê±´ë¬¼ ë°ì´í„° shape (Building 1 data shape): {train_b1.shape}")

# ì¡°ê±´2: X_trainì—ì„œ 1ë²ˆ ê±´ë¬¼ ë°ì´í„°ì— StandardScaler ì ìš©
# Condition 2: Apply StandardScaler to building 1 data from X_train
feature_cols = [col for col in train_b1.columns if col not in drop_cols + [target_col]]

lstm_scaler = StandardScaler()
train_b1_scaled = lstm_scaler.fit_transform(train_b1[feature_cols])

print(f"ìŠ¤ì¼€ì¼ë§ëœ feature shape (Scaled feature shape): {train_b1_scaled.shape}")
print(f"feature ìˆ˜ (Number of features): {len(feature_cols)}")

# ì¡°ê±´3: 10 steps (10ì‹œê°„ ê°„ê²©)ìœ¼ë¡œ í•™ìŠµ
# Condition 3: Train with 10 steps (10-hour intervals)
SEQ_LEN = 10

# ì¡°ê±´4: ì‹œê³„ì—´ ë°ì´í„° ë¶„í• 
# - Train: 2022ë…„ 6ì›” 1ì¼ 0ì‹œ ~ 2022ë…„ 8ì›” 10ì¼ 23ì‹œ
# - Validation: 2022ë…„ 8ì›” 11ì¼ 0ì‹œ ~ 2022ë…„ 8ì›” 16ì¼ 23ì‹œ
# Condition 4: Time series data split
# - Train: 2022-06-01 00:00 ~ 2022-08-10 23:00
# - Validation: 2022-08-11 00:00 ~ 2022-08-16 23:00

train_end_date = pd.to_datetime("2022-08-10 23:00:00")
valid_start_date = pd.to_datetime("2022-08-11 00:00:00")
valid_end_date = pd.to_datetime("2022-08-16 23:00:00")

# ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (Split by date)
train_mask = train_b1[date_col] <= train_end_date
valid_mask = (train_b1[date_col] >= valid_start_date) & (train_b1[date_col] <= valid_end_date)

train_features = train_b1_scaled[train_mask]
train_targets = train_b1[target_col].values[train_mask]

valid_features = train_b1_scaled[valid_mask]
valid_targets = train_b1[target_col].values[valid_mask]

print(f"\nTrain features shape: {train_features.shape}")
print(f"Train targets shape: {train_targets.shape}")
print(f"Valid features shape: {valid_features.shape}")
print(f"Valid targets shape: {valid_targets.shape}")


def create_sequences(features, targets, seq_len):
    """
    ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± í•¨ìˆ˜
    Create time series sequence data
    
    Args:
        features: ì…ë ¥ feature ë°°ì—´ (Input feature array)
        targets: íƒ€ê²Ÿ ê°’ ë°°ì—´ (Target value array)
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´ (Sequence length)
        
    Returns:
        X: (num_samples, seq_len, num_features) í˜•íƒœì˜ ë°°ì—´
        y: (num_samples,) í˜•íƒœì˜ ë°°ì—´
    """
    X, y = [], []
    for i in range(len(features) - seq_len):
        X.append(features[i:i + seq_len])
        y.append(targets[i + seq_len])
    return np.array(X), np.array(y)


# Train ì‹œí€€ìŠ¤ ìƒì„± (Create train sequences)
X_lstm_train, y_lstm_train = create_sequences(train_features, train_targets, SEQ_LEN)

# Valid ì‹œí€€ìŠ¤ ìƒì„± - valid ì‹œì‘ ì „ SEQ_LEN ê°œì˜ ë°ì´í„° í•„ìš”
# Create valid sequences - need SEQ_LEN data points before valid start
# ì „ì²´ ë°ì´í„°ì—ì„œ valid êµ¬ê°„ + ì•ì˜ SEQ_LEN ë§Œí¼ ì¶”ì¶œ
all_features_for_valid = np.concatenate([train_features[-SEQ_LEN:], valid_features], axis=0)
all_targets_for_valid = np.concatenate([train_targets[-SEQ_LEN:], valid_targets], axis=0)
X_lstm_valid, y_lstm_valid = create_sequences(all_features_for_valid, all_targets_for_valid, SEQ_LEN)

print(f"\nìµœì¢… LSTM ë°ì´í„°ì…‹ (Final LSTM Dataset):")
print(f"X_lstm_train shape: {X_lstm_train.shape}")
print(f"y_lstm_train shape: {y_lstm_train.shape}")
print(f"X_lstm_valid shape: {X_lstm_valid.shape}")
print(f"y_lstm_valid shape: {y_lstm_valid.shape}")
```

    1ë²ˆ ê±´ë¬¼ ë°ì´í„° shape (Building 1 data shape): (1872, 26)
    ìŠ¤ì¼€ì¼ë§ëœ feature shape (Scaled feature shape): (1872, 22)
    feature ìˆ˜ (Number of features): 22
    
    Train features shape: (1704, 22)
    Train targets shape: (1704,)
    Valid features shape: (144, 22)
    Valid targets shape: (144,)
    
    ìµœì¢… LSTM ë°ì´í„°ì…‹ (Final LSTM Dataset):
    X_lstm_train shape: (1694, 10, 22)
    y_lstm_train shape: (1694,)
    X_lstm_valid shape: (144, 10, 22)
    y_lstm_valid shape: (144,)


### Q16. Q15ì—ì„œ ìƒì„±í•œ train ë°ì´í„°ì™€ validation ë°ì´í„°ë¥¼ LSTMìœ¼ë¡œ ì˜ˆì¸¡í•œ ê°’ì˜ RMSEë¥¼ ì¸¡ì •í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ë‹¨, ì•„ë˜ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•´ì•¼í•©ë‹ˆë‹¤.

> ì¡°ê±´1 : LSTMì˜ êµ¬í˜„ì€ TensorFlow(& Keras), PyTorch ëª¨ë‘ ìƒê´€ì—†ìŠµë‹ˆë‹¤.


> ì¡°ê±´2 : Q15ì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼ tensorë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¦‰, input tensorì˜ shapeì€ (batch_size, seq_len, input_dim) ë˜ëŠ” (batch_size, input_dim, seq_len)ì…ë‹ˆë‹¤.


> ì¡°ê±´3 : ëª¨ë¸ì— ë“¤ì–´ê°€ëŠ” hyper-parameter(e.g. activation function, hidden_dim, n_layers, batch_size, epochs...)ë“±ì€ ììœ ë¡­ê²Œ ì •ì˜í•˜ì—¬ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

> ì¡°ê±´4 : optimizerëŠ” Adam, loss functionì€ Mean Squared Errorë¥¼ ì‚¬ìš©í•˜ë©°, learning rate schedulerë‚˜ regularization methodì˜ ì‚¬ìš©ì—¬ë¶€ëŠ” ììœ ë¡­ê²Œ ê²°ì •í•˜ì…”ì„œ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤.


```python
# sanity check
# X: (batch_size, seq_len, features)
# Y: (batch_size, target_value)

print("=" * 60)
print("Sanity Check - LSTM ë°ì´í„° í˜•íƒœ í™•ì¸")
print("=" * 60)
print(f"X_lstm_train: {X_lstm_train.shape} -> (samples, seq_len, features)")
print(f"y_lstm_train: {y_lstm_train.shape} -> (samples,)")
print(f"X_lstm_valid: {X_lstm_valid.shape} -> (samples, seq_len, features)")
print(f"y_lstm_valid: {y_lstm_valid.shape} -> (samples,)")
print("=" * 60)
```

    ============================================================
    Sanity Check - LSTM ë°ì´í„° í˜•íƒœ í™•ì¸
    ============================================================
    X_lstm_train: (1694, 10, 22) -> (samples, seq_len, features)
    y_lstm_train: (1694,) -> (samples,)
    X_lstm_valid: (144, 10, 22) -> (samples, seq_len, features)
    y_lstm_valid: (144,) -> (samples,)
    ============================================================



```python
# Q16
# Q16. LSTM ëª¨ë¸ë¡œ ì˜ˆì¸¡í•œ ê°’ì˜ RMSE ì¸¡ì •
# Q16. Measure RMSE of LSTM model predictions


class LSTMModel(nn.Module):
    """
    LSTM ëª¨ë¸ í´ë˜ìŠ¤
    LSTM Model Class
    
    Args:
        input_dim: ì…ë ¥ feature ì°¨ì› (Input feature dimension)
        hidden_dim: ì€ë‹‰ì¸µ ì°¨ì› (Hidden layer dimension)
        n_layers: LSTM ë ˆì´ì–´ ìˆ˜ (Number of LSTM layers)
        output_dim: ì¶œë ¥ ì°¨ì› (Output dimension)
        dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (Dropout rate)
    """
    
    def __init__(self, input_dim, hidden_dim=64, n_layers=2, output_dim=1, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        
        self.lstm = nn.LSTM(
            input_dim, 
            hidden_dim, 
            n_layers, 
            batch_first=True,
            dropout=dropout if n_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        """
        ìˆœì „íŒŒ í•¨ìˆ˜ (Forward pass function)
        
        Args:
            x: ì…ë ¥ í…ì„œ (batch_size, seq_len, input_dim)
            
        Returns:
            ì¶œë ¥ í…ì„œ (batch_size, output_dim)
        """
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])
        return out


# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (Set hyperparameters)
INPUT_DIM = X_lstm_train.shape[2]
HIDDEN_DIM = 64
N_LAYERS = 2
OUTPUT_DIM = 1
DROPOUT = 0.2
BATCH_SIZE = 32
EPOCHS = 50
LEARNING_RATE = 0.001

# í…ì„œ ë³€í™˜ (Convert to tensors)
X_train_tensor = torch.FloatTensor(X_lstm_train).to(device)
y_train_tensor = torch.FloatTensor(y_lstm_train).unsqueeze(1).to(device)
X_valid_tensor = torch.FloatTensor(X_lstm_valid).to(device)
y_valid_tensor = torch.FloatTensor(y_lstm_valid).unsqueeze(1).to(device)

# DataLoader ìƒì„± (Create DataLoader)
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)

# ëª¨ë¸ ì´ˆê¸°í™” (Initialize model)
lstm_model = LSTMModel(
    input_dim=INPUT_DIM,
    hidden_dim=HIDDEN_DIM,
    n_layers=N_LAYERS,
    output_dim=OUTPUT_DIM,
    dropout=DROPOUT
).to(device)

# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € (Loss function and optimizer)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)

print(f"LSTM ëª¨ë¸ êµ¬ì¡° (LSTM Model Architecture):")
print(lstm_model)
print(f"\nì´ íŒŒë¼ë¯¸í„° ìˆ˜ (Total parameters): {sum(p.numel() for p in lstm_model.parameters()):,}")

# í•™ìŠµ ë£¨í”„ (Training loop)
print("\n" + "=" * 60)
print("LSTM í•™ìŠµ ì‹œì‘ (Starting LSTM Training)")
print("=" * 60)

best_valid_loss = float("inf")
train_losses = []
valid_losses = []

for epoch in range(EPOCHS):
    # Train
    lstm_model.train()
    train_loss = 0.0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = lstm_model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * X_batch.size(0)
    train_loss /= len(train_dataset)
    train_losses.append(train_loss)
    
    # Validation
    lstm_model.eval()
    valid_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in valid_loader:
            outputs = lstm_model(X_batch)
            loss = criterion(outputs, y_batch)
            valid_loss += loss.item() * X_batch.size(0)
    valid_loss /= len(valid_dataset)
    valid_losses.append(valid_loss)
    
    # Best model ì €ì¥ (Save best model)
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(lstm_model.state_dict(), "best_lstm_model.pth")
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}")

# Best model ë¡œë“œ (Load best model)
lstm_model.load_state_dict(torch.load("best_lstm_model.pth", weights_only=True))

# ìµœì¢… ì˜ˆì¸¡ ë° RMSE ê³„ì‚° (Final prediction and RMSE calculation)
lstm_model.eval()
with torch.no_grad():
    y_train_pred_lstm = lstm_model(X_train_tensor).cpu().numpy().flatten()
    y_valid_pred_lstm = lstm_model(X_valid_tensor).cpu().numpy().flatten()

# RMSE ê³„ì‚° (Calculate RMSE)
lstm_train_rmse = np.sqrt(mean_squared_error(y_lstm_train, y_train_pred_lstm))
lstm_valid_rmse = np.sqrt(mean_squared_error(y_lstm_valid, y_valid_pred_lstm))

print("\n" + "=" * 60)
print("LSTM ëª¨ë¸ ì„±ëŠ¥ (LSTM Model Performance)")
print("=" * 60)
print(f"Train RMSE: {lstm_train_rmse:.4f}")
print(f"Valid RMSE: {lstm_valid_rmse:.4f}")
print("=" * 60)
```

    LSTM ëª¨ë¸ êµ¬ì¡° (LSTM Model Architecture):
    LSTMModel(
      (lstm): LSTM(22, 64, num_layers=2, batch_first=True, dropout=0.2)
      (fc): Linear(in_features=64, out_features=1, bias=True)
    )
    
    ì´ íŒŒë¼ë¯¸í„° ìˆ˜ (Total parameters): 55,873
    
    ============================================================
    LSTM í•™ìŠµ ì‹œì‘ (Starting LSTM Training)
    ============================================================
    Epoch [10/50] - Train Loss: 7926275.8636, Valid Loss: 8916542.6667
    Epoch [20/50] - Train Loss: 7749165.4050, Valid Loss: 8722604.2222
    Epoch [30/50] - Train Loss: 7576676.4481, Valid Loss: 8533550.0000
    Epoch [40/50] - Train Loss: 7407629.2503, Valid Loss: 8347964.8889
    Epoch [50/50] - Train Loss: 7241608.7190, Valid Loss: 8165387.7778
    
    ============================================================
    LSTM ëª¨ë¸ ì„±ëŠ¥ (LSTM Model Performance)
    ============================================================
    Train RMSE: 2689.4562
    Valid RMSE: 2857.5143
    ============================================================


### Q17. Q15ì—ì„œ ìƒì„±í•œ test ë°ì´í„°ë¥¼ Q16ì—ì„œ í•™ìŠµí•œ LSTMì— ëª¨ë¸ë¡œ ì˜ˆì¸¡ê°’ ë½‘ì•„ì„œ, sample_submissionì˜ answer2 columnì— ì±„ìš°ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. answer2ëŠ” ì•ì— ê±´ë¬¼ë²ˆí˜¸ 1ë²ˆì— ëŒ€í•œ ê°’ë“¤ë§Œ ì±„ì›Œì§€ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.


```python
# Q17
# Q17. Test ë°ì´í„°ë¥¼ LSTM ëª¨ë¸ë¡œ ì˜ˆì¸¡í•˜ì—¬ answer2 columnì— ì±„ì›€
# Q17. Predict test data with LSTM model and fill answer2 column

# Test ë°ì´í„°ì—ì„œ 1ë²ˆ ê±´ë¬¼ë§Œ ì¶”ì¶œ (Extract only building 1 from test data)
test_b1 = test[test[num_col] == 1].copy()
test_b1 = test_b1.sort_values(by=date_col).reset_index(drop=True)

print(f"1ë²ˆ ê±´ë¬¼ Test ë°ì´í„° shape (Building 1 test data shape): {test_b1.shape}")

# Test ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (Scale test data)
test_b1_scaled = lstm_scaler.transform(test_b1[feature_cols])

# Test ì‹œí€€ìŠ¤ ìƒì„±ì„ ìœ„í•´ trainì˜ ë§ˆì§€ë§‰ SEQ_LEN ë°ì´í„° ì‚¬ìš©
# Use last SEQ_LEN data from train for test sequence creation
# train_b1ì˜ ë§ˆì§€ë§‰ SEQ_LENê°œ + test ì „ì²´
all_features_for_test = np.concatenate([train_b1_scaled[-SEQ_LEN:], test_b1_scaled], axis=0)

# Test ì‹œí€€ìŠ¤ ìƒì„± (íƒ€ê²Ÿ ì—†ì´) - Create test sequences (without targets)
X_lstm_test = []
for i in range(len(test_b1_scaled)):
    X_lstm_test.append(all_features_for_test[i:i + SEQ_LEN])
X_lstm_test = np.array(X_lstm_test)

print(f"X_lstm_test shape: {X_lstm_test.shape}")

# LSTM ëª¨ë¸ë¡œ ì˜ˆì¸¡ (Predict with LSTM model)
X_test_tensor = torch.FloatTensor(X_lstm_test).to(device)

lstm_model.eval()
with torch.no_grad():
    preds = lstm_model(X_test_tensor).cpu().numpy().flatten()

print(f"LSTM ì˜ˆì¸¡ê°’ shape (LSTM prediction shape): {preds.shape}")
print(f"ì˜ˆì¸¡ê°’ ìƒ˜í”Œ (Sample predictions): {preds[:5]}")
```

    1ë²ˆ ê±´ë¬¼ Test ë°ì´í„° shape (Building 1 test data shape): (168, 25)
    X_lstm_test shape: (168, 10, 22)
    LSTM ì˜ˆì¸¡ê°’ shape (LSTM prediction shape): (168,)
    ì˜ˆì¸¡ê°’ ìƒ˜í”Œ (Sample predictions): [176.76588 176.76588 176.76588 176.76587 176.76588]



```python
submission['answer2'] = preds.reshape(-1, ).tolist() + np.zeros(len(submission)-len(preds)).tolist()
submission
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num_date_time</th>
      <th>answer</th>
      <th>answer2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1_20220818 00</td>
      <td>2434.541504</td>
      <td>176.765884</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1_20220818 01</td>
      <td>2127.760010</td>
      <td>176.765884</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1_20220818 02</td>
      <td>1987.792969</td>
      <td>176.765884</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1_20220818 03</td>
      <td>2092.826416</td>
      <td>176.765869</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1_20220818 04</td>
      <td>2132.575928</td>
      <td>176.765884</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>16795</th>
      <td>100_20220824 19</td>
      <td>746.910950</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>16796</th>
      <td>100_20220824 20</td>
      <td>619.846497</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>16797</th>
      <td>100_20220824 21</td>
      <td>616.784058</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>16798</th>
      <td>100_20220824 22</td>
      <td>498.228027</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>16799</th>
      <td>100_20220824 23</td>
      <td>432.185028</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
<p>16800 rows Ã— 3 columns</p>
</div>



### << ì•„ë˜ëŠ” ì œì¶œíŒŒì¼ ìƒì„±í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤. ì‹¤í–‰ í›„ ìƒì„±ëœ csv íŒŒì¼ë„ í•¨ê»˜ ì œì¶œí•´ì£¼ì„¸ìš” >>


```python
# ì œì¶œ íŒŒì¼ ìƒì„±í•˜ëŠ” ì½”ë“œ.
# name = input("ì œì¶œí•˜ì‹œëŠ” ë¶„ì˜ ì„±í•¨ì„ ì…ë ¥í•´ì£¼ì„¸ìš” : ")  # ì£¼ì„ ì²˜ë¦¬ (Commented out)
name = "test"  # ê¸°ë³¸ê°’ ì„¤ì • (Set default value)
clock = int(time.time())
submission.to_csv(f"submission_{name}_{clock}.csv", index=False)
print(f"ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: submission_{name}_{clock}.csv")
```


```python

```
