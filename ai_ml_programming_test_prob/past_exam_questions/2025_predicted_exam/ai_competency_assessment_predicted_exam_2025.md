# AI Competency Assessment 2025 - ì˜ˆìƒ ê¸°ì¶œë¬¸ì œ
# AI ì—­ëŸ‰ í‰ê°€ 2025 - Predicted Exam Questions

---

## ë¬¸ì œ ì •ì˜ | Problem Definition

ì£¼ì–´ì§„ **ì‚°ì—…ìš© íŒí”„ ì„¼ì„œ ë°ì´í„°**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‘ ê°€ì§€ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:

1. **ì´ìƒ íƒì§€ (Anomaly Detection)**: íŒí”„ì˜ ì •ìƒ/ì´ìƒ ìƒíƒœë¥¼ ë¶„ë¥˜í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ
2. **ê±´ê°• ì ìˆ˜ ì˜ˆì¸¡ (Health Score Prediction)**: íŒí”„ì˜ ê±´ê°• ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì—°ì† ë³€ìˆ˜ ì˜ˆì¸¡

---

## ë°ì´í„° ì„¤ëª… | Data Description

### 1. train.csv
- í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë¡œ 2024ë…„ 1ì›” 1ì¼ 0ì‹œë¶€í„°ì˜ íŒí”„ ì„¼ì„œ ë°ì´í„°ê°€ 15ë¶„ ê°„ê²©ìœ¼ë¡œ ê¸°ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- **ì»¬ëŸ¼ êµ¬ì„±**:

| ì»¬ëŸ¼ëª… | ì„¤ëª… | ë‹¨ìœ„ |
|--------|------|------|
| `timestamp` | ì¸¡ì • ì‹œê°„ | datetime |
| `temperature` | íŒí”„ ì˜¨ë„ | Â°C |
| `pressure` | ì••ë ¥ | bar |
| `vibration` | ì§„ë™ | mm/s |
| `flow_rate` | ìœ ëŸ‰ | L/min |
| `rpm` | ë¶„ë‹¹ íšŒì „ìˆ˜ | RPM |
| `power` | ì „ë ¥ ì†Œë¹„ | kW |
| `anomaly` | ì´ìƒ ì—¬ë¶€ (0: ì •ìƒ, 1: ì´ìƒ) | **ë¶„ë¥˜ íƒ€ê²Ÿ** |
| `health_score` | ì¥ë¹„ ê±´ê°• ì ìˆ˜ (0-100) | **íšŒê·€ íƒ€ê²Ÿ** |

### 2. test.csv
- ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë¡œ 2024ë…„ 6ì›” 1ì¼ 0ì‹œë¶€í„°ì˜ ì„¼ì„œ ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- `anomaly`ì™€ `health_score` ì»¬ëŸ¼ì´ ì œì™¸ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### 3. sample_submission.csv
- ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê¸°ë¡í•˜ëŠ” íŒŒì¼ì…ë‹ˆë‹¤.
- `anomaly`: ì´ìƒ ì—¬ë¶€ ì˜ˆì¸¡ê°’ (0 ë˜ëŠ” 1)
- `health_score`: ê±´ê°• ì ìˆ˜ ì˜ˆì¸¡ê°’ (0-100)

---

## í‰ê°€ ì§€í‘œ | Evaluation Metrics

| Task | Metric | ì„¤ëª… |
|------|--------|------|
| ì´ìƒ íƒì§€ | **F1-Score** | ë°ì´í„° ë¶ˆê· í˜•ìœ¼ë¡œ ì¸í•´ Accuracy ëŒ€ì‹  ì‚¬ìš© |
| ê±´ê°• ì ìˆ˜ ì˜ˆì¸¡ | **RMSE** | Root Mean Squared Error |

---

## ì£¼ìš” ê³¼ì œ | Key Challenges

- **ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬**: ì´ìƒ ë°ì´í„°ê°€ ì „ì²´ì˜ ì•½ 5%ë¡œ ë¶ˆê· í˜•í•¨
- **ê²°ì¸¡ì¹˜ ì²˜ë¦¬**: ì¼ë¶€ ì„¼ì„œ ë°ì´í„°ì— ê²°ì¸¡ì¹˜ ì¡´ì¬
- **íŠ¹ì„± ê³µí•™**: ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ìœ ì˜ë¯¸í•œ íŠ¹ì„± ì¶”ì¶œ
- **ì„ê³„ê°’ ìµœì í™”**: ë¶„ë¥˜ ëª¨ë¸ì˜ ìµœì  ì„ê³„ê°’ íƒìƒ‰

---

## íŒ¨í‚¤ì§€ ì„¤ì¹˜ | Package Installation

```python
!pip install imbalanced-learn xgboost lightgbm optuna
```

---

## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ | Import Libraries

```python
# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ | Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ | Machine Learning libraries
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import (
    classification_report, confusion_matrix, f1_score,
    precision_recall_curve, roc_auc_score, roc_curve,
    mean_squared_error, r2_score
)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier, XGBRegressor
from lightgbm import LGBMClassifier, LGBMRegressor

# ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬ | Handling imbalanced data
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek

# ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ | Deep Learning libraries
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# Optuna for hyperparameter tuning
import optuna
optuna.logging.set_verbosity(optuna.logging.WARNING)

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì • | Set seed for reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# ë””ë°”ì´ìŠ¤ ì„¤ì • | Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ë°ì´í„° ê²½ë¡œ ì„¤ì • | Data path configuration
DATA_PATH = 'dataset/'
```

---

## Q1. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸ | Load Data and Check Basic Info

> train.csvì™€ test.csvë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ê° ë°ì´í„°ì˜ shapeê³¼ ê¸°ë³¸ í†µê³„ëŸ‰ì„ ì¶œë ¥í•˜ì„¸ìš”.

```python
# A1. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸
# A1. Load data and check basic info

# ë°ì´í„° ë¡œë“œ | Load data
train = pd.read_csv(DATA_PATH + 'train.csv')
test = pd.read_csv(DATA_PATH + 'test.csv')

# ë°ì´í„° í¬ê¸° í™•ì¸ | Check data shape
print("=" * 60)
print("ë°ì´í„° í¬ê¸° | Data Shape")
print("=" * 60)
print(f"Train shape: {train.shape}")
print(f"Test shape: {test.shape}")

# í•™ìŠµ ë°ì´í„° ê¸°ë³¸ ì •ë³´ | Train data basic info
print("\n" + "=" * 60)
print("í•™ìŠµ ë°ì´í„° ì •ë³´ | Train Data Info")
print("=" * 60)
print(train.info())

# í•™ìŠµ ë°ì´í„° í†µê³„ëŸ‰ | Train data statistics
print("\n" + "=" * 60)
print("í•™ìŠµ ë°ì´í„° í†µê³„ëŸ‰ | Train Data Statistics")
print("=" * 60)
print(train.describe())

# í…ŒìŠ¤íŠ¸ ë°ì´í„° í†µê³„ëŸ‰ | Test data statistics
print("\n" + "=" * 60)
print("í…ŒìŠ¤íŠ¸ ë°ì´í„° í†µê³„ëŸ‰ | Test Data Statistics")
print("=" * 60)
print(test.describe())

# ì²« 5ê°œ í–‰ í™•ì¸ | Check first 5 rows
print("\n" + "=" * 60)
print("í•™ìŠµ ë°ì´í„° ì²« 5ê°œ í–‰ | Train Data First 5 Rows")
print("=" * 60)
print(train.head())
```

---

## Q2. ê²°ì¸¡ì¹˜ í™•ì¸ ë° ì²˜ë¦¬ | Check and Handle Missing Values

> train, test ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜ë¥¼ í™•ì¸í•˜ê³ , ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”.
> - íŒíŠ¸: ì„¼ì„œ ë°ì´í„°ì˜ íŠ¹ì„±ìƒ ì„ í˜• ë³´ê°„(linear interpolation)ì´ë‚˜ ì „í›„ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°(ffill/bfill)ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.

```python
# A2. ê²°ì¸¡ì¹˜ í™•ì¸ ë° ì²˜ë¦¬
# A2. Check and handle missing values

# ê²°ì¸¡ì¹˜ í™•ì¸ | Check missing values
print("=" * 60)
print("ê²°ì¸¡ì¹˜ í˜„í™© (ì²˜ë¦¬ ì „) | Missing Values (Before)")
print("=" * 60)
print("\n[Train ê²°ì¸¡ì¹˜ | Train Missing Values]")
print(train.isnull().sum())
print(f"\nì´ ê²°ì¸¡ì¹˜ ìˆ˜: {train.isnull().sum().sum()}")

print("\n[Test ê²°ì¸¡ì¹˜ | Test Missing Values]")
print(test.isnull().sum())
print(f"\nì´ ê²°ì¸¡ì¹˜ ìˆ˜: {test.isnull().sum().sum()}")

# ì„¼ì„œ ë°ì´í„° ì»¬ëŸ¼ ì •ì˜ | Define sensor data columns
sensor_cols = ['temperature', 'pressure', 'vibration', 'flow_rate', 'rpm', 'power']

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ | Define missing value handling function
def handle_missing_values(df: pd.DataFrame, sensor_columns: list) -> pd.DataFrame:
    """
    ì„¼ì„œ ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    Handle missing values in sensor data.

    Args:
        df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„ | Input DataFrame
        sensor_columns: ì„¼ì„œ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ | List of sensor columns

    Returns:
        ê²°ì¸¡ì¹˜ê°€ ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ | DataFrame with handled missing values
    """
    df_copy = df.copy()

    # 1ë‹¨ê³„: ì„ í˜• ë³´ê°„ ì ìš© | Step 1: Apply linear interpolation
    # ì„¼ì„œ ë°ì´í„°ëŠ” ì‹œê³„ì—´ íŠ¹ì„±ìƒ ì„ í˜• ë³´ê°„ì´ ì í•©
    # Linear interpolation is suitable for time-series sensor data
    for col in sensor_columns:
        if col in df_copy.columns:
            df_copy[col] = df_copy[col].interpolate(method='linear')

    # 2ë‹¨ê³„: ë‚¨ì€ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì²« í–‰/ë§ˆì§€ë§‰ í–‰) | Step 2: Handle remaining NaN (first/last rows)
    # ffill: ì•ì˜ ê°’ìœ¼ë¡œ ì±„ìš°ê¸° | Forward fill
    # bfill: ë’¤ì˜ ê°’ìœ¼ë¡œ ì±„ìš°ê¸° | Backward fill
    df_copy = df_copy.ffill()
    df_copy = df_copy.bfill()

    return df_copy

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì ìš© | Apply missing value handling
train = handle_missing_values(train, sensor_cols)
test = handle_missing_values(test, sensor_cols)

# ì²˜ë¦¬ í›„ ê²°ì¸¡ì¹˜ í™•ì¸ | Check missing values after handling
print("\n" + "=" * 60)
print("ê²°ì¸¡ì¹˜ í˜„í™© (ì²˜ë¦¬ í›„) | Missing Values (After)")
print("=" * 60)
print("\n[Train ê²°ì¸¡ì¹˜ | Train Missing Values]")
print(train.isnull().sum())
print(f"\nì´ ê²°ì¸¡ì¹˜ ìˆ˜: {train.isnull().sum().sum()}")

print("\n[Test ê²°ì¸¡ì¹˜ | Test Missing Values]")
print(test.isnull().sum())
print(f"\nì´ ê²°ì¸¡ì¹˜ ìˆ˜: {test.isnull().sum().sum()}")

print("\nâœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ! | Missing value handling completed!")
```

---

## Q3. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„ | Analyze Class Imbalance

> train ë°ì´í„°ì˜ `anomaly` ì»¬ëŸ¼ì—ì„œ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ í™•ì¸í•˜ê³ , ë¶ˆê· í˜• ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì„¸ìš”.

```python
# A3. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„
# A3. Analyze class imbalance

# í´ë˜ìŠ¤ë³„ ê°œìˆ˜ ê³„ì‚° | Calculate class counts
class_counts = train['anomaly'].value_counts().sort_index()  # ì¸ë±ìŠ¤ë¡œ ì •ë ¬ | Sort by index
class_ratio = train['anomaly'].value_counts(normalize=True).sort_index() * 100

# í´ë˜ìŠ¤ ë¶„í¬ ì¶œë ¥ | Print class distribution
print("=" * 60)
print("í´ë˜ìŠ¤ ë¶„í¬ | Class Distribution")
print("=" * 60)
normal_count = class_counts.get(0, 0)
anomaly_count = class_counts.get(1, 0)
normal_ratio = class_ratio.get(0, 0)
anomaly_ratio = class_ratio.get(1, 0)

print(f"\nì •ìƒ (Normal, 0): {normal_count:,}ê°œ ({normal_ratio:.2f}%)")
print(f"ì´ìƒ (Anomaly, 1): {anomaly_count:,}ê°œ ({anomaly_ratio:.2f}%)")

# ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚° | Calculate imbalance ratio
# ë¶ˆê· í˜• ë¹„ìœ¨ = ë‹¤ìˆ˜ í´ë˜ìŠ¤ / ì†Œìˆ˜ í´ë˜ìŠ¤
# Imbalance ratio = majority class / minority class
imbalance_ratio = normal_count / anomaly_count if anomaly_count > 0 else float('inf')
print(f"\në¶ˆê· í˜• ë¹„ìœ¨ (Imbalance Ratio): {imbalance_ratio:.2f}:1")
print(f"  â†’ ì •ìƒ ë°ì´í„°ê°€ ì´ìƒ ë°ì´í„°ë³´ë‹¤ ì•½ {imbalance_ratio:.1f}ë°° ë§ìŒ")
print(f"  â†’ Normal data is approximately {imbalance_ratio:.1f}x more than anomaly data")

# ì‹œê°í™” | Visualization
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# ë§‰ëŒ€ ê·¸ë˜í”„ | Bar plot
colors = ['#2ecc71', '#e74c3c']  # ë…¹ìƒ‰(ì •ìƒ), ë¹¨ê°„ìƒ‰(ì´ìƒ)
plot_counts = [normal_count, anomaly_count]
bars = axes[0].bar(['Normal (0)', 'Anomaly (1)'], plot_counts, color=colors, edgecolor='black')
axes[0].set_title('í´ë˜ìŠ¤ ë¶„í¬ (ë§‰ëŒ€ ê·¸ë˜í”„)\nClass Distribution (Bar Plot)', fontsize=12)
axes[0].set_xlabel('Class')
axes[0].set_ylabel('Count')
# ë§‰ëŒ€ ìœ„ì— ê°œìˆ˜ í‘œì‹œ | Display count on bars
for bar, count in zip(bars, plot_counts):
    axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 500,
                 f'{count:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')

# íŒŒì´ ì°¨íŠ¸ | Pie chart
axes[1].pie(plot_counts, labels=['Normal (0)', 'Anomaly (1)'],
            autopct='%1.1f%%', colors=colors, explode=(0, 0.1),
            shadow=True, startangle=90)
axes[1].set_title('í´ë˜ìŠ¤ ë¶„í¬ (íŒŒì´ ì°¨íŠ¸)\nClass Distribution (Pie Chart)', fontsize=12)

plt.tight_layout()
plt.show()

# ìš”ì•½ | Summary
print("\n" + "=" * 60)
print("ğŸ“Š ë¶„ì„ ìš”ì•½ | Analysis Summary")
print("=" * 60)
print("â€¢ ë°ì´í„°ê°€ ì‹¬ê°í•˜ê²Œ ë¶ˆê· í˜•í•¨ (ì•½ 5% vs 95%)")
print("â€¢ Data is severely imbalanced (approximately 5% vs 95%)")
print("â€¢ ì´ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•  ì˜ˆì •:")
print("â€¢ To address this imbalance, we will use the following methods:")
print("  1. Class weights (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜)")
print("  2. SMOTE oversampling (SMOTE ì˜¤ë²„ìƒ˜í”Œë§)")
print("  3. Threshold optimization (ì„ê³„ê°’ ìµœì í™”)")
```

### í•µì‹¬ ê°œë… | Key Concept

**ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ë°©ë²•**:
1. **Class Weight**: ì†Œìˆ˜ í´ë˜ìŠ¤ì— ë” í° ê°€ì¤‘ì¹˜ ë¶€ì—¬
2. **Oversampling**: SMOTE ë“±ìœ¼ë¡œ ì†Œìˆ˜ í´ë˜ìŠ¤ ì¦ê°•
3. **Undersampling**: ë‹¤ìˆ˜ í´ë˜ìŠ¤ ì¶•ì†Œ
4. **Threshold ì¡°ì •**: ë¶„ë¥˜ ì„ê³„ê°’ ìµœì í™”

---

## Q4. íŠ¹ì„± ê³µí•™ - ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„± | Feature Engineering - Time-based Features

> `timestamp` ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±ë“¤ì„ ìƒì„±í•˜ì„¸ìš”.

```python
# A4. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„±
# A4. Create time-based features

def create_time_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    timestamp ì»¬ëŸ¼ìœ¼ë¡œë¶€í„° ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤.
    Create time-based features from timestamp column.

    Args:
        df: timestamp ì»¬ëŸ¼ì´ ìˆëŠ” ë°ì´í„°í”„ë ˆì„ | DataFrame with timestamp column

    Returns:
        ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„ | DataFrame with time-based features added
    """
    df_copy = df.copy()

    # timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ | Convert timestamp to datetime
    df_copy['timestamp'] = pd.to_datetime(df_copy['timestamp'])

    # ê¸°ë³¸ ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ | Extract basic time features
    df_copy['hour'] = df_copy['timestamp'].dt.hour           # ì‹œê°„ (0-23) | Hour (0-23)
    df_copy['day_of_week'] = df_copy['timestamp'].dt.dayofweek  # ìš”ì¼ (0-6) | Day of week (0-6)
    df_copy['day_of_month'] = df_copy['timestamp'].dt.day       # ì¼ (1-31) | Day of month (1-31)
    df_copy['month'] = df_copy['timestamp'].dt.month            # ì›” (1-12) | Month (1-12)

    # ìˆœí™˜ ë³€í™˜ (Cyclic Transformation) | Cyclic Transformation
    # ì‹œê°„ê³¼ ìš”ì¼ì€ ìˆœí™˜ì  íŠ¹ì„±ì„ ê°€ì§ (23ì‹œ â†’ 0ì‹œ, í† ìš”ì¼ â†’ ì¼ìš”ì¼)
    # Hour and day_of_week have cyclic nature (23h â†’ 0h, Saturday â†’ Sunday)
    # ê³µì‹ | Formula: sin(2Ï€ * value / period), cos(2Ï€ * value / period)

    # ì‹œê°„ ìˆœí™˜ ë³€í™˜ (24ì‹œê°„ ì£¼ê¸°) | Hour cyclic transformation (24-hour period)
    df_copy['hour_sin'] = np.sin(2 * np.pi * df_copy['hour'] / 24)
    df_copy['hour_cos'] = np.cos(2 * np.pi * df_copy['hour'] / 24)

    # ìš”ì¼ ìˆœí™˜ ë³€í™˜ (7ì¼ ì£¼ê¸°) | Day of week cyclic transformation (7-day period)
    df_copy['day_sin'] = np.sin(2 * np.pi * df_copy['day_of_week'] / 7)
    df_copy['day_cos'] = np.cos(2 * np.pi * df_copy['day_of_week'] / 7)

    return df_copy

# ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ì ìš© | Apply time-based features
train = create_time_features(train)
test = create_time_features(test)

# ìƒì„±ëœ íŠ¹ì„± í™•ì¸ | Check created features
print("=" * 60)
print("ìƒì„±ëœ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± | Created Time-based Features")
print("=" * 60)
time_cols = ['timestamp', 'hour', 'day_of_week', 'day_of_month', 'month',
             'hour_sin', 'hour_cos', 'day_sin', 'day_cos']
print(train[time_cols].head(10))

print("\nâœ… ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„± ì™„ë£Œ! | Time-based feature creation completed!")
print(f"Train ì»¬ëŸ¼ ìˆ˜: {len(train.columns)}, Test ì»¬ëŸ¼ ìˆ˜: {len(test.columns)}")
```

### ìˆœí™˜ ë³€í™˜ ê³µì‹ | Cyclic Transformation Formula

$$\text{feature\_sin} = \sin\left(\frac{2\pi \times \text{feature}}{\text{period}}\right)$$

$$\text{feature\_cos} = \cos\left(\frac{2\pi \times \text{feature}}{\text{period}}\right)$$

---

## Q5. íŠ¹ì„± ê³µí•™ - ë¡¤ë§ í†µê³„ëŸ‰ | Feature Engineering - Rolling Statistics

> ì„¼ì„œ ë°ì´í„°ì˜ ì‹œê³„ì—´ íŠ¹ì„±ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ ë¡¤ë§ í†µê³„ëŸ‰ì„ ê³„ì‚°í•˜ì„¸ìš”.

```python
# A5. ë¡¤ë§ í†µê³„ëŸ‰ ê³„ì‚°
# A5. Calculate rolling statistics

def create_rolling_features(df: pd.DataFrame, columns: list, window: int = 4) -> pd.DataFrame:
    """
    ì§€ì •ëœ ì»¬ëŸ¼ì— ëŒ€í•´ ë¡¤ë§ í†µê³„ëŸ‰ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    Calculate rolling statistics for specified columns.

    Args:
        df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„ | Input DataFrame
        columns: ë¡¤ë§ í†µê³„ëŸ‰ì„ ê³„ì‚°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ | List of columns for rolling statistics
        window: ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ê°’: 4 = 1ì‹œê°„) | Window size (default: 4 = 1 hour)

    Returns:
        ë¡¤ë§ í†µê³„ëŸ‰ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„ | DataFrame with rolling statistics added
    """
    df_copy = df.copy()

    for col in columns:
        # ì´ë™ í‰ê·  | Rolling mean
        df_copy[f'{col}_rolling_mean'] = df_copy[col].rolling(
            window=window, min_periods=1
        ).mean()

        # ì´ë™ í‘œì¤€í¸ì°¨ | Rolling standard deviation
        # min_periods=2ë¡œ ì„¤ì •í•˜ì—¬ ìµœì†Œ 2ê°œ ê°’ìœ¼ë¡œ std ê³„ì‚° ê°€ëŠ¥
        # Set min_periods=2 so std can be calculated with at least 2 values
        df_copy[f'{col}_rolling_std'] = df_copy[col].rolling(
            window=window, min_periods=2
        ).std()

        # ì´ë™ í‰ê· ê³¼ì˜ í¸ì°¨ (í˜„ì¬ê°’ - ì´ë™í‰ê· ) | Deviation from rolling mean
        df_copy[f'{col}_rolling_dev'] = df_copy[col] - df_copy[f'{col}_rolling_mean']

    return df_copy

# ë¡¤ë§ í†µê³„ëŸ‰ì„ ì ìš©í•  ì„¼ì„œ ì»¬ëŸ¼ | Sensor columns for rolling statistics
rolling_cols = ['temperature', 'vibration', 'pressure']

# ìœˆë„ìš° í¬ê¸° ì„¤ì • | Set window size
# 15ë¶„ ê°„ê²© ë°ì´í„°ì´ë¯€ë¡œ window=4ëŠ” 1ì‹œê°„ì„ ì˜ë¯¸
# With 15-minute intervals, window=4 means 1 hour
WINDOW_SIZE = 4

# ë¡¤ë§ í†µê³„ëŸ‰ ì ìš© | Apply rolling statistics
train = create_rolling_features(train, rolling_cols, window=WINDOW_SIZE)
test = create_rolling_features(test, rolling_cols, window=WINDOW_SIZE)

# ë¡¤ë§ìœ¼ë¡œ ì¸í•œ NaN ì²˜ë¦¬ | Handle NaN from rolling
# rolling_stdì˜ ì²« ë²ˆì§¸ í–‰ì€ NaNì´ ë¨ (ë‹¨ì¼ ê°’ìœ¼ë¡œ std ê³„ì‚° ë¶ˆê°€)
# First row of rolling_std will be NaN (can't calculate std with single value)
train = train.bfill().ffill()
test = test.bfill().ffill()

# ì—¬ì „íˆ NaNì´ ìˆë‹¤ë©´ 0ìœ¼ë¡œ ì±„ìš°ê¸° | Fill any remaining NaN with 0
train = train.fillna(0)
test = test.fillna(0)

# ìƒì„±ëœ ë¡¤ë§ íŠ¹ì„± í™•ì¸ | Check created rolling features
print("=" * 60)
print("ìƒì„±ëœ ë¡¤ë§ íŠ¹ì„± | Created Rolling Features")
print("=" * 60)

rolling_feature_cols = []
for col in rolling_cols:
    rolling_feature_cols.extend([
        f'{col}_rolling_mean',
        f'{col}_rolling_std',
        f'{col}_rolling_dev'
    ])

print("\n[ìƒˆë¡œ ìƒì„±ëœ ë¡¤ë§ íŠ¹ì„± ì»¬ëŸ¼ | New Rolling Feature Columns]")
for i, col in enumerate(rolling_feature_cols, 1):
    print(f"  {i}. {col}")

# ìµœì¢… ë°ì´í„° í¬ê¸° í™•ì¸ | Check final data shape
print("\n" + "=" * 60)
print("ìµœì¢… ë°ì´í„° í¬ê¸° | Final Data Shape")
print("=" * 60)
print(f"Train: {train.shape}")
print(f"Test: {test.shape}")
print(f"\nâœ… ë¡¤ë§ í†µê³„ëŸ‰ ìƒì„± ì™„ë£Œ! | Rolling statistics creation completed!")
```

---

## Q6. ë°ì´í„° ë¶„í•  | Train/Validation Split

> í•™ìŠµ ë°ì´í„°ë¥¼ train/validationìœ¼ë¡œ 8:2 ë¹„ìœ¨ë¡œ ë¶„í• í•˜ì„¸ìš”.
> **ì¤‘ìš”**: í´ë˜ìŠ¤ ë¶ˆê· í˜• ë°ì´í„°ì´ë¯€ë¡œ **Stratified Split**ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

```python
# A6. ë°ì´í„° ë¶„í• 
# A6. Train/Validation split

# íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ (timestampì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸) | Select feature columns (exclude timestamp and targets)
feature_cols = [col for col in train.columns if col not in ['timestamp', 'anomaly', 'health_score']]

print("=" * 60)
print("íŠ¹ì„± ì»¬ëŸ¼ ëª©ë¡ | Feature Columns")
print("=" * 60)
for i, col in enumerate(feature_cols, 1):
    print(f"  {i:2d}. {col}")
print(f"\nì´ íŠ¹ì„± ê°œìˆ˜: {len(feature_cols)}ê°œ")

# íŠ¹ì„± í–‰ë ¬ X ìƒì„± | Create feature matrix X
X = train[feature_cols]

# ë¶„ë¥˜ íƒ€ê²Ÿ y_cls ìƒì„± | Create classification target y_cls
y_cls = train['anomaly']

# íšŒê·€ íƒ€ê²Ÿ y_reg ìƒì„± | Create regression target y_reg
y_reg = train['health_score']

# Stratified Splitìœ¼ë¡œ ë°ì´í„° ë¶„í•  (80:20) | Split data with Stratified Split (80:20)
# í´ë˜ìŠ¤ ë¶ˆê· í˜• ë°ì´í„°ì´ë¯€ë¡œ stratify ì˜µì…˜ ì‚¬ìš© í•„ìˆ˜
# Use stratify option for imbalanced data
X_train, X_val, y_train_cls, y_val_cls = train_test_split(
    X, y_cls,
    test_size=0.2,
    random_state=RANDOM_SEED,
    stratify=y_cls  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ | Maintain class ratio
)

# ë¶„í•  ê²°ê³¼ í™•ì¸ | Check split results
print("\n" + "=" * 60)
print("ë°ì´í„° ë¶„í•  ê²°ê³¼ | Data Split Results")
print("=" * 60)
print(f"\n[ë°ì´í„° í¬ê¸° | Data Shape]")
print(f"X_train: {X_train.shape}")
print(f"X_val: {X_val.shape}")
print(f"y_train_cls: {y_train_cls.shape}")
print(f"y_val_cls: {y_val_cls.shape}")

# í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸ (Stratified Split ê²€ì¦) | Check class ratio (Stratified Split verification)
print(f"\n[í´ë˜ìŠ¤ ë¹„ìœ¨ ê²€ì¦ | Class Ratio Verification]")

# ì•ˆì „í•œ ì¸ë±ì‹± ì‚¬ìš© | Use safe indexing
train_counts = y_train_cls.value_counts().sort_index()
val_counts = y_val_cls.value_counts().sort_index()

train_normal = train_counts.get(0, 0)
train_anomaly = train_counts.get(1, 0)
val_normal = val_counts.get(0, 0)
val_anomaly = val_counts.get(1, 0)

print(f"\nì›ë³¸ ë°ì´í„° | Original Data:")
print(f"  - ì •ìƒ(0): {(y_cls == 0).sum():,}ê°œ ({(y_cls == 0).mean() * 100:.2f}%)")
print(f"  - ì´ìƒ(1): {(y_cls == 1).sum():,}ê°œ ({(y_cls == 1).mean() * 100:.2f}%)")

print(f"\ní•™ìŠµ ë°ì´í„° | Train Data:")
print(f"  - ì •ìƒ(0): {train_normal:,}ê°œ ({train_normal / len(y_train_cls) * 100:.2f}%)")
print(f"  - ì´ìƒ(1): {train_anomaly:,}ê°œ ({train_anomaly / len(y_train_cls) * 100:.2f}%)")

print(f"\nê²€ì¦ ë°ì´í„° | Validation Data:")
print(f"  - ì •ìƒ(0): {val_normal:,}ê°œ ({val_normal / len(y_val_cls) * 100:.2f}%)")
print(f"  - ì´ìƒ(1): {val_anomaly:,}ê°œ ({val_anomaly / len(y_val_cls) * 100:.2f}%)")

# íšŒê·€ íƒ€ê²Ÿë„ ë™ì¼í•œ ì¸ë±ìŠ¤ë¡œ ë¶„í•  | Split regression target with same indices
y_train_reg = train.loc[X_train.index, 'health_score']
y_val_reg = train.loc[X_val.index, 'health_score']

print(f"\n[íšŒê·€ íƒ€ê²Ÿ | Regression Target]")
print(f"y_train_reg: {y_train_reg.shape}, y_val_reg: {y_val_reg.shape}")
print(f"y_train_reg ë²”ìœ„: {y_train_reg.min():.2f} ~ {y_train_reg.max():.2f}")
print(f"y_val_reg ë²”ìœ„: {y_val_reg.min():.2f} ~ {y_val_reg.max():.2f}")

print("\nâœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ! | Data split completed!")
print("âœ… Stratified Splitìœ¼ë¡œ í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ìœ ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("âœ… Class ratio is maintained with Stratified Split.")
```

---

## Q7. ê¸°ì¤€ ëª¨ë¸ - Class Weight ì ìš© | Baseline Model with Class Weights

> ë°ì´í„° ë¶ˆê· í˜•ì„ ì²˜ë¦¬í•˜ëŠ” ì²« ë²ˆì§¸ ë°©ë²•ìœ¼ë¡œ **class_weight** ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ XGBoost ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.

```python
# A7. Class Weightë¥¼ ì ìš©í•œ ê¸°ì¤€ ëª¨ë¸
# A7. Baseline model with class weights

# scale_pos_weight ê³„ì‚° | Calculate scale_pos_weight
# scale_pos_weight = ìŒì„± í´ë˜ìŠ¤ ìˆ˜ / ì–‘ì„± í´ë˜ìŠ¤ ìˆ˜
# scale_pos_weight = count(negative) / count(positive)
neg_count = (y_train_cls == 0).sum()
pos_count = (y_train_cls == 1).sum()
scale_pos_weight = neg_count / pos_count

print("=" * 60)
print("í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° | Class Weight Calculation")
print("=" * 60)
print(f"ì •ìƒ(0) ê°œìˆ˜: {neg_count:,}")
print(f"ì´ìƒ(1) ê°œìˆ˜: {pos_count:,}")
print(f"scale_pos_weight: {scale_pos_weight:.2f}")

# XGBoost ëª¨ë¸ ì •ì˜ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©) | Define XGBoost model (with class weights)
baseline_model = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    scale_pos_weight=scale_pos_weight,  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ | Handle class imbalance
    random_state=RANDOM_SEED,
    eval_metric='logloss',
    use_label_encoder=False
)

# ëª¨ë¸ í•™ìŠµ | Train model
print("\nëª¨ë¸ í•™ìŠµ ì¤‘... | Training model...")
baseline_model.fit(X_train, y_train_cls)
print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! | Model training completed!")

# ì˜ˆì¸¡ | Prediction
y_pred_baseline = baseline_model.predict(X_val)
y_pred_proba_baseline = baseline_model.predict_proba(X_val)[:, 1]

# ì„±ëŠ¥ í‰ê°€ | Performance evaluation
print("\n" + "=" * 60)
print("ê¸°ì¤€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ | Baseline Model Performance")
print("=" * 60)

f1_baseline = f1_score(y_val_cls, y_pred_baseline)
auc_baseline = roc_auc_score(y_val_cls, y_pred_proba_baseline)

print(f"\nF1-Score: {f1_baseline:.4f}")
print(f"AUC-ROC: {auc_baseline:.4f}")

print("\n[Classification Report]")
print(classification_report(y_val_cls, y_pred_baseline, target_names=['Normal (0)', 'Anomaly (1)']))

# Confusion Matrix ì‹œê°í™” | Visualize Confusion Matrix
cm = confusion_matrix(y_val_cls, y_pred_baseline)

fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
            xticklabels=['Normal (0)', 'Anomaly (1)'],
            yticklabels=['Normal (0)', 'Anomaly (1)'])
ax.set_xlabel('Predicted Label (ì˜ˆì¸¡ê°’)')
ax.set_ylabel('True Label (ì‹¤ì œê°’)')
ax.set_title(f'Confusion Matrix - Baseline Model (Class Weight)\nF1-Score: {f1_baseline:.4f}')
plt.tight_layout()
plt.show()
```

---

## Q8. SMOTEë¥¼ ì‚¬ìš©í•œ ì˜¤ë²„ìƒ˜í”Œë§ | Oversampling with SMOTE

> ë°ì´í„° ë¶ˆê· í˜•ì„ ì²˜ë¦¬í•˜ëŠ” ë‘ ë²ˆì§¸ ë°©ë²•ìœ¼ë¡œ **SMOTE**ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

```python
# A8. SMOTEë¥¼ ì‚¬ìš©í•œ ì˜¤ë²„ìƒ˜í”Œë§
# A8. Oversampling with SMOTE

# SMOTE ì ìš© ì „ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸ | Check class distribution before SMOTE
print("=" * 60)
print("SMOTE ì ìš© ì „ í´ë˜ìŠ¤ ë¶„í¬ | Class Distribution Before SMOTE")
print("=" * 60)
print(f"ì •ìƒ(0): {(y_train_cls == 0).sum():,}ê°œ")
print(f"ì´ìƒ(1): {(y_train_cls == 1).sum():,}ê°œ")
print(f"ë¹„ìœ¨: {(y_train_cls == 0).sum() / (y_train_cls == 1).sum():.2f}:1")

# SMOTE ì ìš© | Apply SMOTE
# SMOTE: Synthetic Minority Over-sampling Technique
# ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ í•©ì„± ìƒ˜í”Œì„ ìƒì„±í•˜ì—¬ í´ë˜ìŠ¤ ê· í˜•ì„ ë§ì¶¤
# Creates synthetic samples for minority class to balance classes
smote = SMOTE(random_state=RANDOM_SEED)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train_cls)

# SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸ | Check class distribution after SMOTE
print("\n" + "=" * 60)
print("SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬ | Class Distribution After SMOTE")
print("=" * 60)
print(f"ì •ìƒ(0): {(y_train_smote == 0).sum():,}ê°œ")
print(f"ì´ìƒ(1): {(y_train_smote == 1).sum():,}ê°œ")
print(f"ë¹„ìœ¨: {(y_train_smote == 0).sum() / (y_train_smote == 1).sum():.2f}:1")
print(f"\nì´ ë°ì´í„° ìˆ˜: {len(X_train):,}ê°œ â†’ {len(X_train_smote):,}ê°œ")

# SMOTE ë°ì´í„°ë¡œ XGBoost ëª¨ë¸ í•™ìŠµ | Train XGBoost model with SMOTE data
print("\n" + "=" * 60)
print("SMOTE ëª¨ë¸ í•™ìŠµ | SMOTE Model Training")
print("=" * 60)

smote_model = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=RANDOM_SEED,
    eval_metric='logloss',
    use_label_encoder=False
)

print("ëª¨ë¸ í•™ìŠµ ì¤‘... | Training model...")
smote_model.fit(X_train_smote, y_train_smote)
print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! | Model training completed!")

# ê²€ì¦ ë°ì´í„°ë¡œ ì˜ˆì¸¡ (ì›ë³¸ ê²€ì¦ ë°ì´í„° ì‚¬ìš©) | Predict on validation data (original validation data)
y_pred_smote = smote_model.predict(X_val)
y_pred_proba_smote = smote_model.predict_proba(X_val)[:, 1]

# ì„±ëŠ¥ í‰ê°€ | Performance evaluation
f1_smote = f1_score(y_val_cls, y_pred_smote)
auc_smote = roc_auc_score(y_val_cls, y_pred_proba_smote)

print("\n" + "=" * 60)
print("SMOTE ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ | SMOTE Model Performance")
print("=" * 60)
print(f"\nF1-Score: {f1_smote:.4f}")
print(f"AUC-ROC: {auc_smote:.4f}")

print("\n[Classification Report]")
print(classification_report(y_val_cls, y_pred_smote, target_names=['Normal (0)', 'Anomaly (1)']))

# ê¸°ì¤€ ëª¨ë¸ê³¼ ë¹„êµ | Compare with baseline model
print("\n" + "=" * 60)
print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ | Model Performance Comparison")
print("=" * 60)
print(f"{'ëª¨ë¸ | Model':<30} {'F1-Score':<15} {'AUC-ROC':<15}")
print("-" * 60)
print(f"{'Baseline (Class Weight)':<30} {f1_baseline:<15.4f} {auc_baseline:<15.4f}")
print(f"{'SMOTE':<30} {f1_smote:<15.4f} {auc_smote:<15.4f}")
print("-" * 60)
```

### SMOTE ë™ì‘ ì›ë¦¬ | How SMOTE Works

SMOTEëŠ” ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œë“¤ ì‚¬ì´ì—ì„œ **ìƒˆë¡œìš´ í•©ì„± ìƒ˜í”Œ**ì„ ìƒì„±í•©ë‹ˆë‹¤:
1. ì†Œìˆ˜ í´ë˜ìŠ¤ì—ì„œ ìƒ˜í”Œ $x_i$ ì„ íƒ
2. $x_i$ì˜ k-ìµœê·¼ì ‘ ì´ì›ƒ ì¤‘ í•˜ë‚˜ $x_{nn}$ ì„ íƒ
3. ìƒˆ ìƒ˜í”Œ ìƒì„±: $x_{new} = x_i + \lambda \cdot (x_{nn} - x_i)$, where $\lambda \in [0, 1]$

---

## Q9. ì„ê³„ê°’ ìµœì í™” | Threshold Optimization

> ë¶„ë¥˜ ëª¨ë¸ì˜ ê¸°ë³¸ ì„ê³„ê°’ì€ 0.5ì´ì§€ë§Œ, ë¶ˆê· í˜• ë°ì´í„°ì—ì„œëŠ” ìµœì  ì„ê³„ê°’ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# A9. ì„ê³„ê°’ ìµœì í™”
# A9. Threshold optimization

# ì˜ˆì¸¡ í™•ë¥  ì–»ê¸° (SMOTE ëª¨ë¸ ì‚¬ìš©) | Get prediction probabilities (using SMOTE model)
y_val_proba = smote_model.predict_proba(X_val)[:, 1]

# Precision-Recall Curve ê³„ì‚° | Calculate Precision-Recall Curve
precision_arr, recall_arr, thresholds = precision_recall_curve(y_val_cls, y_val_proba)

# F1-Score ê³„ì‚° (ê° ì„ê³„ê°’ì— ëŒ€í•´) | Calculate F1-Score (for each threshold)
# F1 = 2 * (precision * recall) / (precision + recall)
# precision_arr, recall_arrëŠ” thresholdsë³´ë‹¤ 1ê°œ ë” ë§ìŒ
# precision_arr, recall_arr have one more element than thresholds
f1_scores = 2 * (precision_arr[:-1] * recall_arr[:-1]) / (precision_arr[:-1] + recall_arr[:-1] + 1e-10)

# ìµœì  ì„ê³„ê°’ ì°¾ê¸° | Find optimal threshold
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]
best_f1 = f1_scores[best_idx]
best_precision = precision_arr[best_idx]
best_recall = recall_arr[best_idx]

print("=" * 60)
print("ìµœì  ì„ê³„ê°’ íƒìƒ‰ | Optimal Threshold Search")
print("=" * 60)
print(f"\nê¸°ë³¸ ì„ê³„ê°’ (Default threshold): 0.5")
print(f"ìµœì  ì„ê³„ê°’ (Optimal threshold): {best_threshold:.4f}")
print(f"\nìµœì  ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ | Performance at Optimal Threshold:")
print(f"  - F1-Score: {best_f1:.4f}")
print(f"  - Precision: {best_precision:.4f}")
print(f"  - Recall: {best_recall:.4f}")

# ì‹œê°í™” | Visualization
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. Precision-Recall Curve
axes[0].plot(recall_arr, precision_arr, 'b-', linewidth=2)
axes[0].scatter([best_recall], [best_precision], color='red', s=100, zorder=5,
                label=f'Best (threshold={best_threshold:.3f})')
axes[0].set_xlabel('Recall')
axes[0].set_ylabel('Precision')
axes[0].set_title('Precision-Recall Curve')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# 2. F1-Score vs Threshold
axes[1].plot(thresholds, f1_scores, 'g-', linewidth=2)
axes[1].axvline(x=best_threshold, color='red', linestyle='--', label=f'Best: {best_threshold:.3f}')
axes[1].axvline(x=0.5, color='gray', linestyle=':', alpha=0.7, label='Default: 0.5')
axes[1].scatter([best_threshold], [best_f1], color='red', s=100, zorder=5)
axes[1].set_xlabel('Threshold (ì„ê³„ê°’)')
axes[1].set_ylabel('F1-Score')
axes[1].set_title('F1-Score vs Threshold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

# 3. Precision & Recall vs Threshold
axes[2].plot(thresholds, precision_arr[:-1], 'b-', linewidth=2, label='Precision')
axes[2].plot(thresholds, recall_arr[:-1], 'r-', linewidth=2, label='Recall')
axes[2].axvline(x=best_threshold, color='green', linestyle='--', label=f'Best: {best_threshold:.3f}')
axes[2].set_xlabel('Threshold (ì„ê³„ê°’)')
axes[2].set_ylabel('Score')
axes[2].set_title('Precision & Recall vs Threshold')
axes[2].legend()
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ìµœì  ì„ê³„ê°’ ì ìš© | Apply optimal threshold
y_pred_optimized = (y_val_proba >= best_threshold).astype(int)

# ìµœì  ì„ê³„ê°’ ì ìš© í›„ Classification Report | Classification Report with optimal threshold
print("\n[ìµœì  ì„ê³„ê°’ ì ìš© í›„ Classification Report]")
print("[Classification Report with Optimal Threshold]")
print(classification_report(y_val_cls, y_pred_optimized, target_names=['Normal (0)', 'Anomaly (1)']))
```

---

## Q10. Autoencoderë¥¼ ì‚¬ìš©í•œ ì´ìƒ íƒì§€ | Anomaly Detection with Autoencoder

> ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ íƒì§€ ë°©ë²•ìœ¼ë¡œ **Autoencoder**ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

### í•µì‹¬ ì•„ì´ë””ì–´ | Key Idea

- ì •ìƒ ë°ì´í„°ë¡œë§Œ Autoencoderë¥¼ í•™ìŠµ
- ì´ìƒ ë°ì´í„°ëŠ” ì¬êµ¬ì„±ì´ ì˜ ì•ˆë˜ì–´ **ì¬êµ¬ì„± ì˜¤ì°¨(reconstruction error)**ê°€ ë†’ìŒ
- ì¬êµ¬ì„± ì˜¤ì°¨ê°€ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ì´ìƒìœ¼ë¡œ ë¶„ë¥˜

```python
# A10. Autoencoder ì •ì˜
# A10. Define Autoencoder

class Autoencoder(nn.Module):
    """
    ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ Autoencoder ëª¨ë¸ì…ë‹ˆë‹¤.
    Autoencoder model for anomaly detection.

    í•µì‹¬ ì•„ì´ë””ì–´ | Key Idea:
    - ì •ìƒ ë°ì´í„°ë¡œë§Œ í•™ìŠµí•˜ë©´ ì´ìƒ ë°ì´í„°ëŠ” ì¬êµ¬ì„± ì˜¤ì°¨ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.
    - When trained only on normal data, anomalies will have high reconstruction error.

    ì•„í‚¤í…ì²˜ | Architecture:
    - Encoder: Input â†’ 32 â†’ 16 â†’ 8 (latent)
    - Decoder: 8 â†’ 16 â†’ 32 â†’ Output
    """

    def __init__(self, input_dim: int):
        """
        Args:
            input_dim: ì…ë ¥ íŠ¹ì„± ì°¨ì› | Input feature dimension
        """
        super(Autoencoder, self).__init__()

        # Encoder: ì…ë ¥ì„ ì €ì°¨ì› ì ì¬ ê³µê°„ìœ¼ë¡œ ì••ì¶•
        # Encoder: Compress input to low-dimensional latent space
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 32),   # Input â†’ 32
            nn.ReLU(),
            nn.Linear(32, 16),          # 32 â†’ 16
            nn.ReLU(),
            nn.Linear(16, 8)            # 16 â†’ 8 (latent)
        )

        # Decoder: ì ì¬ ê³µê°„ì—ì„œ ì›ë³¸ ì°¨ì›ìœ¼ë¡œ ë³µì›
        # Decoder: Reconstruct from latent space to original dimension
        self.decoder = nn.Sequential(
            nn.Linear(8, 16),           # 8 â†’ 16
            nn.ReLU(),
            nn.Linear(16, 32),          # 16 â†’ 32
            nn.ReLU(),
            nn.Linear(32, input_dim)    # 32 â†’ Output
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ìˆœì „íŒŒ í•¨ìˆ˜ì…ë‹ˆë‹¤.
        Forward pass function.

        Args:
            x: ì…ë ¥ í…ì„œ | Input tensor

        Returns:
            ì¬êµ¬ì„±ëœ ì¶œë ¥ í…ì„œ | Reconstructed output tensor
        """
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

print("âœ… Autoencoder í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ! | Autoencoder class defined!")
print("\n[ëª¨ë¸ ì•„í‚¤í…ì²˜ | Model Architecture]")
print("Encoder: Input â†’ 32 (ReLU) â†’ 16 (ReLU) â†’ 8 (latent)")
print("Decoder: 8 â†’ 16 (ReLU) â†’ 32 (ReLU) â†’ Output")
```

```python
# A10-2. Autoencoder í•™ìŠµ ë° ì´ìƒ íƒì§€
# A10-2. Train Autoencoder and detect anomalies

# ë°ì´í„° ì „ì²˜ë¦¬: ì •ê·œí™” | Data preprocessing: Normalization
scaler = StandardScaler()

# ì¸ë±ìŠ¤ ì •ë ¬ì„ ìœ„í•´ reset_index ì‚¬ìš© | Use reset_index for index alignment
X_train_reset = X_train.reset_index(drop=True)
y_train_cls_reset = y_train_cls.reset_index(drop=True)
X_val_reset = X_val.reset_index(drop=True)
y_val_cls_reset = y_val_cls.reset_index(drop=True)

# ì •ìƒ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ | Extract only normal data for training
normal_mask = y_train_cls_reset == 0
X_train_normal = X_train_reset[normal_mask]
X_train_scaled = scaler.fit_transform(X_train_normal)
X_val_scaled = scaler.transform(X_val_reset)

print("=" * 60)
print("Autoencoder í•™ìŠµ ë°ì´í„° | Autoencoder Training Data")
print("=" * 60)
print(f"ì •ìƒ ë°ì´í„°ë¡œë§Œ í•™ìŠµ | Training only on normal data")
print(f"í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train_scaled.shape}")
print(f"ê²€ì¦ ë°ì´í„° í¬ê¸°: {X_val_scaled.shape}")

# PyTorch í…ì„œë¡œ ë³€í™˜ | Convert to PyTorch tensors
X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)
X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)

# DataLoader ìƒì„± | Create DataLoader
train_dataset = TensorDataset(X_train_tensor, X_train_tensor)  # ì…ë ¥ = ì¶œë ¥ (ì¬êµ¬ì„±)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# ëª¨ë¸ ì´ˆê¸°í™” | Initialize model
input_dim = X_train_scaled.shape[1]
autoencoder = Autoencoder(input_dim).to(device)

# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € | Loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)

# í•™ìŠµ ì„¤ì • | Training settings
EPOCHS = 50

# í•™ìŠµ | Training
print("\n" + "=" * 60)
print("Autoencoder í•™ìŠµ ì‹œì‘ | Starting Autoencoder Training")
print("=" * 60)

train_losses = []
for epoch in range(EPOCHS):
    autoencoder.train()
    epoch_loss = 0.0

    for batch_x, _ in train_loader:
        reconstructed = autoencoder(batch_x)
        loss = criterion(reconstructed, batch_x)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    avg_loss = epoch_loss / len(train_loader)
    train_losses.append(avg_loss)

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch + 1:3d}/{EPOCHS}] - Loss: {avg_loss:.6f}")

print("âœ… Autoencoder í•™ìŠµ ì™„ë£Œ! | Autoencoder training completed!")

# ì´ìƒ íƒì§€: ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚° | Anomaly detection: Calculate reconstruction error
autoencoder.eval()
with torch.no_grad():
    val_reconstructed = autoencoder(X_val_tensor)
    reconstruction_errors = torch.mean((X_val_tensor - val_reconstructed) ** 2, dim=1).cpu().numpy()

# ì •ìƒ ë°ì´í„°ì˜ ì¬êµ¬ì„± ì˜¤ì°¨ë¡œ ì„ê³„ê°’ ì„¤ì • (95ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜)
# Set threshold using reconstruction error of normal data (95th percentile)
y_val_cls_np = y_val_cls_reset.values
normal_errors = reconstruction_errors[y_val_cls_np == 0]
anomaly_errors = reconstruction_errors[y_val_cls_np == 1]

ae_threshold = np.percentile(normal_errors, 95)
print(f"\nì¬êµ¬ì„± ì˜¤ì°¨ ì„ê³„ê°’ (95th percentile): {ae_threshold:.6f}")

# ì„ê³„ê°’ ê¸°ë°˜ ì˜ˆì¸¡ | Threshold-based prediction
y_pred_ae = (reconstruction_errors > ae_threshold).astype(int)

# ì„±ëŠ¥ í‰ê°€ | Performance evaluation
f1_ae = f1_score(y_val_cls_np, y_pred_ae)
print(f"\nAutoencoder F1-Score: {f1_ae:.4f}")
```

---

## Q11. ì•™ìƒë¸” ë° ìµœì¢… í‰ê°€ | Ensemble and Final Evaluation

> ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì•™ìƒë¸”í•˜ì—¬ ìµœì¢… ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì„¸ìš”.

```python
# A11. ì•™ìƒë¸” ë° ìµœì¢… í‰ê°€
# A11. Ensemble and final evaluation

print("=" * 60)
print("ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± | Ensemble Model Configuration")
print("=" * 60)
print("\nì‚¬ìš© ëª¨ë¸ | Models Used:")
print("  1. Baseline Model (Q7): XGBoost with class weights")
print("  2. SMOTE Model (Q8): XGBoost with SMOTE")
print("\nì•™ìƒë¸” ë°©ë²• | Ensemble Method: Soft Voting (í™•ë¥  í‰ê· )")

# Soft Voting: í™•ë¥  í‰ê·  | Soft Voting: Average probabilities
y_ensemble_proba = (y_pred_proba_baseline + y_pred_proba_smote) / 2

# ìµœì  ì„ê³„ê°’ ì ìš©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ | Apply optimal threshold for final prediction
y_pred_ensemble = (y_ensemble_proba >= best_threshold).astype(int)

# ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ | Ensemble performance evaluation
f1_ensemble = f1_score(y_val_cls, y_pred_ensemble)
auc_ensemble = roc_auc_score(y_val_cls, y_ensemble_proba)

print("\n" + "=" * 60)
print("ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ | Ensemble Model Performance")
print("=" * 60)
print(f"\nF1-Score: {f1_ensemble:.4f}")
print(f"AUC-ROC: {auc_ensemble:.4f}")

print("\n[Classification Report]")
print(classification_report(y_val_cls, y_pred_ensemble, target_names=['Normal (0)', 'Anomaly (1)']))

# ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ | Compare all model performances
print("\n" + "=" * 60)
print("ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ | All Model Performance Comparison")
print("=" * 60)
print(f"\n{'ëª¨ë¸ | Model':<35} {'F1-Score':<12} {'AUC-ROC':<12}")
print("-" * 60)
print(f"{'1. Baseline (Class Weight)':<35} {f1_baseline:<12.4f} {auc_baseline:<12.4f}")
print(f"{'2. SMOTE':<35} {f1_smote:<12.4f} {auc_smote:<12.4f}")
print(f"{'3. Autoencoder':<35} {f1_ae:<12.4f} {'N/A':<12}")
print(f"{'4. Ensemble (Baseline + SMOTE)':<35} {f1_ensemble:<12.4f} {auc_ensemble:<12.4f}")
print("-" * 60)

# ROC Curve ë¹„êµ | Compare ROC Curves
plt.figure(figsize=(8, 6))

fpr_baseline, tpr_baseline, _ = roc_curve(y_val_cls, y_pred_proba_baseline)
fpr_smote, tpr_smote, _ = roc_curve(y_val_cls, y_pred_proba_smote)
fpr_ensemble, tpr_ensemble, _ = roc_curve(y_val_cls, y_ensemble_proba)

plt.plot(fpr_baseline, tpr_baseline, 'b-', label=f'Baseline (AUC={auc_baseline:.4f})', linewidth=2)
plt.plot(fpr_smote, tpr_smote, 'g-', label=f'SMOTE (AUC={auc_smote:.4f})', linewidth=2)
plt.plot(fpr_ensemble, tpr_ensemble, 'r-', label=f'Ensemble (AUC={auc_ensemble:.4f})', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve ë¹„êµ | ROC Curve Comparison')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.show()

# ìµœì¢… ë¶„ë¥˜ ê²°ê³¼ ì €ì¥ | Save final classification results
final_cls_f1 = f1_ensemble
final_cls_auc = auc_ensemble
print(f"\nâœ… ì•™ìƒë¸” ëª¨ë¸ í‰ê°€ ì™„ë£Œ! | Ensemble model evaluation completed!")
```

---

## Q12. íšŒê·€ ëª¨ë¸ - ê±´ê°• ì ìˆ˜ ì˜ˆì¸¡ | Regression Model - Health Score Prediction

> `health_score`ë¥¼ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.

```python
# A12. íšŒê·€ ëª¨ë¸ í•™ìŠµ
# A12. Train regression model

# íšŒê·€ íƒ€ê²Ÿ ì¤€ë¹„ | Prepare regression target
y_train_reg = train.loc[X_train.index, 'health_score']
y_val_reg = train.loc[X_val.index, 'health_score']

print("=" * 60)
print("íšŒê·€ ëª¨ë¸: ê±´ê°• ì ìˆ˜ ì˜ˆì¸¡ | Regression Model: Health Score Prediction")
print("=" * 60)
print(f"\ní•™ìŠµ ë°ì´í„°: {len(y_train_reg):,}ê°œ")
print(f"ê²€ì¦ ë°ì´í„°: {len(y_val_reg):,}ê°œ")
print(f"íƒ€ê²Ÿ ë²”ìœ„: {y_train_reg.min():.2f} ~ {y_train_reg.max():.2f}")

# Optuna objective í•¨ìˆ˜ ì •ì˜ | Define Optuna objective function
def objective_reg(trial: optuna.Trial) -> float:
    """
    Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ ëª©ì  í•¨ìˆ˜ì…ë‹ˆë‹¤.
    Objective function for Optuna hyperparameter optimization.
    """
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'random_state': RANDOM_SEED,
        'verbose': -1
    }

    model = LGBMRegressor(**params)
    model.fit(X_train, y_train_reg)
    y_pred = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val_reg, y_pred))

    return rmse

# ìŠ¤í„°ë”” ìƒì„± ë° ìµœì í™” | Create study and optimize
print("\n" + "=" * 60)
print("Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” | Optuna Hyperparameter Optimization")
print("=" * 60)

study_reg = optuna.create_study(direction='minimize')  # RMSE ìµœì†Œí™” | Minimize RMSE
study_reg.optimize(objective_reg, n_trials=20, show_progress_bar=True)

# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥ | Print best hyperparameters
print("\n" + "=" * 60)
print("ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° | Best Hyperparameters")
print("=" * 60)
for key, value in study_reg.best_params.items():
    if isinstance(value, float):
        print(f"  {key}: {value:.6f}")
    else:
        print(f"  {key}: {value}")

# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ | Train final model with best hyperparameters
best_params = study_reg.best_params
best_params['random_state'] = RANDOM_SEED
best_params['verbose'] = -1

reg_model = LGBMRegressor(**best_params)
reg_model.fit(X_train, y_train_reg)

# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ | Predict on validation data
y_pred_reg = reg_model.predict(X_val)

# ì„±ëŠ¥ í‰ê°€ | Performance evaluation
final_rmse = np.sqrt(mean_squared_error(y_val_reg, y_pred_reg))
final_r2 = r2_score(y_val_reg, y_pred_reg)

print(f"\n[ìµœì¢… íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ | Final Regression Model Performance]")
print(f"  - RMSE: {final_rmse:.4f}")
print(f"  - RÂ² Score: {final_r2:.4f}")

# ìµœì¢… íšŒê·€ ê²°ê³¼ ì €ì¥ | Save final regression results
final_reg_rmse = final_rmse
final_reg_r2 = final_r2

print(f"\nâœ… íšŒê·€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! | Regression model training completed!")
```

---

## Q13. Test ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± | Predict Test Data and Create Submission

```python
# A13. Test ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±
# A13. Predict test data and create submission

print("=" * 60)
print("Test ë°ì´í„° ì˜ˆì¸¡ | Test Data Prediction")
print("=" * 60)

# Test ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” íŠ¹ì„± ì»¬ëŸ¼ë§Œ ì‚¬ìš© | Use only feature columns that exist in test data
available_feature_cols = [col for col in feature_cols if col in test.columns]

# Test ë°ì´í„°ì˜ íŠ¹ì„± ì¶”ì¶œ | Extract features from test data
X_test = test[available_feature_cols]
print(f"\nTest ë°ì´í„° í¬ê¸°: {X_test.shape}")
print(f"ì‚¬ìš© íŠ¹ì„± ìˆ˜: {len(available_feature_cols)}ê°œ")

# ê²°ì¸¡ì¹˜ í™•ì¸ | Check for missing values
missing_count = X_test.isnull().sum().sum()
print(f"Test ë°ì´í„° ê²°ì¸¡ì¹˜: {missing_count}ê°œ")

if missing_count > 0:
    X_test = X_test.bfill().ffill().fillna(0)

# ë¶„ë¥˜ ì˜ˆì¸¡ (ì•™ìƒë¸” ëª¨ë¸) | Classification prediction (Ensemble model)
print("\n" + "-" * 40)
print("ë¶„ë¥˜ ì˜ˆì¸¡ (ì•™ìƒë¸”) | Classification Prediction (Ensemble)")
print("-" * 40)

test_proba_baseline = baseline_model.predict_proba(X_test)[:, 1]
test_proba_smote = smote_model.predict_proba(X_test)[:, 1]
test_proba_ensemble = (test_proba_baseline + test_proba_smote) / 2
test_pred_anomaly = (test_proba_ensemble >= best_threshold).astype(int)

print(f"ì´ìƒ(1) ì˜ˆì¸¡ ê°œìˆ˜: {test_pred_anomaly.sum():,}ê°œ")
print(f"ì •ìƒ(0) ì˜ˆì¸¡ ê°œìˆ˜: {(test_pred_anomaly == 0).sum():,}ê°œ")

# íšŒê·€ ì˜ˆì¸¡ | Regression prediction
print("\n" + "-" * 40)
print("íšŒê·€ ì˜ˆì¸¡ | Regression Prediction")
print("-" * 40)

test_pred_health = reg_model.predict(X_test)
test_pred_health = np.clip(test_pred_health, 0, 100)

print(f"Health Score ë²”ìœ„: {test_pred_health.min():.2f} ~ {test_pred_health.max():.2f}")

# ì œì¶œ íŒŒì¼ ìƒì„± | Create submission file
submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')
submission['anomaly'] = test_pred_anomaly
submission['health_score'] = test_pred_health

submission.to_csv('submission_2025.csv', index=False)

print("\n" + "=" * 60)
print("âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ! | Submission file created!")
print("=" * 60)
print("íŒŒì¼ëª…: submission_2025.csv")
print(f"\nì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ:")
print(submission.head(10))
```

---

## ìµœì¢… ìš”ì•½ | Final Summary

| Task | Metric | Description |
|------|--------|-------------|
| **ì´ìƒ íƒì§€** | F1-Score | ë°ì´í„° ë¶ˆê· í˜•ì—ì„œ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì • |
| **ì´ìƒ íƒì§€** | AUC-ROC | ë¶„ë¥˜ ëª¨ë¸ì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ |
| **ê±´ê°• ì ìˆ˜ ì˜ˆì¸¡** | RMSE | ì˜ˆì¸¡ ì˜¤ì°¨ì˜ í¬ê¸° |
| **ê±´ê°• ì ìˆ˜ ì˜ˆì¸¡** | RÂ² | ì„¤ëª…ë ¥ |

---

## ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸ | Key Learning Points

1. **ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬**
   - Class Weight ì¡°ì • (`scale_pos_weight`)
   - SMOTE ì˜¤ë²„ìƒ˜í”Œë§
   - ì„ê³„ê°’ ìµœì í™” (Precision-Recall Curve)

2. **íŠ¹ì„± ê³µí•™**
   - ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± (ìˆœí™˜ ë³€í™˜: sin/cos)
   - ë¡¤ë§ í†µê³„ëŸ‰ (ì´ë™ í‰ê· , ì´ë™ í‘œì¤€í¸ì°¨)

3. **ë”¥ëŸ¬ë‹ ì´ìƒ íƒì§€**
   - Autoencoder ê¸°ë°˜ ì¬êµ¬ì„± ì˜¤ì°¨ í™œìš©
   - ì •ìƒ ë°ì´í„°ë¡œë§Œ í•™ìŠµí•˜ì—¬ ì´ìƒ íƒì§€

4. **ì•™ìƒë¸” ê¸°ë²•**
   - Soft Votingìœ¼ë¡œ ì—¬ëŸ¬ ëª¨ë¸ ê²°í•©

5. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
   - Optunaë¥¼ í™œìš©í•œ ìë™ ìµœì í™”
